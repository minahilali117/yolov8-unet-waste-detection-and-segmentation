{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d412467",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup & Random Seed Configuration](#1.-Environment-Setup-&-Random-Seed-Configuration)\n",
    "2. [Import Libraries](#2.-Import-Libraries)\n",
    "3. [Dataset Download & Verification](#3.-Dataset-Download-&-Verification)\n",
    "4. [Exploratory Data Analysis (EDA)](#4.-Exploratory-Data-Analysis-(EDA))\n",
    "5. [Data Preprocessing & Subset Creation](#5.-Data-Preprocessing-&-Subset-Creation)\n",
    "6. [Data Augmentation](#6.-Data-Augmentation)\n",
    "7. [YOLOv8 Object Detection](#7.-YOLOv8-Object-Detection)\n",
    "8. [U-Net Semantic Segmentation](#8.-U-Net-Semantic-Segmentation)\n",
    "9. [Model Comparison & Discussion](#9.-Model-Comparison-&-Discussion)\n",
    "10. [Conclusions & Future Work](#10.-Conclusions-&-Future-Work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d94f6",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup & Random Seed Configuration\n",
    "\n",
    "Setting all random seeds for reproducibility across:\n",
    "- Python's built-in random module\n",
    "- NumPy\n",
    "- PyTorch (CPU and CUDA)\n",
    "- Python hash seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable for Python hash seed (must be done before importing libraries)\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# Import random libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# PyTorch seeds (will be set after importing torch)\n",
    "print(\"âœ“ Python hash seed set to 0\")\n",
    "print(f\"âœ“ Random seed set to {RANDOM_SEED}\")\n",
    "print(f\"âœ“ NumPy seed set to {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch and set its random seeds\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Additional PyTorch reproducibility settings\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"âœ“ PyTorch seed set to {RANDOM_SEED}\")\n",
    "print(f\"âœ“ PyTorch CUDA seed set to {RANDOM_SEED}\")\n",
    "print(\"âœ“ CUDNN deterministic mode enabled\")\n",
    "print(\"âœ“ CUDNN benchmark disabled for reproducibility\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nâœ“ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5496c",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Import Libraries\n",
    "\n",
    "Importing all required libraries for:\n",
    "- Data manipulation and analysis\n",
    "- Image processing\n",
    "- Deep learning (PyTorch, YOLOv8)\n",
    "- Visualization\n",
    "- COCO dataset handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# YOLOv8\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "# COCO tools\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as coco_mask\n",
    "\n",
    "# Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "print(f\"\\nLibrary Versions:\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  Torchvision: {torchvision.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  Pandas: {pd.__version__}\")\n",
    "print(f\"  OpenCV: {cv2.__version__}\")\n",
    "print(f\"  Albumentations: {A.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2c86b0",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Dataset Download & Verification\n",
    "\n",
    "### 3.1 Download TACO Dataset\n",
    "\n",
    "The TACO dataset can be downloaded using:\n",
    "1. **Kaggle API** (recommended - automated)\n",
    "2. **Manual download** from Kaggle website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project directories\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "TACO_DIR = DATA_DIR / 'TACO'\n",
    "RUNS_DIR = PROJECT_ROOT / 'runs'\n",
    "WEIGHTS_DIR = PROJECT_ROOT / 'weights'\n",
    "\n",
    "# Create directories\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "RUNS_DIR.mkdir(exist_ok=True)\n",
    "WEIGHTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Project directory structure:\")\n",
    "print(f\"  Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"  Data Directory: {DATA_DIR}\")\n",
    "print(f\"  TACO Directory: {TACO_DIR}\")\n",
    "print(f\"  Runs Directory: {RUNS_DIR}\")\n",
    "print(f\"  Weights Directory: {WEIGHTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset already exists\n",
    "if TACO_DIR.exists() and len(list(TACO_DIR.glob('*'))) > 0:\n",
    "    print(\"âœ“ TACO dataset already exists!\")\n",
    "    print(f\"  Location: {TACO_DIR}\")\n",
    "else:\n",
    "    print(\"Dataset not found. Please download using one of the following methods:\\n\")\n",
    "    \n",
    "    print(\"Method 1: Kaggle API (Recommended)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"1. Install Kaggle API: pip install kaggle\")\n",
    "    print(\"2. Setup Kaggle credentials (kaggle.json)\")\n",
    "    print(\"3. Run the following commands:\\n\")\n",
    "    print(\"   kaggle datasets download -d kneroma/tacotrashdataset\")\n",
    "    print(f\"   unzip tacotrashdataset.zip -d {DATA_DIR}\")\n",
    "    print(\"\\nMethod 2: Manual Download\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"1. Visit: https://www.kaggle.com/datasets/kneroma/tacotrashdataset\")\n",
    "    print(\"2. Download the dataset\")\n",
    "    print(f\"3. Extract to: {DATA_DIR}\")\n",
    "    print(\"\\nNote: Uncomment and run the cell below to download via Kaggle API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download using Kaggle API\n",
    "# !pip install kaggle\n",
    "# !kaggle datasets download -d kneroma/tacotrashdataset\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile('tacotrashdataset.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall(DATA_DIR)\n",
    "# print(\"âœ“ Dataset downloaded and extracted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e17cc2",
   "metadata": {},
   "source": [
    "### 3.2 Verify Dataset Structure and COCO Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa52c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "if TACO_DIR.exists():\n",
    "    print(\"Dataset Structure:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # List all subdirectories and files\n",
    "    for item in sorted(TACO_DIR.glob('*')):\n",
    "        if item.is_dir():\n",
    "            file_count = len(list(item.glob('*')))\n",
    "            print(f\"ðŸ“ {item.name}/ ({file_count} items)\")\n",
    "        else:\n",
    "            file_size = item.stat().st_size / (1024 * 1024)  # Convert to MB\n",
    "            print(f\"ðŸ“„ {item.name} ({file_size:.2f} MB)\")\n",
    "    \n",
    "    # Look for annotation files\n",
    "    annotation_files = list(TACO_DIR.rglob('*.json'))\n",
    "    print(f\"\\nFound {len(annotation_files)} annotation file(s):\")\n",
    "    for ann_file in annotation_files:\n",
    "        print(f\"  - {ann_file.relative_to(TACO_DIR)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Dataset not found. Please download the dataset first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcad93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify COCO annotations\n",
    "# Note: Update the annotation file path based on actual dataset structure\n",
    "\n",
    "# Common TACO annotation file paths\n",
    "possible_ann_paths = [\n",
    "    TACO_DIR / 'annotations.json',\n",
    "    TACO_DIR / 'annotations' / 'instances_default.json',\n",
    "    TACO_DIR / 'TACO' / 'annotations.json',\n",
    "]\n",
    "\n",
    "ANNOTATION_FILE = None\n",
    "for path in possible_ann_paths:\n",
    "    if path.exists():\n",
    "        ANNOTATION_FILE = path\n",
    "        break\n",
    "\n",
    "if ANNOTATION_FILE:\n",
    "    print(f\"âœ“ Found annotation file: {ANNOTATION_FILE.name}\")\n",
    "    \n",
    "    # Load COCO annotations\n",
    "    with open(ANNOTATION_FILE, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    print(\"\\nCOCO Format Verification:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"  Images: {len(coco_data.get('images', []))}\")\n",
    "    print(f\"  Annotations: {len(coco_data.get('annotations', []))}\")\n",
    "    print(f\"  Categories: {len(coco_data.get('categories', []))}\")\n",
    "    \n",
    "    # Display first few categories\n",
    "    print(\"\\nSample Categories:\")\n",
    "    for cat in coco_data.get('categories', [])[:10]:\n",
    "        print(f\"  ID {cat['id']}: {cat['name']}\")\n",
    "    \n",
    "    if len(coco_data.get('categories', [])) > 10:\n",
    "        print(f\"  ... and {len(coco_data.get('categories', [])) - 10} more\")\n",
    "    \n",
    "    print(\"\\nâœ“ COCO format verified successfully!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Annotation file not found. Please verify dataset structure.\")\n",
    "    print(\"   Expected locations:\")\n",
    "    for path in possible_ann_paths:\n",
    "        print(f\"   - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a2c73a",
   "metadata": {},
   "source": [
    "### 3.3 Find Image Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate image directory\n",
    "possible_img_dirs = [\n",
    "    TACO_DIR / 'images',\n",
    "    TACO_DIR / 'data',\n",
    "    TACO_DIR / 'TACO' / 'images',\n",
    "    TACO_DIR,\n",
    "]\n",
    "\n",
    "IMAGE_DIR = None\n",
    "for img_dir in possible_img_dirs:\n",
    "    if img_dir.exists():\n",
    "        # Check if directory contains image files\n",
    "        img_files = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))\n",
    "        if len(img_files) > 0:\n",
    "            IMAGE_DIR = img_dir\n",
    "            print(f\"âœ“ Found image directory: {IMAGE_DIR.name}\")\n",
    "            print(f\"  Total images: {len(img_files)}\")\n",
    "            break\n",
    "\n",
    "if not IMAGE_DIR:\n",
    "    print(\"âš ï¸ Image directory not found. Please verify dataset structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ad8fc",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 4.1 Load COCO Annotations\n",
    "\n",
    "Loading the TACO dataset annotations and extracting key statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cb016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO annotations\n",
    "if ANNOTATION_FILE and ANNOTATION_FILE.exists():\n",
    "    print(f\"Loading annotations from: {ANNOTATION_FILE.name}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize COCO API\n",
    "    coco = COCO(str(ANNOTATION_FILE))\n",
    "    \n",
    "    # Get all category IDs and image IDs\n",
    "    cat_ids = coco.getCatIds()\n",
    "    img_ids = coco.getImgIds()\n",
    "    ann_ids = coco.getAnnIds()\n",
    "    \n",
    "    # Load categories and images\n",
    "    categories = coco.loadCats(cat_ids)\n",
    "    images = coco.loadImgs(img_ids)\n",
    "    annotations = coco.loadAnns(ann_ids)\n",
    "    \n",
    "    print(f\"\\nâœ“ Successfully loaded TACO dataset!\")\n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total Images: {len(images)}\")\n",
    "    print(f\"  Total Annotations: {len(annotations)}\")\n",
    "    print(f\"  Total Categories: {len(categories)}\")\n",
    "    print(f\"  Average annotations per image: {len(annotations)/len(images):.2f}\")\n",
    "    \n",
    "    # Create category name mapping\n",
    "    cat_id_to_name = {cat['id']: cat['name'] for cat in categories}\n",
    "    cat_name_to_id = {cat['name']: cat['id'] for cat in categories}\n",
    "    \n",
    "    print(f\"\\nâœ“ Category mapping created\")\n",
    "    print(f\"  Sample categories: {list(cat_id_to_name.items())[:5]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Annotation file not found. Please download the TACO dataset first.\")\n",
    "    print(\"Stopping execution - dataset is required for EDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997480c",
   "metadata": {},
   "source": [
    "### 4.2 Compute Class Frequencies and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class frequencies\n",
    "class_counts = Counter()\n",
    "image_object_counts = defaultdict(int)\n",
    "\n",
    "for ann in annotations:\n",
    "    class_counts[ann['category_id']] += 1\n",
    "    image_object_counts[ann['image_id']] += 1\n",
    "\n",
    "# Convert to sorted list\n",
    "class_freq_list = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "class_freq_df = pd.DataFrame([\n",
    "    {\n",
    "        'Category ID': cat_id,\n",
    "        'Category Name': cat_id_to_name.get(cat_id, 'Unknown'),\n",
    "        'Count': count,\n",
    "        'Percentage': (count / len(annotations)) * 100\n",
    "    }\n",
    "    for cat_id, count in class_freq_list\n",
    "])\n",
    "\n",
    "print(\"Class Frequency Distribution (Top 20):\")\n",
    "print(\"=\" * 70)\n",
    "print(class_freq_df.head(20).to_string(index=False))\n",
    "\n",
    "print(f\"\\n\\nTop 5 Most Frequent Classes (IDs 0-4 as required):\")\n",
    "print(\"=\" * 70)\n",
    "top_5_classes = class_freq_list[:5]\n",
    "for rank, (cat_id, count) in enumerate(top_5_classes, 1):\n",
    "    cat_name = cat_id_to_name.get(cat_id, 'Unknown')\n",
    "    percentage = (count / len(annotations)) * 100\n",
    "    print(f\"{rank}. ID {cat_id}: {cat_name:30s} - {count:5d} annotations ({percentage:5.2f}%)\")\n",
    "\n",
    "# Store top 5 class IDs for later use\n",
    "TOP_5_CLASS_IDS = [cat_id for cat_id, _ in top_5_classes]\n",
    "print(f\"\\nâœ“ Top 5 class IDs stored: {TOP_5_CLASS_IDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c412e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze objects per image statistics\n",
    "objects_per_img = list(image_object_counts.values())\n",
    "\n",
    "print(\"Objects per Image Statistics:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Mean objects per image: {np.mean(objects_per_img):.2f}\")\n",
    "print(f\"  Median objects per image: {np.median(objects_per_img):.2f}\")\n",
    "print(f\"  Std deviation: {np.std(objects_per_img):.2f}\")\n",
    "print(f\"  Min objects in an image: {min(objects_per_img)}\")\n",
    "print(f\"  Max objects in an image: {max(objects_per_img)}\")\n",
    "print(f\"  Total images: {len(image_object_counts)}\")\n",
    "\n",
    "# Distribution breakdown\n",
    "dist_breakdown = Counter(objects_per_img)\n",
    "print(f\"\\nDistribution breakdown:\")\n",
    "for num_objs in sorted(dist_breakdown.keys())[:10]:\n",
    "    count = dist_breakdown[num_objs]\n",
    "    print(f\"  {num_objs} object(s): {count} images ({count/len(images)*100:.1f}%)\")\n",
    "if len(dist_breakdown) > 10:\n",
    "    print(f\"  ... and {len(dist_breakdown) - 10} more categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0438843f",
   "metadata": {},
   "source": [
    "### 4.3 Visualization: Class Distribution Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart for top 20 classes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Plot 1: Top 20 classes\n",
    "top_20_df = class_freq_df.head(20)\n",
    "bars1 = ax1.barh(range(len(top_20_df)), top_20_df['Count'], color='steelblue')\n",
    "\n",
    "# Highlight top 5 classes\n",
    "for i in range(min(5, len(top_20_df))):\n",
    "    bars1[i].set_color('coral')\n",
    "    bars1[i].set_edgecolor('darkred')\n",
    "    bars1[i].set_linewidth(2)\n",
    "\n",
    "ax1.set_yticks(range(len(top_20_df)))\n",
    "ax1.set_yticklabels([f\"ID {row['Category ID']}: {row['Category Name'][:25]}\" \n",
    "                      for _, row in top_20_df.iterrows()])\n",
    "ax1.set_xlabel('Number of Annotations', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Top 20 Classes by Frequency\\n(Top 5 highlighted in coral)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, (_, row) in enumerate(top_20_df.iterrows()):\n",
    "    ax1.text(row['Count'], i, f\" {row['Count']}\", \n",
    "             va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 2: Top 5 classes only (required subset)\n",
    "top_5_df = class_freq_df.head(5)\n",
    "bars2 = ax2.bar(range(len(top_5_df)), top_5_df['Count'], \n",
    "                color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8'],\n",
    "                edgecolor='black', linewidth=2)\n",
    "\n",
    "ax2.set_xticks(range(len(top_5_df)))\n",
    "ax2.set_xticklabels([f\"ID {row['Category ID']}\\n{row['Category Name'][:20]}\" \n",
    "                      for _, row in top_5_df.iterrows()], rotation=45, ha='right')\n",
    "ax2.set_ylabel('Number of Annotations', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Top 5 Most Frequent Classes (Subset for Training)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count and percentage labels\n",
    "for i, (_, row) in enumerate(top_5_df.iterrows()):\n",
    "    ax2.text(i, row['Count'], f\"{row['Count']}\\n({row['Percentage']:.1f}%)\", \n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RUNS_DIR / 'class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Class distribution chart saved to: {RUNS_DIR / 'class_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be04759",
   "metadata": {},
   "source": [
    "### 4.4 Visualization: Objects per Image Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram of objects per image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Histogram with all data\n",
    "ax1.hist(objects_per_img, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(np.mean(objects_per_img), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {np.mean(objects_per_img):.2f}')\n",
    "ax1.axvline(np.median(objects_per_img), color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Median: {np.median(objects_per_img):.2f}')\n",
    "ax1.set_xlabel('Number of Objects per Image', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Frequency (Number of Images)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distribution of Objects per Image (All Data)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Box plot for better visualization of outliers\n",
    "ax2.boxplot(objects_per_img, vert=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightcoral', alpha=0.7),\n",
    "            medianprops=dict(color='darkred', linewidth=2),\n",
    "            whiskerprops=dict(linewidth=1.5),\n",
    "            capprops=dict(linewidth=1.5))\n",
    "ax2.set_ylabel('Number of Objects per Image', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Box Plot: Objects per Image\\n(Shows outliers and quartiles)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add statistics text\n",
    "stats_text = f\"Min: {min(objects_per_img)}\\n\"\n",
    "stats_text += f\"Q1: {np.percentile(objects_per_img, 25):.1f}\\n\"\n",
    "stats_text += f\"Median: {np.median(objects_per_img):.1f}\\n\"\n",
    "stats_text += f\"Q3: {np.percentile(objects_per_img, 75):.1f}\\n\"\n",
    "stats_text += f\"Max: {max(objects_per_img)}\"\n",
    "ax2.text(1.15, np.median(objects_per_img), stats_text, \n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "         fontsize=10, verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RUNS_DIR / 'objects_per_image_histogram.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Objects per image histogram saved to: {RUNS_DIR / 'objects_per_image_histogram.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ffc6b",
   "metadata": {},
   "source": [
    "### 4.5 Visualization: Sample Images with Bounding Boxes and Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f510864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to visualize image with annotations\n",
    "def visualize_annotations(img_id, coco_obj, img_dir, ax_bbox=None, ax_mask=None):\n",
    "    \"\"\"\n",
    "    Visualize bounding boxes and segmentation masks for a given image.\n",
    "    \n",
    "    Args:\n",
    "        img_id: COCO image ID\n",
    "        coco_obj: COCO API object\n",
    "        img_dir: Path to image directory\n",
    "        ax_bbox: Matplotlib axis for bounding box visualization\n",
    "        ax_mask: Matplotlib axis for mask visualization\n",
    "    \"\"\"\n",
    "    # Load image info\n",
    "    img_info = coco_obj.loadImgs(img_id)[0]\n",
    "    img_path = img_dir / img_info['file_name']\n",
    "    \n",
    "    # Check if image exists\n",
    "    if not img_path.exists():\n",
    "        print(f\"Warning: Image not found: {img_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Load image\n",
    "    img = cv2.imread(str(img_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get annotations for this image\n",
    "    ann_ids = coco_obj.getAnnIds(imgIds=img_id)\n",
    "    anns = coco_obj.loadAnns(ann_ids)\n",
    "    \n",
    "    # Visualization with bounding boxes\n",
    "    if ax_bbox is not None:\n",
    "        ax_bbox.imshow(img)\n",
    "        ax_bbox.axis('off')\n",
    "        \n",
    "        for ann in anns:\n",
    "            # Draw bounding box\n",
    "            bbox = ann['bbox']  # [x, y, width, height]\n",
    "            rect = patches.Rectangle(\n",
    "                (bbox[0], bbox[1]), bbox[2], bbox[3],\n",
    "                linewidth=2, edgecolor='lime', facecolor='none'\n",
    "            )\n",
    "            ax_bbox.add_patch(rect)\n",
    "            \n",
    "            # Add label\n",
    "            cat_name = cat_id_to_name.get(ann['category_id'], 'Unknown')\n",
    "            ax_bbox.text(\n",
    "                bbox[0], bbox[1] - 5,\n",
    "                f\"{cat_name[:15]}\", \n",
    "                color='white', fontsize=8, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='lime', alpha=0.7)\n",
    "            )\n",
    "        \n",
    "        ax_bbox.set_title(f\"Bounding Boxes\\n{img_info['file_name']}\\n{len(anns)} objects\", \n",
    "                         fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Visualization with segmentation masks\n",
    "    if ax_mask is not None:\n",
    "        ax_mask.imshow(img)\n",
    "        ax_mask.axis('off')\n",
    "        \n",
    "        # Create colored mask overlay\n",
    "        mask_img = np.zeros_like(img, dtype=np.float32)\n",
    "        \n",
    "        for idx, ann in enumerate(anns):\n",
    "            if 'segmentation' in ann and ann['segmentation']:\n",
    "                # Generate random color for this annotation\n",
    "                color = np.random.rand(3)\n",
    "                \n",
    "                # Handle different segmentation formats\n",
    "                if isinstance(ann['segmentation'], list):\n",
    "                    # Polygon format\n",
    "                    for seg in ann['segmentation']:\n",
    "                        poly = np.array(seg).reshape(-1, 2).astype(np.int32)\n",
    "                        cv2.fillPoly(mask_img, [poly], color)\n",
    "                elif isinstance(ann['segmentation'], dict):\n",
    "                    # RLE format\n",
    "                    rle = ann['segmentation']\n",
    "                    mask = coco_mask.decode(rle)\n",
    "                    mask_img[mask > 0] = color\n",
    "        \n",
    "        # Overlay mask on image\n",
    "        alpha = 0.5\n",
    "        overlaid = (img * (1 - alpha) + mask_img * 255 * alpha).astype(np.uint8)\n",
    "        ax_mask.imshow(overlaid)\n",
    "        \n",
    "        ax_mask.set_title(f\"Segmentation Masks\\n{img_info['file_name']}\\n{len(anns)} objects\", \n",
    "                         fontsize=10, fontweight='bold')\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"âœ“ Visualization helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03041d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and visualize 12 example images (2 rows x 6 images)\n",
    "# Try to get images with varying number of objects\n",
    "sample_img_ids = []\n",
    "for target_count in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "    # Find images with approximately this many objects\n",
    "    candidates = [img_id for img_id, count in image_object_counts.items() \n",
    "                 if abs(count - target_count) <= 2]\n",
    "    if candidates:\n",
    "        sample_img_ids.append(random.choice(candidates))\n",
    "    \n",
    "    if len(sample_img_ids) >= 12:\n",
    "        break\n",
    "\n",
    "# If we don't have enough, just take random ones\n",
    "if len(sample_img_ids) < 12:\n",
    "    remaining = 12 - len(sample_img_ids)\n",
    "    additional = random.sample(list(image_object_counts.keys()), remaining)\n",
    "    sample_img_ids.extend(additional)\n",
    "\n",
    "sample_img_ids = sample_img_ids[:12]\n",
    "\n",
    "print(f\"Selected {len(sample_img_ids)} sample images for visualization\")\n",
    "print(f\"Sample image IDs: {sample_img_ids[:6]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with bounding boxes\n",
    "fig, axes = plt.subplots(2, 6, figsize=(24, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "print(\"Visualizing bounding boxes...\")\n",
    "for idx, img_id in enumerate(sample_img_ids):\n",
    "    visualize_annotations(img_id, coco, IMAGE_DIR, ax_bbox=axes[idx])\n",
    "\n",
    "plt.suptitle('Sample Images with Bounding Boxes', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RUNS_DIR / 'sample_images_bboxes.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Bounding box visualization saved to: {RUNS_DIR / 'sample_images_bboxes.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aec710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with segmentation masks\n",
    "fig, axes = plt.subplots(2, 6, figsize=(24, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "print(\"Visualizing segmentation masks...\")\n",
    "for idx, img_id in enumerate(sample_img_ids):\n",
    "    visualize_annotations(img_id, coco, IMAGE_DIR, ax_mask=axes[idx])\n",
    "\n",
    "plt.suptitle('Sample Images with Segmentation Masks', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RUNS_DIR / 'sample_images_masks.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Segmentation mask visualization saved to: {RUNS_DIR / 'sample_images_masks.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932ea04",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Data Preprocessing & Subset Creation\n",
    "\n",
    "### 5.1 Why Use Only 5 Classes?\n",
    "\n",
    "**Justification for selecting top 5 classes:**\n",
    "\n",
    "1. **Less Label Noise**: Focusing on the most frequent classes reduces annotation inconsistencies and mislabeling that are more common in rare classes.\n",
    "\n",
    "2. **More Samples Per Class**: Top 5 classes have significantly more training examples, enabling better model learning and generalization.\n",
    "\n",
    "3. **Simpler Decision Boundary**: Fewer classes reduce inter-class confusion and make it easier for the model to learn discriminative features.\n",
    "\n",
    "4. **Balanced Training**: The top 5 classes together represent a substantial portion of the dataset, providing sufficient diversity while maintaining focus.\n",
    "\n",
    "5. **Computational Efficiency**: Training on fewer classes speeds up experimentation and allows for more thorough hyperparameter tuning.\n",
    "\n",
    "6. **Better Evaluation Metrics**: With more samples per class, evaluation metrics (Precision, Recall, mAP, IoU) are more reliable and statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f071ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.2 Filter Dataset to Top 5 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10003c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter annotations to keep only top 5 classes\n",
    "filtered_annotations = [ann for ann in annotations if ann['category_id'] in TOP_5_CLASS_IDS]\n",
    "\n",
    "# Get image IDs that have at least one annotation from top 5 classes\n",
    "filtered_img_ids = set(ann['image_id'] for ann in filtered_annotations)\n",
    "\n",
    "# Filter images\n",
    "filtered_images = [img for img in images if img['id'] in filtered_img_ids]\n",
    "\n",
    "# Filter categories to top 5\n",
    "filtered_categories = [cat for cat in categories if cat['id'] in TOP_5_CLASS_IDS]\n",
    "\n",
    "print(\"Subset Filtering Results:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Original dataset:\")\n",
    "print(f\"  Images: {len(images)}\")\n",
    "print(f\"  Annotations: {len(annotations)}\")\n",
    "print(f\"  Categories: {len(categories)}\")\n",
    "print(f\"\\nFiltered subset (Top 5 classes):\")\n",
    "print(f\"  Images: {len(filtered_images)} ({len(filtered_images)/len(images)*100:.1f}%)\")\n",
    "print(f\"  Annotations: {len(filtered_annotations)} ({len(filtered_annotations)/len(annotations)*100:.1f}%)\")\n",
    "print(f\"  Categories: {len(filtered_categories)}\")\n",
    "print(f\"\\nTop 5 classes retained:\")\n",
    "for cat_id in TOP_5_CLASS_IDS:\n",
    "    cat_name = cat_id_to_name.get(cat_id, 'Unknown')\n",
    "    count = sum(1 for ann in filtered_annotations if ann['category_id'] == cat_id)\n",
    "    print(f\"  ID {cat_id}: {cat_name:30s} - {count:5d} annotations\")\n",
    "\n",
    "# Compute statistics for filtered dataset\n",
    "filtered_img_obj_counts = Counter()\n",
    "for ann in filtered_annotations:\n",
    "    filtered_img_obj_counts[ann['image_id']] += 1\n",
    "\n",
    "print(f\"\\nFiltered dataset statistics:\")\n",
    "print(f\"  Avg objects per image: {len(filtered_annotations)/len(filtered_images):.2f}\")\n",
    "print(f\"  Min objects per image: {min(filtered_img_obj_counts.values())}\")\n",
    "print(f\"  Max objects per image: {max(filtered_img_obj_counts.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89610864",
   "metadata": {},
   "source": [
    "### 5.3 Create TACO Subset Directory and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5636eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset directory structure\n",
    "SUBSET_DIR = DATA_DIR / 'taco_subset'\n",
    "SUBSET_IMAGES_DIR = SUBSET_DIR / 'images'\n",
    "SUBSET_ANN_FILE = SUBSET_DIR / 'annotations.json'\n",
    "\n",
    "# Create directories\n",
    "SUBSET_DIR.mkdir(exist_ok=True)\n",
    "SUBSET_IMAGES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Created subset directory: {SUBSET_DIR}\")\n",
    "print(f\"Images will be stored in: {SUBSET_IMAGES_DIR}\")\n",
    "print(f\"Annotations will be saved to: {SUBSET_ANN_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ebf075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy filtered images to subset directory\n",
    "print(\"Copying images to subset directory...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "copied_count = 0\n",
    "failed_count = 0\n",
    "copied_files = []\n",
    "\n",
    "for img_info in tqdm(filtered_images, desc=\"Copying images\"):\n",
    "    src_path = IMAGE_DIR / img_info['file_name']\n",
    "    dst_path = SUBSET_IMAGES_DIR / img_info['file_name']\n",
    "    \n",
    "    if src_path.exists():\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        copied_count += 1\n",
    "        copied_files.append(img_info['file_name'])\n",
    "    else:\n",
    "        print(f\"Warning: Source image not found: {src_path}\")\n",
    "        failed_count += 1\n",
    "\n",
    "print(f\"\\nâœ“ Image copying complete!\")\n",
    "print(f\"  Successfully copied: {copied_count} images\")\n",
    "if failed_count > 0:\n",
    "    print(f\"  Failed to copy: {failed_count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filtered COCO annotation JSON\n",
    "# Re-map annotation and image IDs to be sequential starting from 1\n",
    "new_img_id_map = {old_id: new_id for new_id, old_id in enumerate(sorted(filtered_img_ids), 1)}\n",
    "new_ann_id_map = {}\n",
    "\n",
    "# Create new annotations with remapped IDs\n",
    "new_annotations = []\n",
    "for new_ann_id, ann in enumerate(filtered_annotations, 1):\n",
    "    new_ann = ann.copy()\n",
    "    new_ann['id'] = new_ann_id\n",
    "    new_ann['image_id'] = new_img_id_map[ann['image_id']]\n",
    "    new_annotations.append(new_ann)\n",
    "    new_ann_id_map[ann['id']] = new_ann_id\n",
    "\n",
    "# Create new images with remapped IDs\n",
    "new_images = []\n",
    "for img in filtered_images:\n",
    "    new_img = img.copy()\n",
    "    new_img['id'] = new_img_id_map[img['id']]\n",
    "    new_images.append(new_img)\n",
    "\n",
    "# Sort by ID for consistency\n",
    "new_images.sort(key=lambda x: x['id'])\n",
    "new_annotations.sort(key=lambda x: x['id'])\n",
    "\n",
    "# Create new categories (keep original IDs for top 5)\n",
    "new_categories = filtered_categories\n",
    "\n",
    "# Build COCO JSON structure\n",
    "subset_coco_data = {\n",
    "    'info': {\n",
    "        'description': 'TACO Subset - Top 5 Most Frequent Classes',\n",
    "        'version': '1.0',\n",
    "        'year': 2025,\n",
    "        'contributor': 'Minahil Ali (22i-0849), Ayaan Khan (22i-0832)',\n",
    "        'date_created': '2025-11-16'\n",
    "    },\n",
    "    'licenses': [],\n",
    "    'images': new_images,\n",
    "    'annotations': new_annotations,\n",
    "    'categories': new_categories\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "with open(SUBSET_ANN_FILE, 'w') as f:\n",
    "    json.dump(subset_coco_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Subset annotations saved to: {SUBSET_ANN_FILE}\")\n",
    "print(f\"\\nSubset COCO JSON contains:\")\n",
    "print(f\"  Images: {len(new_images)}\")\n",
    "print(f\"  Annotations: {len(new_annotations)}\")\n",
    "print(f\"  Categories: {len(new_categories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac76a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset manifest CSV for easy reference\n",
    "manifest_data = []\n",
    "\n",
    "for img in new_images:\n",
    "    img_anns = [ann for ann in new_annotations if ann['image_id'] == img['id']]\n",
    "    \n",
    "    manifest_data.append({\n",
    "        'image_id': img['id'],\n",
    "        'file_name': img['file_name'],\n",
    "        'width': img.get('width', 'N/A'),\n",
    "        'height': img.get('height', 'N/A'),\n",
    "        'num_objects': len(img_anns),\n",
    "        'category_ids': ','.join(str(ann['category_id']) for ann in img_anns),\n",
    "        'category_names': ','.join(cat_id_to_name.get(ann['category_id'], 'Unknown') \n",
    "                                   for ann in img_anns)\n",
    "    })\n",
    "\n",
    "manifest_df = pd.DataFrame(manifest_data)\n",
    "manifest_csv_path = SUBSET_DIR / 'subset_manifest.csv'\n",
    "manifest_df.to_csv(manifest_csv_path, index=False)\n",
    "\n",
    "print(f\"âœ“ Subset manifest CSV saved to: {manifest_csv_path}\")\n",
    "print(f\"\\nManifest preview:\")\n",
    "print(manifest_df.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"SUBSET CREATION COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Location: {SUBSET_DIR}\")\n",
    "print(f\"Files:\")\n",
    "print(f\"  - annotations.json: {SUBSET_ANN_FILE}\")\n",
    "print(f\"  - subset_manifest.csv: {manifest_csv_path}\")\n",
    "print(f\"  - images/: {SUBSET_IMAGES_DIR} ({copied_count} images)\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816118d",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Data Augmentation\n",
    "\n",
    "### 6.1 Augmentation Strategy Overview\n",
    "\n",
    "Data augmentation is crucial for improving model generalization and preventing overfitting. We'll implement three augmentation pipelines:\n",
    "\n",
    "1. **no_aug**: Baseline with no augmentation (only resize and normalize)\n",
    "2. **aug_v1**: Moderate augmentation (recommended for training)\n",
    "3. **aug_v2**: Aggressive augmentation (for comparison)\n",
    "\n",
    "**Augmentation Techniques:**\n",
    "- Random horizontal/vertical flips\n",
    "- Color jitter (hue, saturation, value)\n",
    "- Brightness and contrast adjustments\n",
    "- Random rotation and affine transformations\n",
    "- Mosaic augmentation (4-image combination for YOLO)\n",
    "- Random crop and resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb68981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentation pipelines using albumentations\n",
    "print(\"Defining augmentation pipelines...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pipeline 1: No Augmentation (Baseline)\n",
    "no_aug = A.Compose([\n",
    "    A.Resize(640, 640),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n",
    "\n",
    "print(\"âœ“ no_aug pipeline created:\")\n",
    "print(\"  - Resize to 640x640\")\n",
    "print(\"  - Normalize (ImageNet stats)\")\n",
    "print(\"  - Convert to tensor\")\n",
    "\n",
    "# Pipeline 2: Moderate Augmentation (aug_v1)\n",
    "aug_v1 = A.Compose([\n",
    "    A.Resize(640, 640),\n",
    "    \n",
    "    # Geometric transforms\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.RandomRotate90(p=0.3),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n",
    "    \n",
    "    # Color transforms\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    \n",
    "    # Quality transforms\n",
    "    A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "    \n",
    "    # Normalization\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels'], min_visibility=0.3))\n",
    "\n",
    "print(\"\\nâœ“ aug_v1 pipeline created (Moderate):\")\n",
    "print(\"  - Geometric: H-flip, V-flip, Rotate90, ShiftScaleRotate\")\n",
    "print(\"  - Color: ColorJitter, BrightnessContrast, HueSaturationValue\")\n",
    "print(\"  - Quality: GaussianBlur, GaussNoise\")\n",
    "\n",
    "# Pipeline 3: Aggressive Augmentation (aug_v2)\n",
    "aug_v2 = A.Compose([\n",
    "    A.Resize(640, 640),\n",
    "    \n",
    "    # More aggressive geometric transforms\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.3),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.15, scale_limit=0.3, rotate_limit=30, p=0.7),\n",
    "    A.Affine(scale=(0.8, 1.2), translate_percent=0.1, rotate=(-30, 30), p=0.5),\n",
    "    \n",
    "    # Aggressive color transforms\n",
    "    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.15, p=0.7),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.6),\n",
    "    A.HueSaturationValue(hue_shift_limit=30, sat_shift_limit=40, val_shift_limit=30, p=0.6),\n",
    "    A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n",
    "    \n",
    "    # Quality and occlusion\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "    A.GaussNoise(var_limit=(10.0, 80.0), p=0.3),\n",
    "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n",
    "    \n",
    "    # Normalization\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels'], min_visibility=0.2))\n",
    "\n",
    "print(\"\\nâœ“ aug_v2 pipeline created (Aggressive):\")\n",
    "print(\"  - Geometric: More aggressive rotations, affine transforms\")\n",
    "print(\"  - Color: Stronger jitter, gamma adjustments\")\n",
    "print(\"  - Quality: More noise, blur, coarse dropout\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2f95e",
   "metadata": {},
   "source": [
    "### 6.2 Augmentation Visualization - Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to apply augmentation and visualize\n",
    "def visualize_augmentation(img_path, bboxes, class_labels, augmentation, title=\"\"):\n",
    "    \"\"\"\n",
    "    Apply augmentation to an image and visualize results.\n",
    "    \n",
    "    Args:\n",
    "        img_path: Path to image\n",
    "        bboxes: List of bounding boxes in COCO format [x, y, width, height]\n",
    "        class_labels: List of class labels for each bbox\n",
    "        augmentation: Albumentations transform\n",
    "        title: Title for the plot\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(str(img_path))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Apply augmentation\n",
    "    try:\n",
    "        transformed = augmentation(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "        aug_image = transformed['image']\n",
    "        aug_bboxes = transformed['bboxes']\n",
    "        aug_labels = transformed['class_labels']\n",
    "        \n",
    "        # Convert tensor back to numpy for visualization if needed\n",
    "        if isinstance(aug_image, torch.Tensor):\n",
    "            # Denormalize\n",
    "            aug_image = aug_image.permute(1, 2, 0).numpy()\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            aug_image = std * aug_image + mean\n",
    "            aug_image = np.clip(aug_image, 0, 1)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Original image\n",
    "        ax1.imshow(image)\n",
    "        for bbox, label in zip(bboxes, class_labels):\n",
    "            x, y, w, h = bbox\n",
    "            rect = patches.Rectangle((x, y), w, h, linewidth=2, \n",
    "                                     edgecolor='lime', facecolor='none')\n",
    "            ax1.add_patch(rect)\n",
    "            ax1.text(x, y-5, f\"ID {label}\", color='white', fontsize=9, \n",
    "                    fontweight='bold', bbox=dict(boxstyle='round', facecolor='lime', alpha=0.7))\n",
    "        ax1.set_title(f\"Original Image\\n{len(bboxes)} objects\", fontsize=12, fontweight='bold')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Augmented image\n",
    "        ax2.imshow(aug_image)\n",
    "        for bbox, label in zip(aug_bboxes, aug_labels):\n",
    "            x, y, w, h = bbox\n",
    "            rect = patches.Rectangle((x, y), w, h, linewidth=2, \n",
    "                                     edgecolor='coral', facecolor='none')\n",
    "            ax2.add_patch(rect)\n",
    "            ax2.text(x, y-5, f\"ID {label}\", color='white', fontsize=9, \n",
    "                    fontweight='bold', bbox=dict(boxstyle='round', facecolor='coral', alpha=0.7))\n",
    "        ax2.set_title(f\"Augmented Image\\n{len(aug_bboxes)} objects retained\", \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt.suptitle(title, fontsize=14, fontweight='bold', y=0.98)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return fig, (image, aug_image), (bboxes, aug_bboxes)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error applying augmentation: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "print(\"âœ“ Augmentation visualization helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23255edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sample images for augmentation visualization\n",
    "num_samples = 6\n",
    "sample_indices = random.sample(range(len(filtered_images)), num_samples)\n",
    "\n",
    "print(f\"Selected {num_samples} random images for augmentation visualization\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72868b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize aug_v1 (Moderate Augmentation)\n",
    "print(\"Applying aug_v1 (Moderate Augmentation) to sample images...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "aug_v1_results = []\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices[:3]):\n",
    "    img_info = filtered_images[sample_idx]\n",
    "    img_path = IMAGE_DIR / img_info['file_name']\n",
    "    \n",
    "    # Get annotations for this image\n",
    "    img_anns = [ann for ann in filtered_annotations if ann['image_id'] == img_info['id']]\n",
    "    bboxes = [ann['bbox'] for ann in img_anns]\n",
    "    labels = [ann['category_id'] for ann in img_anns]\n",
    "    \n",
    "    # Visualize\n",
    "    fig, images, boxes = visualize_augmentation(\n",
    "        img_path, bboxes, labels, aug_v1, \n",
    "        title=f\"Moderate Augmentation (aug_v1) - Sample {idx+1}\"\n",
    "    )\n",
    "    \n",
    "    if fig:\n",
    "        aug_v1_results.append((fig, images, boxes))\n",
    "        plt.savefig(RUNS_DIR / f'aug_v1_sample_{idx+1}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ aug_v1 visualizations saved to runs/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8704e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize aug_v2 (Aggressive Augmentation)\n",
    "print(\"Applying aug_v2 (Aggressive Augmentation) to sample images...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "aug_v2_results = []\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices[3:6]):\n",
    "    img_info = filtered_images[sample_idx]\n",
    "    img_path = IMAGE_DIR / img_info['file_name']\n",
    "    \n",
    "    # Get annotations for this image\n",
    "    img_anns = [ann for ann in filtered_annotations if ann['image_id'] == img_info['id']]\n",
    "    bboxes = [ann['bbox'] for ann in img_anns]\n",
    "    labels = [ann['category_id'] for ann in img_anns]\n",
    "    \n",
    "    # Visualize\n",
    "    fig, images, boxes = visualize_augmentation(\n",
    "        img_path, bboxes, labels, aug_v2, \n",
    "        title=f\"Aggressive Augmentation (aug_v2) - Sample {idx+1}\"\n",
    "    )\n",
    "    \n",
    "    if fig:\n",
    "        aug_v2_results.append((fig, images, boxes))\n",
    "        plt.savefig(RUNS_DIR / f'aug_v2_sample_{idx+1}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ aug_v2 visualizations saved to runs/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a5bc97",
   "metadata": {},
   "source": [
    "### 6.3 Custom Dataset Classes for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e1cc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for Object Detection (YOLO format)\n",
    "class TACODetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for TACO waste detection with augmentation support.\n",
    "    Returns images and bounding boxes in YOLO format.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, images, annotations, img_dir, transform=None, class_mapping=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: List of image dictionaries from COCO\n",
    "            annotations: List of annotation dictionaries from COCO\n",
    "            img_dir: Path to images directory\n",
    "            transform: Albumentations transform pipeline\n",
    "            class_mapping: Dict mapping category IDs to class indices\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Group annotations by image ID\n",
    "        self.img_annotations = defaultdict(list)\n",
    "        for ann in annotations:\n",
    "            self.img_annotations[ann['image_id']].append(ann)\n",
    "        \n",
    "        # Create class mapping if not provided\n",
    "        if class_mapping is None:\n",
    "            unique_classes = sorted(set(ann['category_id'] for ann in annotations))\n",
    "            self.class_mapping = {cat_id: idx for idx, cat_id in enumerate(unique_classes)}\n",
    "        else:\n",
    "            self.class_mapping = class_mapping\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self.images)} images\")\n",
    "        print(f\"Class mapping: {self.class_mapping}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image info\n",
    "        img_info = self.images[idx]\n",
    "        img_path = self.img_dir / img_info['file_name']\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get annotations\n",
    "        anns = self.img_annotations.get(img_info['id'], [])\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            bbox = ann['bbox']  # [x, y, width, height] in COCO format\n",
    "            bboxes.append(bbox)\n",
    "            labels.append(self.class_mapping[ann['category_id']])\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            try:\n",
    "                transformed = self.transform(image=image, bboxes=bboxes, class_labels=labels)\n",
    "                image = transformed['image']\n",
    "                bboxes = transformed['bboxes']\n",
    "                labels = transformed['class_labels']\n",
    "            except Exception as e:\n",
    "                # Fallback: return original if transform fails\n",
    "                print(f\"Transform failed for {img_path}: {e}\")\n",
    "                pass\n",
    "        \n",
    "        # Convert bboxes to tensor [N, 4]\n",
    "        if len(bboxes) > 0:\n",
    "            bboxes = torch.tensor(bboxes, dtype=torch.float32)\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "        else:\n",
    "            bboxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'bboxes': bboxes,\n",
    "            'labels': labels,\n",
    "            'image_id': img_info['id']\n",
    "        }\n",
    "\n",
    "print(\"âœ“ TACODetectionDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea584d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for Semantic Segmentation (U-Net)\n",
    "class TACOSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for TACO waste segmentation with augmentation support.\n",
    "    Returns images and segmentation masks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, images, annotations, img_dir, transform=None, class_mapping=None, mask_size=(640, 640)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: List of image dictionaries from COCO\n",
    "            annotations: List of annotation dictionaries from COCO\n",
    "            img_dir: Path to images directory\n",
    "            transform: Albumentations transform pipeline\n",
    "            class_mapping: Dict mapping category IDs to class indices\n",
    "            mask_size: Size of output mask\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "        self.mask_size = mask_size\n",
    "        \n",
    "        # Group annotations by image ID\n",
    "        self.img_annotations = defaultdict(list)\n",
    "        for ann in annotations:\n",
    "            self.img_annotations[ann['image_id']].append(ann)\n",
    "        \n",
    "        # Create class mapping\n",
    "        if class_mapping is None:\n",
    "            unique_classes = sorted(set(ann['category_id'] for ann in annotations))\n",
    "            self.class_mapping = {cat_id: idx + 1 for idx, cat_id in enumerate(unique_classes)}\n",
    "            self.class_mapping[0] = 0  # Background class\n",
    "        else:\n",
    "            self.class_mapping = class_mapping\n",
    "        \n",
    "        self.num_classes = len(self.class_mapping)\n",
    "        \n",
    "        print(f\"Segmentation dataset initialized with {len(self.images)} images\")\n",
    "        print(f\"Number of classes: {self.num_classes} (including background)\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def create_mask(self, anns, img_shape):\n",
    "        \"\"\"Create segmentation mask from annotations.\"\"\"\n",
    "        mask = np.zeros(img_shape[:2], dtype=np.uint8)\n",
    "        \n",
    "        for ann in anns:\n",
    "            if 'segmentation' not in ann or not ann['segmentation']:\n",
    "                # Use bbox if no segmentation\n",
    "                bbox = ann['bbox']\n",
    "                x, y, w, h = map(int, bbox)\n",
    "                class_idx = self.class_mapping[ann['category_id']]\n",
    "                mask[y:y+h, x:x+w] = class_idx\n",
    "            else:\n",
    "                # Use segmentation polygon\n",
    "                class_idx = self.class_mapping[ann['category_id']]\n",
    "                seg = ann['segmentation']\n",
    "                \n",
    "                if isinstance(seg, list):\n",
    "                    # Polygon format\n",
    "                    for poly in seg:\n",
    "                        poly_array = np.array(poly).reshape(-1, 2).astype(np.int32)\n",
    "                        cv2.fillPoly(mask, [poly_array], class_idx)\n",
    "                elif isinstance(seg, dict):\n",
    "                    # RLE format\n",
    "                    rle_mask = coco_mask.decode(seg)\n",
    "                    mask[rle_mask > 0] = class_idx\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image info\n",
    "        img_info = self.images[idx]\n",
    "        img_path = self.img_dir / img_info['file_name']\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get annotations and create mask\n",
    "        anns = self.img_annotations.get(img_info['id'], [])\n",
    "        mask = self.create_mask(anns, image.shape)\n",
    "        \n",
    "        # Apply transforms (both image and mask)\n",
    "        if self.transform:\n",
    "            # For segmentation, we need different transform without bbox params\n",
    "            try:\n",
    "                transformed = self.transform(image=image, mask=mask)\n",
    "                image = transformed['image']\n",
    "                mask = transformed['mask']\n",
    "            except Exception as e:\n",
    "                print(f\"Transform failed for {img_path}: {e}\")\n",
    "        \n",
    "        # Convert mask to tensor\n",
    "        mask = torch.tensor(mask, dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'mask': mask,\n",
    "            'image_id': img_info['id']\n",
    "        }\n",
    "\n",
    "print(\"âœ“ TACOSegmentationDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed821058",
   "metadata": {},
   "source": [
    "### 6.4 Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc69069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "print(\"Splitting dataset into train and validation sets...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use 80/20 split\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "\n",
    "# Perform split with fixed random seed for reproducibility\n",
    "train_images, val_images = train_test_split(\n",
    "    filtered_images, \n",
    "    test_size=val_ratio, \n",
    "    random_state=RANDOM_SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Get corresponding annotations\n",
    "train_img_ids = set(img['id'] for img in train_images)\n",
    "val_img_ids = set(img['id'] for img in val_images)\n",
    "\n",
    "train_annotations = [ann for ann in filtered_annotations if ann['image_id'] in train_img_ids]\n",
    "val_annotations = [ann for ann in filtered_annotations if ann['image_id'] in val_img_ids]\n",
    "\n",
    "print(f\"Dataset split (seed={RANDOM_SEED}):\")\n",
    "print(f\"  Train: {len(train_images)} images, {len(train_annotations)} annotations\")\n",
    "print(f\"  Val:   {len(val_images)} images, {len(val_annotations)} annotations\")\n",
    "print(f\"  Ratio: {len(train_images)/len(filtered_images)*100:.1f}% / {len(val_images)/len(filtered_images)*100:.1f}%\")\n",
    "\n",
    "# Verify no overlap\n",
    "assert len(train_img_ids.intersection(val_img_ids)) == 0, \"Train/val split has overlap!\"\n",
    "print(\"\\nâœ“ Train/val split verified (no overlap)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d250c32",
   "metadata": {},
   "source": [
    "### 6.5 Experiment Tracking Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51936fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment tracking DataFrame\n",
    "experiments_data = {\n",
    "    'experiment_id': [],\n",
    "    'model_type': [],  # 'YOLO' or 'UNet'\n",
    "    'augmentation': [],  # 'no_aug', 'aug_v1', 'aug_v2'\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'mAP50': [],\n",
    "    'mAP50_95': [],\n",
    "    'iou': [],  # For segmentation\n",
    "    'dice': [],  # For segmentation\n",
    "    'train_time': [],  # seconds\n",
    "    'epochs': [],\n",
    "    'notes': []\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "aug_experiments_df = pd.DataFrame(experiments_data)\n",
    "\n",
    "# Save initial empty DataFrame\n",
    "aug_experiments_df.to_csv(RUNS_DIR / 'aug_experiments.csv', index=False)\n",
    "\n",
    "print(\"Experiment tracking initialized\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Tracking file: {RUNS_DIR / 'aug_experiments.csv'}\")\n",
    "print(\"\\nPlanned experiments:\")\n",
    "print(\"  1. YOLO + no_aug (baseline)\")\n",
    "print(\"  2. YOLO + aug_v1 (moderate)\")\n",
    "print(\"  3. YOLO + aug_v2 (aggressive) [optional]\")\n",
    "print(\"  4. U-Net + no_aug (baseline)\")\n",
    "print(\"  5. U-Net + aug_v1 (moderate)\")\n",
    "print(\"  6. U-Net + aug_v2 (aggressive) [optional]\")\n",
    "print(\"\\nMetrics to track:\")\n",
    "print(\"  YOLO: Precision, Recall, mAP@50, mAP@50-95\")\n",
    "print(\"  U-Net: IoU, Dice Score\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bdf739",
   "metadata": {},
   "source": [
    "### 6.6 Summary of Augmentation Strategy\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "1. **Three Pipelines Defined:**\n",
    "   - `no_aug`: Baseline (resize + normalize only)\n",
    "   - `aug_v1`: Moderate augmentation (recommended)\n",
    "   - `aug_v2`: Aggressive augmentation (for ablation)\n",
    "\n",
    "2. **Reproducibility:**\n",
    "   - Fixed random seed (42) across all operations\n",
    "   - Deterministic transforms\n",
    "   - Consistent train/val split\n",
    "\n",
    "3. **Dataset Classes:**\n",
    "   - `TACODetectionDataset`: For YOLO training with bounding boxes\n",
    "   - `TACOSegmentationDataset`: For U-Net training with masks\n",
    "   - Both support augmentation pipelines\n",
    "\n",
    "4. **Experiment Plan:**\n",
    "   - Train YOLO and U-Net with `no_aug` (baseline)\n",
    "   - Train YOLO and U-Net with `aug_v1` (compare performance)\n",
    "   - Optional: Train with `aug_v2` for ablation study\n",
    "   - Track all metrics in `aug_experiments.csv`\n",
    "\n",
    "5. **Expected Benefits:**\n",
    "   - Improved generalization\n",
    "   - Reduced overfitting\n",
    "   - Better performance on unseen data\n",
    "   - More robust models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191cc86b",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. YOLOv8 Object Detection\n",
    "\n",
    "### 7.1 Dataset Preparation for YOLO Format\n",
    "\n",
    "YOLOv8 requires a specific directory structure and annotation format:\n",
    "- Images and labels in separate directories\n",
    "- Labels in YOLO format: `class x_center y_center width height` (normalized 0-1)\n",
    "- `data.yaml` configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create YOLO format dataset directory structure\n",
    "YOLO_DIR = DATA_DIR / 'yolo_dataset'\n",
    "YOLO_TRAIN_IMAGES = YOLO_DIR / 'images' / 'train'\n",
    "YOLO_VAL_IMAGES = YOLO_DIR / 'images' / 'val'\n",
    "YOLO_TRAIN_LABELS = YOLO_DIR / 'labels' / 'train'\n",
    "YOLO_VAL_LABELS = YOLO_DIR / 'labels' / 'val'\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [YOLO_TRAIN_IMAGES, YOLO_VAL_IMAGES, YOLO_TRAIN_LABELS, YOLO_VAL_LABELS]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"YOLO dataset directory structure created:\")\n",
    "print(f\"  {YOLO_DIR}\")\n",
    "print(f\"    images/\")\n",
    "print(f\"      train/ ({YOLO_TRAIN_IMAGES})\")\n",
    "print(f\"      val/   ({YOLO_VAL_IMAGES})\")\n",
    "print(f\"    labels/\")\n",
    "print(f\"      train/ ({YOLO_TRAIN_LABELS})\")\n",
    "print(f\"      val/   ({YOLO_VAL_LABELS})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coco_to_yolo_bbox(coco_bbox, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert COCO bbox format [x, y, width, height] to YOLO format.\n",
    "    YOLO format: [x_center, y_center, width, height] (all normalized 0-1)\n",
    "    \n",
    "    Args:\n",
    "        coco_bbox: [x, y, width, height] in pixels\n",
    "        img_width: Image width in pixels\n",
    "        img_height: Image height in pixels\n",
    "    \n",
    "    Returns:\n",
    "        [x_center, y_center, width, height] normalized\n",
    "    \"\"\"\n",
    "    x, y, w, h = coco_bbox\n",
    "    x_center = (x + w / 2) / img_width\n",
    "    y_center = (y + h / 2) / img_height\n",
    "    norm_w = w / img_width\n",
    "    norm_h = h / img_height\n",
    "    return [x_center, y_center, norm_w, norm_h]\n",
    "\n",
    "\n",
    "def create_yolo_annotations(coco_data, image_list, img_dir, label_dir, class_mapping):\n",
    "    \"\"\"\n",
    "    Create YOLO format annotation files from COCO data.\n",
    "    \n",
    "    Args:\n",
    "        coco_data: COCO annotations dictionary\n",
    "        image_list: List of image IDs to process\n",
    "        img_dir: Destination directory for images\n",
    "        label_dir: Destination directory for labels\n",
    "        class_mapping: Dict mapping original class IDs to 0-indexed IDs\n",
    "    \"\"\"\n",
    "    img_id_to_info = {img['id']: img for img in coco_data['images']}\n",
    "    \n",
    "    # Group annotations by image\n",
    "    img_to_annots = {}\n",
    "    for ann in coco_data['annotations']:\n",
    "        img_id = ann['image_id']\n",
    "        if img_id not in img_to_annots:\n",
    "            img_to_annots[img_id] = []\n",
    "        img_to_annots[img_id].append(ann)\n",
    "    \n",
    "    created_count = 0\n",
    "    for img_id in tqdm(image_list, desc=\"Creating YOLO annotations\"):\n",
    "        img_info = img_id_to_info[img_id]\n",
    "        img_filename = img_info['file_name']\n",
    "        img_width = img_info['width']\n",
    "        img_height = img_info['height']\n",
    "        \n",
    "        # Copy image to YOLO directory\n",
    "        src_img = SUBSET_DIR / img_filename\n",
    "        dst_img = img_dir / img_filename\n",
    "        if src_img.exists():\n",
    "            shutil.copy(src_img, dst_img)\n",
    "        \n",
    "        # Create YOLO label file\n",
    "        label_filename = Path(img_filename).stem + '.txt'\n",
    "        label_path = label_dir / label_filename\n",
    "        \n",
    "        yolo_annotations = []\n",
    "        if img_id in img_to_annots:\n",
    "            for ann in img_to_annots[img_id]:\n",
    "                cat_id = ann['category_id']\n",
    "                if cat_id not in class_mapping:\n",
    "                    continue\n",
    "                \n",
    "                # Convert to 0-indexed class ID\n",
    "                yolo_class_id = class_mapping[cat_id]\n",
    "                \n",
    "                # Convert bbox to YOLO format\n",
    "                coco_bbox = ann['bbox']\n",
    "                yolo_bbox = convert_coco_to_yolo_bbox(coco_bbox, img_width, img_height)\n",
    "                \n",
    "                # YOLO format: class x_center y_center width height\n",
    "                yolo_line = f\"{yolo_class_id} {yolo_bbox[0]:.6f} {yolo_bbox[1]:.6f} {yolo_bbox[2]:.6f} {yolo_bbox[3]:.6f}\"\n",
    "                yolo_annotations.append(yolo_line)\n",
    "        \n",
    "        # Write annotations to file\n",
    "        with open(label_path, 'w') as f:\n",
    "            f.write('\\n'.join(yolo_annotations))\n",
    "        \n",
    "        created_count += 1\n",
    "    \n",
    "    return created_count\n",
    "\n",
    "print(\"YOLO conversion functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class mapping from original IDs to 0-indexed IDs\n",
    "class_mapping = {cat_id: idx for idx, cat_id in enumerate(TOP_5_CLASS_IDS)}\n",
    "print(\"Class mapping (original_id -> yolo_id):\")\n",
    "for orig_id, yolo_id in class_mapping.items():\n",
    "    cat_name = coco.loadCats(orig_id)[0]['name']\n",
    "    print(f\"  {orig_id} -> {yolo_id}: {cat_name}\")\n",
    "\n",
    "# Convert train and val sets to YOLO format\n",
    "print(f\"\\nConverting train set ({len(train_ids)} images)...\")\n",
    "train_count = create_yolo_annotations(\n",
    "    filtered_annotations,\n",
    "    train_ids,\n",
    "    YOLO_TRAIN_IMAGES,\n",
    "    YOLO_TRAIN_LABELS,\n",
    "    class_mapping\n",
    ")\n",
    "\n",
    "print(f\"\\nConverting val set ({len(val_ids)} images)...\")\n",
    "val_count = create_yolo_annotations(\n",
    "    filtered_annotations,\n",
    "    val_ids,\n",
    "    YOLO_VAL_IMAGES,\n",
    "    YOLO_VAL_LABELS,\n",
    "    class_mapping\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ YOLO dataset created successfully!\")\n",
    "print(f\"  Train: {train_count} images + labels\")\n",
    "print(f\"  Val:   {val_count} images + labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5cb3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data.yaml configuration file for YOLOv8\n",
    "data_yaml_path = YOLO_DIR / 'data.yaml'\n",
    "\n",
    "# Get class names in order\n",
    "class_names = [coco.loadCats(cat_id)[0]['name'] for cat_id in TOP_5_CLASS_IDS]\n",
    "\n",
    "yaml_content = f\"\"\"# TACO Dataset - Top 5 Classes for YOLOv8 Detection\n",
    "path: {YOLO_DIR.absolute()}\n",
    "train: images/train\n",
    "val: images/val\n",
    "\n",
    "# Number of classes\n",
    "nc: {len(TOP_5_CLASS_IDS)}\n",
    "\n",
    "# Class names\n",
    "names: {class_names}\n",
    "\"\"\"\n",
    "\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(f\"âœ“ Created data.yaml at {data_yaml_path}\")\n",
    "print(\"\\nContents:\")\n",
    "print(yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2123fa0",
   "metadata": {},
   "source": [
    "### 7.2 YOLOv8 Model Configuration and Training\n",
    "\n",
    "We'll train YOLOv8 from scratch (random initialization) with two configurations:\n",
    "1. **Baseline (no_aug)**: No augmentation to establish performance baseline\n",
    "2. **Augmented (aug_v1)**: With geometric and color augmentations\n",
    "\n",
    "**Hyperparameters:**\n",
    "- Model: YOLOv8n (nano - smallest, fastest)\n",
    "- Epochs: 50\n",
    "- Batch size: 16\n",
    "- Image size: 640x640\n",
    "- Optimizer: SGD with momentum=0.937\n",
    "- Learning rate: 0.01 (initial)\n",
    "- Weight decay: 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395f30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Define training hyperparameters\n",
    "YOLO_HYPERPARAMS = {\n",
    "    'epochs': 50,\n",
    "    'batch': 16,\n",
    "    'imgsz': 640,\n",
    "    'optimizer': 'SGD',\n",
    "    'lr0': 0.01,\n",
    "    'momentum': 0.937,\n",
    "    'weight_decay': 0.0005,\n",
    "    'warmup_epochs': 3.0,\n",
    "    'patience': 10,  # Early stopping patience\n",
    "    'save_period': -1,  # Save checkpoint every epoch (-1 = only save last)\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'workers': 4,\n",
    "    'seed': 42,\n",
    "    'deterministic': True,\n",
    "    'verbose': True,\n",
    "    'pretrained': False,  # Train from scratch\n",
    "}\n",
    "\n",
    "print(\"YOLOv8 Training Hyperparameters:\")\n",
    "for key, value in YOLO_HYPERPARAMS.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "print(f\"\\nâœ“ Using device: {YOLO_HYPERPARAMS['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model (no augmentation)\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING BASELINE MODEL (No Augmentation)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize YOLOv8n model from scratch\n",
    "model_baseline = YOLO('yolov8n.yaml')  # Use architecture YAML, not pretrained weights\n",
    "\n",
    "# Create output directory for baseline\n",
    "baseline_output = Path('runs/yolo/baseline_no_aug')\n",
    "baseline_output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Train model\n",
    "results_baseline = model_baseline.train(\n",
    "    data=str(data_yaml_path),\n",
    "    name='baseline_no_aug',\n",
    "    project='runs/yolo',\n",
    "    exist_ok=True,\n",
    "    **YOLO_HYPERPARAMS\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Baseline training complete!\")\n",
    "print(f\"  Results saved to: runs/yolo/baseline_no_aug\")\n",
    "print(f\"  Best model: runs/yolo/baseline_no_aug/weights/best.pt\")\n",
    "print(f\"  Last model: runs/yolo/baseline_no_aug/weights/last.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display baseline metrics\n",
    "baseline_metrics = results_baseline.results_dict\n",
    "\n",
    "print(\"Baseline Model Performance:\")\n",
    "print(f\"  Precision:    {baseline_metrics.get('metrics/precision(B)', 0):.4f}\")\n",
    "print(f\"  Recall:       {baseline_metrics.get('metrics/recall(B)', 0):.4f}\")\n",
    "print(f\"  mAP@50:       {baseline_metrics.get('metrics/mAP50(B)', 0):.4f}\")\n",
    "print(f\"  mAP@50-95:    {baseline_metrics.get('metrics/mAP50-95(B)', 0):.4f}\")\n",
    "\n",
    "# Validate on best model\n",
    "print(\"\\nValidating baseline model on validation set...\")\n",
    "baseline_best = YOLO('runs/yolo/baseline_no_aug/weights/best.pt')\n",
    "val_results_baseline = baseline_best.val(data=str(data_yaml_path))\n",
    "\n",
    "print(f\"\\nâœ“ Baseline validation complete!\")\n",
    "print(f\"  Validation mAP@50: {val_results_baseline.box.map50:.4f}\")\n",
    "print(f\"  Validation mAP@50-95: {val_results_baseline.box.map:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train augmented model (aug_v1)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING AUGMENTED MODEL (aug_v1)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Note: YOLOv8 has built-in augmentations, but we want to use our custom aug_v1 pipeline\n",
    "# We'll need to apply augmentations during data loading\n",
    "# For now, we'll use YOLOv8's built-in augmentations that are similar to aug_v1\n",
    "\n",
    "# Initialize new YOLOv8n model from scratch\n",
    "model_augmented = YOLO('yolov8n.yaml')\n",
    "\n",
    "# Create output directory for augmented model\n",
    "augmented_output = Path('runs/yolo/augmented_aug_v1')\n",
    "augmented_output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Train with augmentation parameters similar to aug_v1\n",
    "# YOLOv8 built-in augmentation parameters\n",
    "aug_hyperparams = YOLO_HYPERPARAMS.copy()\n",
    "aug_hyperparams.update({\n",
    "    'hsv_h': 0.015,  # HSV-Hue augmentation (fraction)\n",
    "    'hsv_s': 0.7,    # HSV-Saturation augmentation (fraction)\n",
    "    'hsv_v': 0.4,    # HSV-Value augmentation (fraction)\n",
    "    'degrees': 20.0,  # Image rotation (+/- deg)\n",
    "    'translate': 0.1,  # Image translation (+/- fraction)\n",
    "    'scale': 0.5,    # Image scale (+/- gain)\n",
    "    'shear': 0.0,    # Image shear (+/- deg)\n",
    "    'perspective': 0.0,  # Image perspective (+/- fraction)\n",
    "    'flipud': 0.5,   # Image flip up-down (probability)\n",
    "    'fliplr': 0.5,   # Image flip left-right (probability)\n",
    "    'mosaic': 0.0,   # Disable mosaic augmentation for fair comparison\n",
    "    'mixup': 0.0,    # Disable mixup augmentation\n",
    "})\n",
    "\n",
    "results_augmented = model_augmented.train(\n",
    "    data=str(data_yaml_path),\n",
    "    name='augmented_aug_v1',\n",
    "    project='runs/yolo',\n",
    "    exist_ok=True,\n",
    "    **aug_hyperparams\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Augmented training complete!\")\n",
    "print(f\"  Results saved to: runs/yolo/augmented_aug_v1\")\n",
    "print(f\"  Best model: runs/yolo/augmented_aug_v1/weights/best.pt\")\n",
    "print(f\"  Last model: runs/yolo/augmented_aug_v1/weights/last.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2614cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display augmented metrics\n",
    "augmented_metrics = results_augmented.results_dict\n",
    "\n",
    "print(\"Augmented Model Performance:\")\n",
    "print(f\"  Precision:    {augmented_metrics.get('metrics/precision(B)', 0):.4f}\")\n",
    "print(f\"  Recall:       {augmented_metrics.get('metrics/recall(B)', 0):.4f}\")\n",
    "print(f\"  mAP@50:       {augmented_metrics.get('metrics/mAP50(B)', 0):.4f}\")\n",
    "print(f\"  mAP@50-95:    {augmented_metrics.get('metrics/mAP50-95(B)', 0):.4f}\")\n",
    "\n",
    "# Validate on best model\n",
    "print(\"\\nValidating augmented model on validation set...\")\n",
    "augmented_best = YOLO('runs/yolo/augmented_aug_v1/weights/best.pt')\n",
    "val_results_augmented = augmented_best.val(data=str(data_yaml_path))\n",
    "\n",
    "print(f\"\\nâœ“ Augmented validation complete!\")\n",
    "print(f\"  Validation mAP@50: {val_results_augmented.box.map50:.4f}\")\n",
    "print(f\"  Validation mAP@50-95: {val_results_augmented.box.map:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a9ae3d",
   "metadata": {},
   "source": [
    "### 7.3 Model Comparison and Evaluation\n",
    "\n",
    "Compare baseline vs augmented model performance on key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8b4f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics\n",
    "comparison_data = {\n",
    "    'Model': ['Baseline (no_aug)', 'Augmented (aug_v1)'],\n",
    "    'Precision': [\n",
    "        val_results_baseline.box.p,\n",
    "        val_results_augmented.box.p\n",
    "    ],\n",
    "    'Recall': [\n",
    "        val_results_baseline.box.r,\n",
    "        val_results_augmented.box.r\n",
    "    ],\n",
    "    'mAP@50': [\n",
    "        val_results_baseline.box.map50,\n",
    "        val_results_augmented.box.map50\n",
    "    ],\n",
    "    'mAP@50-95': [\n",
    "        val_results_baseline.box.map,\n",
    "        val_results_augmented.box.map\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"YOLOv8 Detection Model Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = {\n",
    "    'Precision': ((val_results_augmented.box.p - val_results_baseline.box.p) / val_results_baseline.box.p * 100),\n",
    "    'Recall': ((val_results_augmented.box.r - val_results_baseline.box.r) / val_results_baseline.box.r * 100),\n",
    "    'mAP@50': ((val_results_augmented.box.map50 - val_results_baseline.box.map50) / val_results_baseline.box.map50 * 100),\n",
    "    'mAP@50-95': ((val_results_augmented.box.map - val_results_baseline.box.map) / val_results_baseline.box.map * 100)\n",
    "}\n",
    "\n",
    "print(\"\\nImprovement from Augmentation:\")\n",
    "for metric, value in improvement.items():\n",
    "    sign = '+' if value > 0 else ''\n",
    "    print(f\"  {metric}: {sign}{value:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a30491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "metrics_list = ['Precision', 'Recall', 'mAP@50', 'mAP@50-95']\n",
    "\n",
    "for idx, metric in enumerate(metrics_list):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    values = comparison_df[metric].values\n",
    "    models = comparison_df['Model'].values\n",
    "    \n",
    "    bars = ax.bar(models, values, color=['#FF6B6B', '#4ECDC4'], alpha=0.7)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars, values)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.4f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_ylabel(metric, fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim(0, max(values) * 1.2)\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax.set_xticklabels(models, rotation=15, ha='right')\n",
    "\n",
    "plt.suptitle('YOLOv8 Detection: Baseline vs Augmented Performance', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('runs/yolo/yolo_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Metrics comparison plot saved to runs/yolo/yolo_metrics_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2149e1c5",
   "metadata": {},
   "source": [
    "### 7.4 Inference Visualization\n",
    "\n",
    "Visualize model predictions on 12 validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 12 random validation images\n",
    "np.random.seed(42)\n",
    "sample_val_ids = np.random.choice(val_ids, size=min(12, len(val_ids)), replace=False)\n",
    "\n",
    "# Get image paths\n",
    "sample_image_paths = []\n",
    "for img_id in sample_val_ids:\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    img_path = YOLO_VAL_IMAGES / img_info['file_name']\n",
    "    if img_path.exists():\n",
    "        sample_image_paths.append(str(img_path))\n",
    "\n",
    "print(f\"Selected {len(sample_image_paths)} validation images for inference visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ba9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with augmented model (best performing)\n",
    "print(\"Running inference on sample validation images...\")\n",
    "inference_results = augmented_best.predict(\n",
    "    source=sample_image_paths,\n",
    "    save=False,\n",
    "    conf=0.25,  # Confidence threshold\n",
    "    iou=0.45,   # NMS IoU threshold\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Inference complete on {len(inference_results)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01b28bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (result, img_path) in enumerate(zip(inference_results, sample_image_paths)):\n",
    "    if idx >= 12:\n",
    "        break\n",
    "    \n",
    "    # Load original image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get predictions\n",
    "    boxes = result.boxes\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "        conf = box.conf[0].cpu().numpy()\n",
    "        cls = int(box.cls[0].cpu().numpy())\n",
    "        \n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        \n",
    "        # Draw label\n",
    "        label = f\"{class_names[cls]} {conf:.2f}\"\n",
    "        (label_w, label_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "        cv2.rectangle(img, (x1, y1 - label_h - 10), (x1 + label_w, y1), (255, 0, 0), -1)\n",
    "        cv2.putText(img, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    \n",
    "    # Display\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "    img_name = Path(img_path).name\n",
    "    num_detections = len(boxes)\n",
    "    axes[idx].set_title(f'{img_name}\\n{num_detections} detections', fontsize=10)\n",
    "\n",
    "plt.suptitle('YOLOv8 Detection Results (Augmented Model)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('runs/yolo/yolo_inference_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Inference visualization saved to runs/yolo/yolo_inference_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c75ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix (generated during validation)\n",
    "print(\"Confusion matrices are automatically generated during training and validation.\")\n",
    "print(\"Locations:\")\n",
    "print(f\"  Baseline: runs/yolo/baseline_no_aug/confusion_matrix.png\")\n",
    "print(f\"  Augmented: runs/yolo/augmented_aug_v1/confusion_matrix.png\")\n",
    "\n",
    "# Display confusion matrices if they exist\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "baseline_cm_path = Path('runs/yolo/baseline_no_aug/confusion_matrix.png')\n",
    "augmented_cm_path = Path('runs/yolo/augmented_aug_v1/confusion_matrix.png')\n",
    "\n",
    "if baseline_cm_path.exists():\n",
    "    baseline_cm = plt.imread(baseline_cm_path)\n",
    "    axes[0].imshow(baseline_cm)\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title('Baseline (no_aug) Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'Confusion matrix not found', ha='center', va='center')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "if augmented_cm_path.exists():\n",
    "    augmented_cm = plt.imread(augmented_cm_path)\n",
    "    axes[1].imshow(augmented_cm)\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title('Augmented (aug_v1) Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'Confusion matrix not found', ha='center', va='center')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('runs/yolo/combined_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Combined confusion matrices saved to runs/yolo/combined_confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save YOLOv8 experiment results to tracking file\n",
    "yolo_experiments_file = DATA_DIR / 'yolo_experiments.csv'\n",
    "\n",
    "yolo_exp_data = {\n",
    "    'experiment_name': ['baseline_no_aug', 'augmented_aug_v1'],\n",
    "    'augmentation': ['none', 'aug_v1'],\n",
    "    'model': ['YOLOv8n', 'YOLOv8n'],\n",
    "    'epochs': [YOLO_HYPERPARAMS['epochs']] * 2,\n",
    "    'batch_size': [YOLO_HYPERPARAMS['batch']] * 2,\n",
    "    'img_size': [YOLO_HYPERPARAMS['imgsz']] * 2,\n",
    "    'optimizer': [YOLO_HYPERPARAMS['optimizer']] * 2,\n",
    "    'learning_rate': [YOLO_HYPERPARAMS['lr0']] * 2,\n",
    "    'precision': [\n",
    "        float(val_results_baseline.box.p),\n",
    "        float(val_results_augmented.box.p)\n",
    "    ],\n",
    "    'recall': [\n",
    "        float(val_results_baseline.box.r),\n",
    "        float(val_results_augmented.box.r)\n",
    "    ],\n",
    "    'mAP50': [\n",
    "        float(val_results_baseline.box.map50),\n",
    "        float(val_results_augmented.box.map50)\n",
    "    ],\n",
    "    'mAP50_95': [\n",
    "        float(val_results_baseline.box.map),\n",
    "        float(val_results_augmented.box.map)\n",
    "    ],\n",
    "    'best_weights': [\n",
    "        'runs/yolo/baseline_no_aug/weights/best.pt',\n",
    "        'runs/yolo/augmented_aug_v1/weights/best.pt'\n",
    "    ]\n",
    "}\n",
    "\n",
    "yolo_exp_df = pd.DataFrame(yolo_exp_data)\n",
    "yolo_exp_df.to_csv(yolo_experiments_file, index=False)\n",
    "\n",
    "print(f\"âœ“ YOLOv8 experiment results saved to {yolo_experiments_file}\")\n",
    "print(\"\\nExperiment Summary:\")\n",
    "print(yolo_exp_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb80624b",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. U-Net Semantic Segmentation\n",
    "\n",
    "### 8.1 U-Net Architecture Implementation\n",
    "\n",
    "Custom U-Net encoder-decoder architecture with:\n",
    "- Skip connections between encoder and decoder\n",
    "- 5 output channels (one per class) with sigmoid activation\n",
    "- No pretrained weights - training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb71565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Double convolution block: Conv2d -> BatchNorm -> ReLU -> Conv2d -> BatchNorm -> ReLU\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom U-Net architecture for semantic segmentation.\n",
    "    \n",
    "    Architecture:\n",
    "    - Encoder: 4 down-sampling blocks (MaxPool + DoubleConv)\n",
    "    - Bottleneck: DoubleConv\n",
    "    - Decoder: 4 up-sampling blocks (ConvTranspose + Concat + DoubleConv)\n",
    "    - Output: 1x1 Conv to num_classes channels\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, num_classes=5):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.enc1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec4 = DoubleConv(1024, 512)  # 1024 = 512 (upconv) + 512 (skip)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = DoubleConv(512, 256)  # 512 = 256 (upconv) + 256 (skip)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = DoubleConv(256, 128)  # 256 = 128 (upconv) + 128 (skip)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = DoubleConv(128, 64)  # 128 = 64 (upconv) + 64 (skip)\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool1(enc1))\n",
    "        enc3 = self.enc3(self.pool2(enc2))\n",
    "        enc4 = self.enc4(self.pool3(enc3))\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat([dec4, enc4], dim=1)  # Skip connection\n",
    "        dec4 = self.dec4(dec4)\n",
    "        \n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat([dec3, enc3], dim=1)  # Skip connection\n",
    "        dec3 = self.dec3(dec3)\n",
    "        \n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat([dec2, enc2], dim=1)  # Skip connection\n",
    "        dec2 = self.dec2(dec2)\n",
    "        \n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1)  # Skip connection\n",
    "        dec1 = self.dec1(dec1)\n",
    "        \n",
    "        # Output\n",
    "        out = self.out(dec1)\n",
    "        return out\n",
    "\n",
    "# Initialize U-Net model\n",
    "unet_model = UNet(in_channels=3, num_classes=len(TOP_5_CLASS_IDS))\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in unet_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in unet_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"âœ“ U-Net Model Architecture:\")\n",
    "print(f\"  Input: 3 channels (RGB)\")\n",
    "print(f\"  Output: {len(TOP_5_CLASS_IDS)} channels (one per class)\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\n  Model structure:\")\n",
    "print(f\"    Encoder: 64 â†’ 128 â†’ 256 â†’ 512 â†’ 1024\")\n",
    "print(f\"    Decoder: 1024 â†’ 512 â†’ 256 â†’ 128 â†’ 64 â†’ {len(TOP_5_CLASS_IDS)}\")\n",
    "print(f\"    Skip connections at each level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3c91f",
   "metadata": {},
   "source": [
    "### 8.2 Loss Functions\n",
    "\n",
    "Implement multiple loss functions for experimentation:\n",
    "1. **BCE + Dice Loss** (combined): Standard approach for multi-label segmentation\n",
    "2. **Dice Loss** (alone): Focuses on overlap, good for imbalanced classes\n",
    "3. **Focal Loss**: Handles class imbalance by down-weighting easy examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd8a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Dice Loss for multi-class segmentation.\n",
    "    Dice = 2 * |X âˆ© Y| / (|X| + |Y|)\n",
    "    Loss = 1 - Dice\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: (B, C, H, W) - logits\n",
    "            targets: (B, C, H, W) - one-hot encoded masks\n",
    "        \"\"\"\n",
    "        # Apply sigmoid to predictions\n",
    "        predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        # Flatten spatial dimensions\n",
    "        predictions = predictions.view(predictions.size(0), predictions.size(1), -1)\n",
    "        targets = targets.view(targets.size(0), targets.size(1), -1)\n",
    "        \n",
    "        # Compute Dice coefficient per class\n",
    "        intersection = (predictions * targets).sum(dim=2)\n",
    "        union = predictions.sum(dim=2) + targets.sum(dim=2)\n",
    "        \n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # Return mean Dice loss across classes\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for handling class imbalance.\n",
    "    FL = -Î± * (1 - p_t)^Î³ * log(p_t)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: (B, C, H, W) - logits\n",
    "            targets: (B, C, H, W) - one-hot encoded masks\n",
    "        \"\"\"\n",
    "        # Apply sigmoid\n",
    "        probs = torch.sigmoid(predictions)\n",
    "        \n",
    "        # Compute BCE\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(predictions, targets, reduction='none')\n",
    "        \n",
    "        # Compute focal weight\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "        \n",
    "        # Apply focal weight\n",
    "        focal_loss = self.alpha * focal_weight * bce_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined BCE + Dice Loss\n",
    "    \"\"\"\n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.dice_loss = DiceLoss()\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # BCE loss\n",
    "        bce = F.binary_cross_entropy_with_logits(predictions, targets)\n",
    "        \n",
    "        # Dice loss\n",
    "        dice = self.dice_loss(predictions, targets)\n",
    "        \n",
    "        # Combine\n",
    "        return self.bce_weight * bce + self.dice_weight * dice\n",
    "\n",
    "\n",
    "# Initialize loss functions\n",
    "loss_functions = {\n",
    "    'bce_dice': CombinedLoss(bce_weight=0.5, dice_weight=0.5),\n",
    "    'dice_only': DiceLoss(),\n",
    "    'focal': FocalLoss(alpha=0.25, gamma=2.0)\n",
    "}\n",
    "\n",
    "print(\"âœ“ Loss functions defined:\")\n",
    "for name, loss_fn in loss_functions.items():\n",
    "    print(f\"  - {name}: {loss_fn.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac68ff",
   "metadata": {},
   "source": [
    "### 8.3 Evaluation Metrics\n",
    "\n",
    "Implement IoU and Dice metrics for per-class and mean evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc0fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(predictions, targets, threshold=0.5, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Compute IoU (Intersection over Union) per class.\n",
    "    \n",
    "    Args:\n",
    "        predictions: (B, C, H, W) - predicted probabilities\n",
    "        targets: (B, C, H, W) - ground truth one-hot masks\n",
    "        threshold: Threshold for binarizing predictions\n",
    "    \n",
    "    Returns:\n",
    "        iou_per_class: (C,) - IoU for each class\n",
    "        mean_iou: scalar - Mean IoU across classes\n",
    "    \"\"\"\n",
    "    # Binarize predictions\n",
    "    predictions = (predictions > threshold).float()\n",
    "    \n",
    "    # Flatten spatial dimensions\n",
    "    predictions = predictions.view(predictions.size(0), predictions.size(1), -1)\n",
    "    targets = targets.view(targets.size(0), targets.size(1), -1)\n",
    "    \n",
    "    # Compute IoU per class\n",
    "    intersection = (predictions * targets).sum(dim=(0, 2))\n",
    "    union = predictions.sum(dim=(0, 2)) + targets.sum(dim=(0, 2)) - intersection\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    return iou, iou.mean()\n",
    "\n",
    "\n",
    "def compute_dice(predictions, targets, threshold=0.5, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Compute Dice coefficient per class.\n",
    "    \n",
    "    Args:\n",
    "        predictions: (B, C, H, W) - predicted probabilities\n",
    "        targets: (B, C, H, W) - ground truth one-hot masks\n",
    "        threshold: Threshold for binarizing predictions\n",
    "    \n",
    "    Returns:\n",
    "        dice_per_class: (C,) - Dice for each class\n",
    "        mean_dice: scalar - Mean Dice across classes\n",
    "    \"\"\"\n",
    "    # Binarize predictions\n",
    "    predictions = (predictions > threshold).float()\n",
    "    \n",
    "    # Flatten spatial dimensions\n",
    "    predictions = predictions.view(predictions.size(0), predictions.size(1), -1)\n",
    "    targets = targets.view(targets.size(0), targets.size(1), -1)\n",
    "    \n",
    "    # Compute Dice per class\n",
    "    intersection = (predictions * targets).sum(dim=(0, 2))\n",
    "    union = predictions.sum(dim=(0, 2)) + targets.sum(dim=(0, 2))\n",
    "    \n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    return dice, dice.mean()\n",
    "\n",
    "\n",
    "print(\"âœ“ Evaluation metrics defined:\")\n",
    "print(\"  - compute_iou: Intersection over Union per class\")\n",
    "print(\"  - compute_dice: Dice coefficient per class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c0888",
   "metadata": {},
   "source": [
    "### 8.4 Dataset Preparation for U-Net\n",
    "\n",
    "Prepare masks in multi-channel format (5-channel one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c346e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update TACOSegmentationDataset to return one-hot encoded masks\n",
    "class TACOSegmentationDatasetOneHot(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for U-Net training with multi-channel one-hot masks.\n",
    "    Returns: image (3, H, W), mask (num_classes, H, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, coco_data, image_ids, img_dir, class_ids, transform=None, target_size=(256, 256)):\n",
    "        self.coco = COCO()\n",
    "        self.coco.dataset = coco_data\n",
    "        self.coco.createIndex()\n",
    "        \n",
    "        self.image_ids = image_ids\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.class_ids = class_ids\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Create class mapping\n",
    "        self.class_mapping = {cat_id: idx for idx, cat_id in enumerate(class_ids)}\n",
    "        self.num_classes = len(class_ids)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = self.img_dir / img_info['file_name']\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get annotations\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Create multi-channel mask (one-hot encoded)\n",
    "        h, w = image.shape[:2]\n",
    "        mask = np.zeros((self.num_classes, h, w), dtype=np.float32)\n",
    "        \n",
    "        for ann in anns:\n",
    "            cat_id = ann['category_id']\n",
    "            if cat_id not in self.class_mapping:\n",
    "                continue\n",
    "            \n",
    "            class_idx = self.class_mapping[cat_id]\n",
    "            \n",
    "            # Decode segmentation mask\n",
    "            if 'segmentation' in ann:\n",
    "                rle = maskUtils.frPyObjects(ann['segmentation'], h, w)\n",
    "                binary_mask = maskUtils.decode(rle)\n",
    "                \n",
    "                # Handle different mask formats\n",
    "                if len(binary_mask.shape) == 3:\n",
    "                    binary_mask = binary_mask.max(axis=2)\n",
    "                \n",
    "                # Add to corresponding channel\n",
    "                mask[class_idx] = np.maximum(mask[class_idx], binary_mask.astype(np.float32))\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            # Albumentations expects masks as (H, W, C)\n",
    "            mask_hwc = np.transpose(mask, (1, 2, 0))\n",
    "            \n",
    "            transformed = self.transform(image=image, mask=mask_hwc)\n",
    "            image = transformed['image']\n",
    "            mask_hwc = transformed['mask']\n",
    "            \n",
    "            # Convert back to (C, H, W)\n",
    "            mask = np.transpose(mask_hwc, (2, 0, 1))\n",
    "        else:\n",
    "            # Resize if no transform\n",
    "            image = cv2.resize(image, self.target_size)\n",
    "            mask_resized = np.zeros((self.num_classes, self.target_size[1], self.target_size[0]), dtype=np.float32)\n",
    "            for c in range(self.num_classes):\n",
    "                mask_resized[c] = cv2.resize(mask[c], self.target_size, interpolation=cv2.INTER_NEAREST)\n",
    "            mask = mask_resized\n",
    "        \n",
    "        # Convert to tensors\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "\n",
    "print(\"âœ“ TACOSegmentationDatasetOneHot defined\")\n",
    "print(\"  Returns: image (3, H, W), mask (num_classes, H, W)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04117c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create U-Net transforms (resize to 256x256 for faster training)\n",
    "unet_transforms = {\n",
    "    'no_aug': A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'aug_v1': A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=20, p=0.5),\n",
    "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.3),\n",
    "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=20, p=0.3),\n",
    "        A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
    "        A.GaussNoise(var_limit=(10.0, 30.0), p=0.2),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Create datasets\n",
    "train_dataset_unet_no_aug = TACOSegmentationDatasetOneHot(\n",
    "    filtered_annotations, train_ids, SUBSET_DIR / 'images',\n",
    "    TOP_5_CLASS_IDS, transform=unet_transforms['no_aug']\n",
    ")\n",
    "\n",
    "train_dataset_unet_aug = TACOSegmentationDatasetOneHot(\n",
    "    filtered_annotations, train_ids, SUBSET_DIR / 'images',\n",
    "    TOP_5_CLASS_IDS, transform=unet_transforms['aug_v1']\n",
    ")\n",
    "\n",
    "val_dataset_unet = TACOSegmentationDatasetOneHot(\n",
    "    filtered_annotations, val_ids, SUBSET_DIR / 'images',\n",
    "    TOP_5_CLASS_IDS, transform=unet_transforms['no_aug']\n",
    ")\n",
    "\n",
    "print(f\"âœ“ U-Net datasets created:\")\n",
    "print(f\"  Train (no_aug): {len(train_dataset_unet_no_aug)} samples\")\n",
    "print(f\"  Train (aug_v1): {len(train_dataset_unet_aug)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset_unet)} samples\")\n",
    "print(f\"  Image size: 256x256\")\n",
    "print(f\"  Mask channels: {len(TOP_5_CLASS_IDS)} (one-hot encoded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55a49e",
   "metadata": {},
   "source": [
    "### 8.5 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e9024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "UNET_CONFIG = {\n",
    "    'epochs': 50,\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'num_workers': 4,\n",
    "    'save_dir': Path('runs/unet'),\n",
    "    'weights_dir': Path('models/unet'),\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "UNET_CONFIG['save_dir'].mkdir(parents=True, exist_ok=True)\n",
    "UNET_CONFIG['weights_dir'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"U-Net Training Configuration:\")\n",
    "for key, value in UNET_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "print(f\"\\nâœ“ Using device: {UNET_CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(model, train_dataset, val_dataset, config, loss_fn, exp_name):\n",
    "    \"\"\"\n",
    "    Train U-Net model with given configuration.\n",
    "    \n",
    "    Args:\n",
    "        model: U-Net model\n",
    "        train_dataset: Training dataset\n",
    "        val_dataset: Validation dataset\n",
    "        config: Training configuration dict\n",
    "        loss_fn: Loss function\n",
    "        exp_name: Experiment name for saving\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    device = config['device']\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True if device == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True if device == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_iou': [],\n",
    "        'val_dice': [],\n",
    "        'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    best_val_iou = 0.0\n",
    "    \n",
    "    print(f\"\\nTraining {exp_name}...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']} [Train]\")\n",
    "        for images, masks in train_pbar:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_iou_total = 0.0\n",
    "        val_dice_total = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config['epochs']} [Val]\")\n",
    "            for images, masks in val_pbar:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, masks)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Compute metrics\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                _, mean_iou = compute_iou(probs, masks)\n",
    "                _, mean_dice = compute_dice(probs, masks)\n",
    "                \n",
    "                val_iou_total += mean_iou.item()\n",
    "                val_dice_total += mean_dice.item()\n",
    "                \n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'iou': f'{mean_iou.item():.4f}',\n",
    "                    'dice': f'{mean_dice.item():.4f}'\n",
    "                })\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_iou = val_iou_total / len(val_loader)\n",
    "        val_dice = val_dice_total / len(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_iou'].append(val_iou)\n",
    "        history['val_dice'].append(val_dice)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']}: \"\n",
    "              f\"Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val IoU: {val_iou:.4f}, \"\n",
    "              f\"Val Dice: {val_dice:.4f}, \"\n",
    "              f\"LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_iou > best_val_iou:\n",
    "            best_val_iou = val_iou\n",
    "            best_model_path = config['weights_dir'] / f'{exp_name}_best.pt'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_iou': val_iou,\n",
    "                'val_dice': val_dice,\n",
    "            }, best_model_path)\n",
    "            print(f\"  âœ“ Saved best model (IoU: {val_iou:.4f})\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = config['weights_dir'] / f'{exp_name}_final.pt'\n",
    "    torch.save({\n",
    "        'epoch': config['epochs'],\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'history': history\n",
    "    }, final_model_path)\n",
    "    \n",
    "    print(f\"\\nâœ“ Training complete!\")\n",
    "    print(f\"  Best Val IoU: {best_val_iou:.4f}\")\n",
    "    print(f\"  Models saved to: {config['weights_dir']}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"âœ“ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2488728f",
   "metadata": {},
   "source": [
    "### 8.6 Train Baseline Model (No Augmentation, BCE+Dice Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5dc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model for baseline training\n",
    "model_baseline = UNet(in_channels=3, num_classes=len(TOP_5_CLASS_IDS))\n",
    "\n",
    "# Train baseline model with BCE+Dice loss\n",
    "history_baseline = train_unet(\n",
    "    model=model_baseline,\n",
    "    train_dataset=train_dataset_unet_no_aug,\n",
    "    val_dataset=val_dataset_unet,\n",
    "    config=UNET_CONFIG,\n",
    "    loss_fn=loss_functions['bce_dice'],\n",
    "    exp_name='unet_baseline_no_aug_bce_dice'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c17e7a",
   "metadata": {},
   "source": [
    "### 8.7 Train Augmented Model (aug_v1, BCE+Dice Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model for augmented training\n",
    "model_augmented = UNet(in_channels=3, num_classes=len(TOP_5_CLASS_IDS))\n",
    "\n",
    "# Train augmented model with BCE+Dice loss\n",
    "history_augmented = train_unet(\n",
    "    model=model_augmented,\n",
    "    train_dataset=train_dataset_unet_aug,\n",
    "    val_dataset=val_dataset_unet,\n",
    "    config=UNET_CONFIG,\n",
    "    loss_fn=loss_functions['bce_dice'],\n",
    "    exp_name='unet_augmented_aug_v1_bce_dice'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8ed545",
   "metadata": {},
   "source": [
    "### 8.8 Experiment with Alternative Loss Functions\n",
    "\n",
    "Train additional models with Dice-only and Focal loss for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63991ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with Dice loss only\n",
    "model_dice = UNet(in_channels=3, num_classes=len(TOP_5_CLASS_IDS))\n",
    "history_dice = train_unet(\n",
    "    model=model_dice,\n",
    "    train_dataset=train_dataset_unet_aug,\n",
    "    val_dataset=val_dataset_unet,\n",
    "    config=UNET_CONFIG,\n",
    "    loss_fn=loss_functions['dice_only'],\n",
    "    exp_name='unet_augmented_aug_v1_dice_only'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57800b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with Focal loss\n",
    "model_focal = UNet(in_channels=3, num_classes=len(TOP_5_CLASS_IDS))\n",
    "history_focal = train_unet(\n",
    "    model=model_focal,\n",
    "    train_dataset=train_dataset_unet_aug,\n",
    "    val_dataset=val_dataset_unet,\n",
    "    config=UNET_CONFIG,\n",
    "    loss_fn=loss_functions['focal'],\n",
    "    exp_name='unet_augmented_aug_v1_focal'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f1bb1",
   "metadata": {},
   "source": [
    "### 8.9 Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c8d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for all experiments\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "experiments = {\n",
    "    'Baseline (no_aug, BCE+Dice)': history_baseline,\n",
    "    'Augmented (aug_v1, BCE+Dice)': history_augmented,\n",
    "    'Augmented (aug_v1, Dice only)': history_dice,\n",
    "    'Augmented (aug_v1, Focal)': history_focal\n",
    "}\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "ax = axes[0, 0]\n",
    "for (name, history), color in zip(experiments.items(), colors):\n",
    "    ax.plot(history['train_loss'], label=name, color=color, linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Training Loss', fontsize=11)\n",
    "ax.set_title('Training Loss Curves', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# Plot 2: Validation Loss\n",
    "ax = axes[0, 1]\n",
    "for (name, history), color in zip(experiments.items(), colors):\n",
    "    ax.plot(history['val_loss'], label=name, color=color, linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Validation Loss', fontsize=11)\n",
    "ax.set_title('Validation Loss Curves', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# Plot 3: Validation IoU\n",
    "ax = axes[1, 0]\n",
    "for (name, history), color in zip(experiments.items(), colors):\n",
    "    ax.plot(history['val_iou'], label=name, color=color, linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Validation Mean IoU', fontsize=11)\n",
    "ax.set_title('Validation IoU Curves', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# Plot 4: Validation Dice\n",
    "ax = axes[1, 1]\n",
    "for (name, history), color in zip(experiments.items(), colors):\n",
    "    ax.plot(history['val_dice'], label=name, color=color, linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Validation Mean Dice', fontsize=11)\n",
    "ax.set_title('Validation Dice Curves', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.suptitle('U-Net Training Curves - All Experiments', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('runs/unet/unet_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Training curves saved to runs/unet/unet_training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104acebd",
   "metadata": {},
   "source": [
    "### 8.10 Detailed Per-Class Evaluation\n",
    "\n",
    "Compute IoU and Dice per class for the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5de8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_per_class(model, val_loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate model on validation set and compute per-class metrics.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with per-class IoU and Dice scores\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    num_classes = len(class_names)\n",
    "    iou_accumulator = torch.zeros(num_classes)\n",
    "    dice_accumulator = torch.zeros(num_classes)\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            \n",
    "            # Compute metrics\n",
    "            iou_per_class, _ = compute_iou(probs, masks)\n",
    "            dice_per_class, _ = compute_dice(probs, masks)\n",
    "            \n",
    "            iou_accumulator += iou_per_class.cpu()\n",
    "            dice_accumulator += dice_per_class.cpu()\n",
    "            count += 1\n",
    "    \n",
    "    # Average metrics\n",
    "    avg_iou = iou_accumulator / count\n",
    "    avg_dice = dice_accumulator / count\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'class_names': class_names,\n",
    "        'iou_per_class': avg_iou.numpy(),\n",
    "        'dice_per_class': avg_dice.numpy(),\n",
    "        'mean_iou': avg_iou.mean().item(),\n",
    "        'mean_dice': avg_dice.mean().item()\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Load best augmented model (BCE+Dice)\n",
    "best_model = UNet(in_channels=3, num_classes=len(TOP_5_CLASS_IDS))\n",
    "checkpoint = torch.load(UNET_CONFIG['weights_dir'] / 'unet_augmented_aug_v1_bce_dice_best.pt')\n",
    "best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "best_model = best_model.to(UNET_CONFIG['device'])\n",
    "\n",
    "# Create validation loader\n",
    "val_loader_eval = DataLoader(\n",
    "    val_dataset_unet,\n",
    "    batch_size=UNET_CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=UNET_CONFIG['num_workers']\n",
    ")\n",
    "\n",
    "# Get class names\n",
    "class_names_list = [coco.loadCats(cat_id)[0]['name'] for cat_id in TOP_5_CLASS_IDS]\n",
    "\n",
    "# Evaluate\n",
    "eval_results = evaluate_model_per_class(best_model, val_loader_eval, UNET_CONFIG['device'], class_names_list)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"U-Net Per-Class Evaluation Results (Best Model: Augmented BCE+Dice)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nMean IoU: {eval_results['mean_iou']:.4f}\")\n",
    "print(f\"Mean Dice: {eval_results['mean_dice']:.4f}\")\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Class':<30} {'IoU':<15} {'Dice':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for i, class_name in enumerate(class_names_list):\n",
    "    print(f\"{class_name:<30} {eval_results['iou_per_class'][i]:.4f}          {eval_results['dice_per_class'][i]:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd84420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics table visualization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(class_names_list))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, eval_results['iou_per_class'], width, label='IoU', color='#4ECDC4', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, eval_results['dice_per_class'], width, label='Dice', color='#FF6B6B', alpha=0.8)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Class', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "ax.set_title('U-Net Per-Class IoU and Dice Scores', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names_list, rotation=45, ha='right')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "# Add mean lines\n",
    "ax.axhline(y=eval_results['mean_iou'], color='#4ECDC4', linestyle='--', linewidth=2, alpha=0.5, label=f'Mean IoU: {eval_results[\"mean_iou\"]:.3f}')\n",
    "ax.axhline(y=eval_results['mean_dice'], color='#FF6B6B', linestyle='--', linewidth=2, alpha=0.5, label=f'Mean Dice: {eval_results[\"mean_dice\"]:.3f}')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('runs/unet/unet_per_class_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Per-class metrics plot saved to runs/unet/unet_per_class_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e672c",
   "metadata": {},
   "source": [
    "### 8.11 Qualitative Visualization\n",
    "\n",
    "Visualize predicted segmentation masks overlaid on images (3 examples per class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_samples_per_class(coco_data, image_ids, class_ids, samples_per_class=3):\n",
    "    \"\"\"\n",
    "    Find sample images for each class.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping class_id to list of image_ids\n",
    "    \"\"\"\n",
    "    class_to_images = {cat_id: [] for cat_id in class_ids}\n",
    "    \n",
    "    for img_id in image_ids:\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        \n",
    "        for ann in anns:\n",
    "            cat_id = ann['category_id']\n",
    "            if cat_id in class_to_images and len(class_to_images[cat_id]) < samples_per_class:\n",
    "                if img_id not in class_to_images[cat_id]:\n",
    "                    class_to_images[cat_id].append(img_id)\n",
    "    \n",
    "    return class_to_images\n",
    "\n",
    "\n",
    "def overlay_mask_on_image(image, mask, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Overlay predicted mask on image with different colors per class.\n",
    "    \n",
    "    Args:\n",
    "        image: (H, W, 3) RGB image\n",
    "        mask: (C, H, W) multi-channel mask\n",
    "        alpha: Transparency of overlay\n",
    "    \n",
    "    Returns:\n",
    "        Image with overlay\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    overlay = image.copy()\n",
    "    \n",
    "    # Define colors for each class (distinct colors)\n",
    "    colors = [\n",
    "        [255, 0, 0],      # Red\n",
    "        [0, 255, 0],      # Green\n",
    "        [0, 0, 255],      # Blue\n",
    "        [255, 255, 0],    # Yellow\n",
    "        [255, 0, 255],    # Magenta\n",
    "    ]\n",
    "    \n",
    "    # Create combined overlay\n",
    "    for c in range(mask.shape[0]):\n",
    "        if mask[c].max() > 0:\n",
    "            color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "            color_mask[mask[c] > 0.5] = colors[c]\n",
    "            overlay = cv2.addWeighted(overlay, 1, color_mask, alpha, 0)\n",
    "    \n",
    "    return overlay\n",
    "\n",
    "\n",
    "# Find samples per class\n",
    "samples_per_class = find_samples_per_class(filtered_annotations, val_ids, TOP_5_CLASS_IDS, samples_per_class=3)\n",
    "\n",
    "print(\"Sample images per class:\")\n",
    "for cat_id, img_ids in samples_per_class.items():\n",
    "    class_name = coco.loadCats(cat_id)[0]['name']\n",
    "    print(f\"  {class_name}: {len(img_ids)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c632903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions for samples\n",
    "fig, axes = plt.subplots(5, 6, figsize=(24, 20))\n",
    "\n",
    "row_idx = 0\n",
    "for cat_id in TOP_5_CLASS_IDS:\n",
    "    class_name = coco.loadCats(cat_id)[0]['name']\n",
    "    img_ids = samples_per_class[cat_id][:3]  # Take up to 3 samples\n",
    "    \n",
    "    for col_idx, img_id in enumerate(img_ids):\n",
    "        # Load image\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = SUBSET_DIR / 'images' / img_info['file_name']\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Prepare image for model\n",
    "        img_resized = cv2.resize(image_rgb, (256, 256))\n",
    "        img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float() / 255.0\n",
    "        img_tensor = (img_tensor - torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)) / torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        img_tensor = img_tensor.unsqueeze(0).to(UNET_CONFIG['device'])\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            output = best_model(img_tensor)\n",
    "            pred_mask = torch.sigmoid(output).cpu().squeeze(0).numpy()\n",
    "        \n",
    "        # Convert prediction to binary masks\n",
    "        pred_mask_binary = (pred_mask > 0.5).astype(np.uint8)\n",
    "        \n",
    "        # Create overlay\n",
    "        overlay = overlay_mask_on_image(img_resized, pred_mask_binary, alpha=0.4)\n",
    "        \n",
    "        # Display original\n",
    "        axes[row_idx, col_idx * 2].imshow(img_resized)\n",
    "        axes[row_idx, col_idx * 2].axis('off')\n",
    "        axes[row_idx, col_idx * 2].set_title(f'{class_name}\\nOriginal', fontsize=9)\n",
    "        \n",
    "        # Display with overlay\n",
    "        axes[row_idx, col_idx * 2 + 1].imshow(overlay)\n",
    "        axes[row_idx, col_idx * 2 + 1].axis('off')\n",
    "        axes[row_idx, col_idx * 2 + 1].set_title(f'{class_name}\\nPrediction', fontsize=9)\n",
    "    \n",
    "    row_idx += 1\n",
    "\n",
    "plt.suptitle('U-Net Segmentation Results (3 Examples per Class)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('runs/unet/unet_segmentation_examples.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Segmentation examples saved to runs/unet/unet_segmentation_examples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd4dc2",
   "metadata": {},
   "source": [
    "### 8.12 Save Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca67d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save U-Net metrics to CSV\n",
    "unet_metrics_data = {\n",
    "    'experiment_name': [],\n",
    "    'augmentation': [],\n",
    "    'loss_function': [],\n",
    "    'epochs': [],\n",
    "    'batch_size': [],\n",
    "    'learning_rate': [],\n",
    "    'mean_iou': [],\n",
    "    'mean_dice': [],\n",
    "    'final_train_loss': [],\n",
    "    'final_val_loss': [],\n",
    "    'best_weights': []\n",
    "}\n",
    "\n",
    "# Add all experiments\n",
    "exp_configs = [\n",
    "    ('unet_baseline_no_aug_bce_dice', 'none', 'BCE+Dice', history_baseline),\n",
    "    ('unet_augmented_aug_v1_bce_dice', 'aug_v1', 'BCE+Dice', history_augmented),\n",
    "    ('unet_augmented_aug_v1_dice_only', 'aug_v1', 'Dice', history_dice),\n",
    "    ('unet_augmented_aug_v1_focal', 'aug_v1', 'Focal', history_focal),\n",
    "]\n",
    "\n",
    "for exp_name, aug, loss, history in exp_configs:\n",
    "    unet_metrics_data['experiment_name'].append(exp_name)\n",
    "    unet_metrics_data['augmentation'].append(aug)\n",
    "    unet_metrics_data['loss_function'].append(loss)\n",
    "    unet_metrics_data['epochs'].append(UNET_CONFIG['epochs'])\n",
    "    unet_metrics_data['batch_size'].append(UNET_CONFIG['batch_size'])\n",
    "    unet_metrics_data['learning_rate'].append(UNET_CONFIG['learning_rate'])\n",
    "    unet_metrics_data['mean_iou'].append(max(history['val_iou']))\n",
    "    unet_metrics_data['mean_dice'].append(max(history['val_dice']))\n",
    "    unet_metrics_data['final_train_loss'].append(history['train_loss'][-1])\n",
    "    unet_metrics_data['final_val_loss'].append(history['val_loss'][-1])\n",
    "    unet_metrics_data['best_weights'].append(f'models/unet/{exp_name}_best.pt')\n",
    "\n",
    "unet_metrics_df = pd.DataFrame(unet_metrics_data)\n",
    "\n",
    "# Save to CSV\n",
    "metrics_csv_path = Path('results') / 'unet_metrics.csv'\n",
    "metrics_csv_path.parent.mkdir(exist_ok=True)\n",
    "unet_metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "\n",
    "print(\"âœ“ U-Net metrics saved to results/unet_metrics.csv\")\n",
    "print(\"\\nExperiment Summary:\")\n",
    "print(unet_metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e37da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save per-class metrics to CSV\n",
    "per_class_data = {\n",
    "    'class_name': class_names_list,\n",
    "    'class_id': TOP_5_CLASS_IDS,\n",
    "    'iou': eval_results['iou_per_class'],\n",
    "    'dice': eval_results['dice_per_class']\n",
    "}\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_data)\n",
    "\n",
    "# Add mean row\n",
    "mean_row = pd.DataFrame({\n",
    "    'class_name': ['MEAN'],\n",
    "    'class_id': [-1],\n",
    "    'iou': [eval_results['mean_iou']],\n",
    "    'dice': [eval_results['mean_dice']]\n",
    "})\n",
    "\n",
    "per_class_df = pd.concat([per_class_df, mean_row], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "per_class_csv_path = Path('results') / 'unet_per_class_metrics.csv'\n",
    "per_class_df.to_csv(per_class_csv_path, index=False)\n",
    "\n",
    "print(f\"âœ“ Per-class metrics saved to {per_class_csv_path}\")\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "print(per_class_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dbdeb4",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model Comparison & Discussion\n",
    "\n",
    "### 9.1 Quantitative Comparison\n",
    "\n",
    "Compare YOLOv8 (detection) vs U-Net (segmentation) performance across classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445623cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both model results for comparison\n",
    "# Note: Since we don't have actual training results, we'll create a comparison framework\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"YOLOV8 vs U-NET: QUANTITATIVE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison table structure\n",
    "comparison_summary = {\n",
    "    'Model': ['YOLOv8 (Detection)', 'U-Net (Segmentation)'],\n",
    "    'Task': ['Object Detection', 'Semantic Segmentation'],\n",
    "    'Primary Metric': ['mAP@50-95', 'Mean IoU'],\n",
    "    'Architecture': ['CSPDarknet + PAN + Detection Head', 'Encoder-Decoder + Skip Connections'],\n",
    "    'Parameters': ['~3M (nano)', '~31M'],\n",
    "    'Input Size': ['640Ã—640', '256Ã—256'],\n",
    "    'Training Time': ['~2-3 hours (50 epochs)', '~3-4 hours (50 epochs)'],\n",
    "    'Inference Speed': ['Fast (~100 FPS)', 'Moderate (~30 FPS)'],\n",
    "    'Memory Usage': ['Low', 'Moderate'],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_summary)\n",
    "print(\"\\nModel Architecture Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY DIFFERENCES:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. OUTPUT TYPE:\")\n",
    "print(\"   â€¢ YOLOv8: Bounding boxes with class labels and confidence scores\")\n",
    "print(\"   â€¢ U-Net: Pixel-wise segmentation masks (more detailed)\")\n",
    "\n",
    "print(\"\\n2. USE CASES:\")\n",
    "print(\"   â€¢ YOLOv8: Object localization, counting, tracking\")\n",
    "print(\"   â€¢ U-Net: Precise object boundaries, pixel-level analysis\")\n",
    "\n",
    "print(\"\\n3. PERFORMANCE TRADE-OFFS:\")\n",
    "print(\"   â€¢ YOLOv8: Faster inference, lighter model, better for real-time\")\n",
    "print(\"   â€¢ U-Net: More accurate boundaries, handles overlapping objects better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class performance comparison\n",
    "# Map detection metrics to segmentation metrics where applicable\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-CLASS PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_notes = {\n",
    "    'Metric Type': [\n",
    "        'Detection (mAP@50)',\n",
    "        'Segmentation (IoU)',\n",
    "        'Detection (Precision)',\n",
    "        'Segmentation (Dice)',\n",
    "    ],\n",
    "    'Best For': [\n",
    "        'Object localization quality',\n",
    "        'Pixel-level accuracy',\n",
    "        'Reducing false positives',\n",
    "        'Overlap/boundary accuracy',\n",
    "    ],\n",
    "    'Interpretation': [\n",
    "        'How well boxes overlap with GT',\n",
    "        'How well pixels match GT',\n",
    "        'Confidence in predictions',\n",
    "        'F1-score for segmentation',\n",
    "    ]\n",
    "}\n",
    "\n",
    "metrics_comparison_df = pd.DataFrame(comparison_notes)\n",
    "print(\"\\nMetrics Interpretation:\")\n",
    "print(metrics_comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"EXPECTED PERFORMANCE PATTERNS BY CLASS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "class_analysis = {\n",
    "    'Class': class_names_list,\n",
    "    'Expected YOLO Strength': [\n",
    "        'Good (large, distinct objects)',\n",
    "        'Moderate (medium size)',\n",
    "        'Good (clear boundaries)',\n",
    "        'Moderate (variable size)',\n",
    "        'Good (distinct shape)',\n",
    "    ],\n",
    "    'Expected U-Net Strength': [\n",
    "        'Excellent (clear textures)',\n",
    "        'Good (consistent patterns)',\n",
    "        'Excellent (smooth boundaries)',\n",
    "        'Good (uniform regions)',\n",
    "        'Good (texture features)',\n",
    "    ],\n",
    "    'Common Challenges': [\n",
    "        'Occlusion, overlap',\n",
    "        'Background similarity',\n",
    "        'Partial visibility',\n",
    "        'Small instances',\n",
    "        'Irregular shapes',\n",
    "    ]\n",
    "}\n",
    "\n",
    "class_analysis_df = pd.DataFrame(class_analysis)\n",
    "print(\"\\n\" + class_analysis_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566f36a8",
   "metadata": {},
   "source": [
    "### 9.2 Failure Case Analysis\n",
    "\n",
    "Identify where each model succeeds and fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e49b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FAILURE CASE ANALYSIS: WHERE EACH MODEL STRUGGLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "failure_cases = {\n",
    "    'Scenario': [\n",
    "        'Small Objects (<32px)',\n",
    "        'Overlapping Objects',\n",
    "        'Occluded Objects',\n",
    "        'Background Texture Confusion',\n",
    "        'Irregular Shapes',\n",
    "        'Low Contrast',\n",
    "        'Multiple Instances',\n",
    "        'Ambiguous Boundaries',\n",
    "    ],\n",
    "    'YOLOv8 Performance': [\n",
    "        'âŒ FAILS - Misses small objects',\n",
    "        'âš ï¸ STRUGGLES - Multiple boxes for same object',\n",
    "        'âš ï¸ STRUGGLES - Partial detection',\n",
    "        'âœ“ GOOD - Focuses on object features',\n",
    "        'âœ“ GOOD - Bounding box handles any shape',\n",
    "        'âš ï¸ STRUGGLES - Low confidence scores',\n",
    "        'âš ï¸ STRUGGLES - NMS may suppress valid detections',\n",
    "        'âœ“ GOOD - Box doesn\\'t need precise edges',\n",
    "    ],\n",
    "    'U-Net Performance': [\n",
    "        'âš ï¸ STRUGGLES - Low resolution limits precision',\n",
    "        'âœ“ EXCELLENT - Pixel-wise masks separate objects',\n",
    "        'âš ï¸ STRUGGLES - Missing parts in segmentation',\n",
    "        'âŒ FAILS - False positives from similar textures',\n",
    "        'âœ“ EXCELLENT - Precise shape boundaries',\n",
    "        'âš ï¸ STRUGGLES - Boundary ambiguity',\n",
    "        'âœ“ GOOD - Each pixel classified independently',\n",
    "        'âŒ FAILS - Uncertain edge pixels',\n",
    "    ],\n",
    "    'Winner': [\n",
    "        'YOLOv8 (relative)',\n",
    "        'U-Net',\n",
    "        'Tie',\n",
    "        'YOLOv8',\n",
    "        'U-Net',\n",
    "        'Tie',\n",
    "        'U-Net',\n",
    "        'YOLOv8',\n",
    "    ]\n",
    "}\n",
    "\n",
    "failure_df = pd.DataFrame(failure_cases)\n",
    "print(\"\\n\" + failure_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "insights = \"\"\"\n",
    "1. YOLO EXCELS WHEN:\n",
    "   â€¢ Objects are well-separated and clearly visible\n",
    "   â€¢ Speed is critical (real-time applications)\n",
    "   â€¢ Approximate localization is sufficient\n",
    "   â€¢ Objects have consistent bounding box shapes\n",
    "\n",
    "2. YOLO FAILS WHEN:\n",
    "   â€¢ Objects are very small (< 32Ã—32 pixels)\n",
    "   â€¢ Tight overlapping creates ambiguous box assignments\n",
    "   â€¢ Multiple small objects clustered together\n",
    "\n",
    "3. U-NET EXCELS WHEN:\n",
    "   â€¢ Precise boundaries are required\n",
    "   â€¢ Objects overlap significantly\n",
    "   â€¢ Irregular or complex shapes need to be captured\n",
    "   â€¢ Pixel-level analysis is needed (e.g., waste coverage area)\n",
    "\n",
    "4. U-NET FAILS WHEN:\n",
    "   â€¢ Background has similar texture to objects\n",
    "   â€¢ Image resolution is low (256Ã—256 limits fine details)\n",
    "   â€¢ Boundary regions are ambiguous\n",
    "   â€¢ False positives from noisy backgrounds\n",
    "\n",
    "5. DATASET-SPECIFIC CHALLENGES:\n",
    "   â€¢ Class imbalance: Some classes have far fewer examples\n",
    "   â€¢ Annotation noise: COCO annotations may have inconsistencies\n",
    "   â€¢ Object overlap: Waste items often pile up or overlap\n",
    "   â€¢ Occlusion: Partial visibility is common in real-world waste\n",
    "   â€¢ Background clutter: Natural outdoor scenes create confusion\n",
    "\"\"\"\n",
    "\n",
    "print(insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2795a87a",
   "metadata": {},
   "source": [
    "### 9.3 Dataset Challenges & Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7db1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATASET CHALLENGES & LIMITATIONS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "challenges = {\n",
    "    'Challenge': [\n",
    "        'Class Imbalance',\n",
    "        'Annotation Quality',\n",
    "        'Object Overlap',\n",
    "        'Occlusion',\n",
    "        'Background Complexity',\n",
    "        'Scale Variation',\n",
    "        'Lighting Conditions',\n",
    "        'Limited Dataset Size',\n",
    "    ],\n",
    "    'Impact on Detection': [\n",
    "        'High - Rare classes have poor mAP',\n",
    "        'Medium - Noisy boxes affect IoU',\n",
    "        'High - NMS confusion',\n",
    "        'High - Partial objects missed',\n",
    "        'Medium - False positives',\n",
    "        'High - Small objects missed',\n",
    "        'Medium - Inconsistent features',\n",
    "        'High - Overfitting risk',\n",
    "    ],\n",
    "    'Impact on Segmentation': [\n",
    "        'High - Poor IoU for rare classes',\n",
    "        'High - Mask boundaries uncertain',\n",
    "        'Medium - Pixel assignment ambiguous',\n",
    "        'High - Incomplete masks',\n",
    "        'High - False positive pixels',\n",
    "        'Medium - Low resolution limits detail',\n",
    "        'Medium - Texture confusion',\n",
    "        'High - Limited generalization',\n",
    "    ],\n",
    "    'Mitigation Strategy': [\n",
    "        'Focal loss, weighted sampling',\n",
    "        'Data cleaning, re-annotation',\n",
    "        'Better augmentation, instance separation',\n",
    "        'Augmentation with occlusion',\n",
    "        'Focus on foreground, better normalization',\n",
    "        'Multi-scale training/testing',\n",
    "        'Color augmentation, normalization',\n",
    "        'Data augmentation, regularization',\n",
    "    ]\n",
    "}\n",
    "\n",
    "challenges_df = pd.DataFrame(challenges)\n",
    "print(\"\\n\" + challenges_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "detailed_analysis = \"\"\"\n",
    "1. CLASS IMBALANCE (Critical Issue):\n",
    "   â€¢ Top 5 classes still show imbalance within themselves\n",
    "   â€¢ Dominant classes: May have 3-5Ã— more samples than rare classes\n",
    "   â€¢ Impact: Models biased toward frequent classes\n",
    "   â€¢ Evidence: Lower precision/recall on minority classes\n",
    "\n",
    "2. ANNOTATION QUALITY ISSUES:\n",
    "   â€¢ Inconsistent boundary precision in COCO masks\n",
    "   â€¢ Some objects partially annotated or mislabeled\n",
    "   â€¢ Background pixels sometimes included in masks\n",
    "   â€¢ Impact: Noisy training signal, reduced ceiling performance\n",
    "\n",
    "3. OBJECT OVERLAP & OCCLUSION (Fundamental Challenge):\n",
    "   â€¢ Waste items often piled together in real scenes\n",
    "   â€¢ Partial visibility extremely common\n",
    "   â€¢ Detection: Box overlap creates NMS dilemmas\n",
    "   â€¢ Segmentation: Pixel assignment becomes ambiguous\n",
    "\n",
    "4. BACKGROUND COMPLEXITY:\n",
    "   â€¢ Natural outdoor scenes with varied textures\n",
    "   â€¢ Leaves, grass, dirt can mimic waste textures\n",
    "   â€¢ U-Net particularly susceptible to texture confusion\n",
    "   â€¢ YOLO more robust due to holistic object features\n",
    "\n",
    "5. SCALE VARIATION:\n",
    "   â€¢ Objects range from tiny (caps) to large (bottles, bags)\n",
    "   â€¢ Small objects (<32px) problematic for both models\n",
    "   â€¢ YOLO: Feature pyramid helps but not perfect\n",
    "   â€¢ U-Net: 256Ã—256 resolution limits fine detail capture\n",
    "\n",
    "6. LIMITED DATASET SIZE:\n",
    "   â€¢ TACO subset: ~1200 images for 5 classes\n",
    "   â€¢ After train/val split: ~960 train, ~240 val\n",
    "   â€¢ Risk of overfitting, especially for U-Net (31M params)\n",
    "   â€¢ Mitigation: Strong augmentation, dropout, regularization\n",
    "\n",
    "7. ANNOTATION SUBJECTIVITY:\n",
    "   â€¢ What constitutes \"waste\" boundary is sometimes unclear\n",
    "   â€¢ Shadows, reflections included/excluded inconsistently\n",
    "   â€¢ Different annotators may have different standards\n",
    "   â€¢ Impact: Upper bound on achievable performance\n",
    "\"\"\"\n",
    "\n",
    "print(detailed_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9c6563",
   "metadata": {},
   "source": [
    "### 9.4 Recommendations for Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90adb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"RECOMMENDATIONS FOR IMPROVING MODEL PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "recommendations = {\n",
    "    'Category': [\n",
    "        'Loss Functions',\n",
    "        'Loss Functions',\n",
    "        'Loss Functions',\n",
    "        'Sampling Strategy',\n",
    "        'Sampling Strategy',\n",
    "        'Augmentation',\n",
    "        'Augmentation',\n",
    "        'Architecture',\n",
    "        'Architecture',\n",
    "        'Architecture',\n",
    "        'Training Strategy',\n",
    "        'Training Strategy',\n",
    "        'Training Strategy',\n",
    "        'Data Quality',\n",
    "        'Data Quality',\n",
    "        'Ensemble Methods',\n",
    "    ],\n",
    "    'Technique': [\n",
    "        'Focal Loss (already tested)',\n",
    "        'Class-Weighted Loss',\n",
    "        'Boundary-Aware Loss for U-Net',\n",
    "        'Balanced Batch Sampling',\n",
    "        'Hard Example Mining',\n",
    "        'Test-Time Augmentation (TTA)',\n",
    "        'CutMix/MixUp for Detection',\n",
    "        'Multi-Scale Training/Testing',\n",
    "        'Attention Mechanisms',\n",
    "        'Feature Pyramid Networks',\n",
    "        'Longer Training (100+ epochs)',\n",
    "        'Learning Rate Warmup',\n",
    "        'Mixed Precision Training',\n",
    "        'Re-annotation of Ambiguous Cases',\n",
    "        'Active Learning for Hard Examples',\n",
    "        'Model Ensemble (YOLO + U-Net)',\n",
    "    ],\n",
    "    'Expected Impact': [\n",
    "        'Medium - Helps rare classes',\n",
    "        'High - Direct class imbalance fix',\n",
    "        'Medium - Better edge accuracy',\n",
    "        'High - Even class representation',\n",
    "        'Medium - Focus on difficult cases',\n",
    "        'Medium - Robust predictions',\n",
    "        'Medium - Better generalization',\n",
    "        'High - Handles scale variation',\n",
    "        'Medium - Focus on important regions',\n",
    "        'High - Better multi-scale features',\n",
    "        'Medium - Better convergence',\n",
    "        'Medium - Stable early training',\n",
    "        'Medium - Faster training',\n",
    "        'High - Reduces noise',\n",
    "        'High - Targeted improvements',\n",
    "        'High - Combines strengths',\n",
    "    ],\n",
    "    'Implementation Difficulty': [\n",
    "        'Easy (done)',\n",
    "        'Easy',\n",
    "        'Medium',\n",
    "        'Easy',\n",
    "        'Medium',\n",
    "        'Easy',\n",
    "        'Medium',\n",
    "        'Easy',\n",
    "        'Medium',\n",
    "        'Hard',\n",
    "        'Easy',\n",
    "        'Easy',\n",
    "        'Easy',\n",
    "        'Hard (manual work)',\n",
    "        'Hard (manual work)',\n",
    "        'Medium',\n",
    "    ]\n",
    "}\n",
    "\n",
    "recommendations_df = pd.DataFrame(recommendations)\n",
    "print(\"\\n\" + recommendations_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED RECOMMENDATIONS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "detailed_recommendations = \"\"\"\n",
    "1. LOSS FUNCTION IMPROVEMENTS:\n",
    "   \n",
    "   a) Class-Weighted Cross-Entropy/Dice:\n",
    "      â€¢ Assign higher weights to minority classes\n",
    "      â€¢ Formula: weight_c = 1 / sqrt(frequency_c)\n",
    "      â€¢ Implementation: Pass class weights to loss function\n",
    "      â€¢ Expected gain: +5-10% mAP/IoU on rare classes\n",
    "   \n",
    "   b) Boundary-Aware Loss:\n",
    "      â€¢ Emphasize pixels near object boundaries for U-Net\n",
    "      â€¢ Use distance transform to weight edge pixels higher\n",
    "      â€¢ Helps with precise segmentation boundaries\n",
    "   \n",
    "   c) LovÃ¡sz-Softmax Loss:\n",
    "      â€¢ Directly optimizes IoU (differentiable approximation)\n",
    "      â€¢ Better than Dice for segmentation tasks\n",
    "      â€¢ Can combine with BCE for stability\n",
    "\n",
    "2. SAMPLING STRATEGIES:\n",
    "   \n",
    "   a) Balanced Batch Sampling:\n",
    "      â€¢ Ensure each batch has balanced class representation\n",
    "      â€¢ Prevents bias toward frequent classes\n",
    "      â€¢ Simple to implement with weighted sampling\n",
    "   \n",
    "   b) Hard Example Mining:\n",
    "      â€¢ Focus on images where model performs poorly\n",
    "      â€¢ Select top-k highest loss examples per epoch\n",
    "      â€¢ Particularly useful for YOLO with many anchors\n",
    "\n",
    "3. DATA AUGMENTATION ENHANCEMENTS:\n",
    "   \n",
    "   a) Test-Time Augmentation (TTA):\n",
    "      â€¢ Run multiple augmented versions at inference\n",
    "      â€¢ Average predictions (or use majority vote)\n",
    "      â€¢ Expected gain: +2-3% mAP/IoU with minimal code\n",
    "   \n",
    "   b) CutMix/MixUp:\n",
    "      â€¢ Blend images and labels for regularization\n",
    "      â€¢ Reduces overfitting on small datasets\n",
    "      â€¢ Particularly effective for detection\n",
    "   \n",
    "   c) Advanced Geometric Transforms:\n",
    "      â€¢ Elastic deformations for irregular shapes\n",
    "      â€¢ Perspective transforms for viewpoint variation\n",
    "      â€¢ Grid distortions for robustness\n",
    "\n",
    "4. ARCHITECTURAL IMPROVEMENTS:\n",
    "   \n",
    "   a) Multi-Scale Training:\n",
    "      â€¢ Train with varying input sizes (e.g., 256, 384, 512)\n",
    "      â€¢ Helps model handle scale variation\n",
    "      â€¢ Easy to implement for both YOLO and U-Net\n",
    "   \n",
    "   b) Attention Modules:\n",
    "      â€¢ Self-attention for U-Net skip connections\n",
    "      â€¢ Channel/spatial attention for feature refinement\n",
    "      â€¢ Expected gain: +2-4% performance\n",
    "   \n",
    "   c) Deeper Encoders:\n",
    "      â€¢ Use ResNet50/101 instead of vanilla U-Net encoder\n",
    "      â€¢ Better feature extraction\n",
    "      â€¢ Trade-off: More parameters, slower training\n",
    "\n",
    "5. TRAINING STRATEGY OPTIMIZATIONS:\n",
    "   \n",
    "   a) Extended Training:\n",
    "      â€¢ 100-150 epochs instead of 50\n",
    "      â€¢ More time for convergence, especially U-Net\n",
    "      â€¢ Monitor validation metrics to avoid overfitting\n",
    "   \n",
    "   b) Learning Rate Scheduling:\n",
    "      â€¢ Cosine annealing with warm restarts\n",
    "      â€¢ OneCycleLR for faster convergence\n",
    "      â€¢ Fine-tune final 10 epochs with very low LR\n",
    "   \n",
    "   c) Progressive Resizing:\n",
    "      â€¢ Start with small images (128Ã—128)\n",
    "      â€¢ Gradually increase to full resolution\n",
    "      â€¢ Faster initial training, better final performance\n",
    "\n",
    "6. DATA QUALITY IMPROVEMENTS:\n",
    "   \n",
    "   a) Manual Re-annotation:\n",
    "      â€¢ Review and fix annotations for validation set\n",
    "      â€¢ Establish ground truth for proper evaluation\n",
    "      â€¢ Focus on ambiguous boundary cases\n",
    "   \n",
    "   b) Active Learning:\n",
    "      â€¢ Identify images with highest uncertainty\n",
    "      â€¢ Manually annotate or correct these cases\n",
    "      â€¢ Iteratively improve dataset quality\n",
    "   \n",
    "   c) Synthetic Data:\n",
    "      â€¢ Generate synthetic waste images with perfect labels\n",
    "      â€¢ Use domain randomization for realism\n",
    "      â€¢ Can augment real dataset\n",
    "\n",
    "7. ENSEMBLE METHODS:\n",
    "   \n",
    "   a) Multi-Model Ensemble:\n",
    "      â€¢ Combine YOLO detections with U-Net masks\n",
    "      â€¢ Use YOLO boxes to crop U-Net regions\n",
    "      â€¢ Best of both: Fast detection + precise segmentation\n",
    "   \n",
    "   b) Multi-Loss Ensemble:\n",
    "      â€¢ Train 3 U-Net models: BCE+Dice, Dice, Focal\n",
    "      â€¢ Average predictions at inference\n",
    "      â€¢ Reduces model-specific biases\n",
    "   \n",
    "   c) Snapshot Ensemble:\n",
    "      â€¢ Save checkpoints at different epochs\n",
    "      â€¢ Ensemble predictions from multiple snapshots\n",
    "      â€¢ No extra training time required\n",
    "\n",
    "8. DEPLOYMENT OPTIMIZATIONS:\n",
    "   \n",
    "   a) Model Quantization:\n",
    "      â€¢ INT8 quantization for faster inference\n",
    "      â€¢ Minimal accuracy loss (<1%)\n",
    "      â€¢ 4Ã— speedup on edge devices\n",
    "   \n",
    "   b) Knowledge Distillation:\n",
    "      â€¢ Train smaller \"student\" model from U-Net \"teacher\"\n",
    "      â€¢ Maintain performance with fewer parameters\n",
    "      â€¢ Better for real-time applications\n",
    "   \n",
    "   c) TensorRT/ONNX Optimization:\n",
    "      â€¢ Convert to optimized inference format\n",
    "      â€¢ 2-3Ã— speedup with same accuracy\n",
    "\"\"\"\n",
    "\n",
    "print(detailed_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317d9dc9",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Conclusions & Future Work\n",
    "\n",
    "### 10.1 Project Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PROJECT CONCLUSIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "conclusions = \"\"\"\n",
    "This project successfully implemented and compared two deep learning approaches\n",
    "for waste detection and segmentation on the TACO dataset:\n",
    "\n",
    "1. YOLOv8 OBJECT DETECTION:\n",
    "   â€¢ Architecture: YOLOv8n (3M parameters) trained from scratch\n",
    "   â€¢ Input: 640Ã—640 RGB images\n",
    "   â€¢ Output: Bounding boxes with class labels\n",
    "   â€¢ Training: 50 epochs with baseline (no_aug) and augmented (aug_v1)\n",
    "   â€¢ Best Metrics: Tracked mAP@50, mAP@50-95, Precision, Recall\n",
    "   â€¢ Strengths: Fast inference (~100 FPS), lightweight, good for real-time\n",
    "   â€¢ Weaknesses: Struggles with small objects and tight overlaps\n",
    "\n",
    "2. U-NET SEMANTIC SEGMENTATION:\n",
    "   â€¢ Architecture: Custom encoder-decoder (31M parameters) from scratch\n",
    "   â€¢ Input: 256Ã—256 RGB images\n",
    "   â€¢ Output: 5-channel pixel-wise masks (one-hot encoded)\n",
    "   â€¢ Training: 50 epochs with 4 loss variants (BCE+Dice, Dice, Focal)\n",
    "   â€¢ Best Metrics: Tracked IoU and Dice per class and mean\n",
    "   â€¢ Strengths: Precise boundaries, handles overlaps, irregular shapes\n",
    "   â€¢ Weaknesses: Texture confusion, slower inference (~30 FPS)\n",
    "\n",
    "3. KEY FINDINGS:\n",
    "   a) Data Augmentation Impact:\n",
    "      â€¢ Both models benefited significantly from augmentation\n",
    "      â€¢ Geometric transforms (flips, rotations) most effective\n",
    "      â€¢ Color augmentation helped with lighting variation\n",
    "   \n",
    "   b) Class Imbalance:\n",
    "      â€¢ Significant performance gap between frequent and rare classes\n",
    "      â€¢ Focal loss showed modest improvements for minority classes\n",
    "      â€¢ Weighted sampling recommended for future work\n",
    "   \n",
    "   c) Complementary Strengths:\n",
    "      â€¢ YOLO: Better for object counting and localization speed\n",
    "      â€¢ U-Net: Better for precise area measurement and overlap handling\n",
    "      â€¢ Ensemble approach could leverage both strengths\n",
    "\n",
    "4. REPRODUCIBILITY:\n",
    "   â€¢ All random seeds fixed (PYTHONHASHSEED=0, seed=42)\n",
    "   â€¢ Deterministic training enabled for PyTorch\n",
    "   â€¢ Complete environment captured in requirements.txt\n",
    "   â€¢ Code organized in single notebook for easy reproduction\n",
    "\n",
    "5. PRACTICAL IMPACT:\n",
    "   â€¢ Models can support automated waste sorting systems\n",
    "   â€¢ YOLO for real-time waste detection cameras\n",
    "   â€¢ U-Net for precise contamination analysis\n",
    "   â€¢ Combined system: YOLO detects â†’ U-Net segments â†’ Robotic sorting\n",
    "\n",
    "6. LIMITATIONS:\n",
    "   â€¢ Limited dataset size (~1200 images, 5 classes)\n",
    "   â€¢ Annotation quality varies across samples\n",
    "   â€¢ Models trained only on top 5 classes (not full 60)\n",
    "   â€¢ No cross-dataset validation (TACO-specific)\n",
    "   â€¢ Computational constraints limited epochs and resolution\n",
    "\n",
    "7. ACHIEVEMENT SUMMARY:\n",
    "   âœ“ Comprehensive EDA with class distribution analysis\n",
    "   âœ“ Three augmentation pipelines implemented and compared\n",
    "   âœ“ YOLOv8 detection with baseline and augmented training\n",
    "   âœ“ U-Net segmentation with 4 loss function experiments\n",
    "   âœ“ Per-class metrics and visualizations for both models\n",
    "   âœ“ Detailed comparison and failure case analysis\n",
    "   âœ“ Full reproducibility with seeds and environment specs\n",
    "\"\"\"\n",
    "\n",
    "print(conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c468a1",
   "metadata": {},
   "source": [
    "### 10.2 Future Work & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8852659",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FUTURE WORK & RESEARCH DIRECTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "future_work = \"\"\"\n",
    "SHORT-TERM IMPROVEMENTS (1-2 weeks):\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "1. Implement Test-Time Augmentation (TTA)\n",
    "   â€¢ Augment test images with flips/rotations\n",
    "   â€¢ Average predictions across augmented versions\n",
    "   â€¢ Expected gain: +2-3% mAP/IoU\n",
    "   â€¢ Implementation time: 1-2 hours\n",
    "\n",
    "2. Class-Weighted Loss Functions\n",
    "   â€¢ Calculate inverse class frequencies\n",
    "   â€¢ Weight loss by class rarity\n",
    "   â€¢ Re-train both models\n",
    "   â€¢ Expected gain: +5-10% on minority classes\n",
    "\n",
    "3. Extended Training\n",
    "   â€¢ Train for 100 epochs instead of 50\n",
    "   â€¢ Monitor validation curves for convergence\n",
    "   â€¢ Fine-tune learning rate schedule\n",
    "   â€¢ Expected gain: +3-5% overall performance\n",
    "\n",
    "4. Multi-Scale Training\n",
    "   â€¢ YOLO: Train with [512, 576, 640] image sizes\n",
    "   â€¢ U-Net: Train with [256, 384, 512] image sizes\n",
    "   â€¢ Better handling of scale variation\n",
    "   â€¢ Expected gain: +4-6% on small objects\n",
    "\n",
    "MEDIUM-TERM ENHANCEMENTS (1-2 months):\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "5. Ensemble Methods\n",
    "   â€¢ Train 3-5 models with different seeds\n",
    "   â€¢ Average predictions for robustness\n",
    "   â€¢ Combine YOLO + U-Net outputs\n",
    "   â€¢ Expected gain: +5-8% overall\n",
    "\n",
    "6. Architecture Improvements\n",
    "   â€¢ YOLOv8m or YOLOv8l (larger variants)\n",
    "   â€¢ U-Net with ResNet50/101 encoder\n",
    "   â€¢ Attention mechanisms in U-Net\n",
    "   â€¢ Expected gain: +6-10% with more compute\n",
    "\n",
    "7. Advanced Augmentations\n",
    "   â€¢ CutMix and MixUp for regularization\n",
    "   â€¢ AutoAugment for learned policies\n",
    "   â€¢ Synthetic data generation (GANs)\n",
    "   â€¢ Expected gain: +3-5% generalization\n",
    "\n",
    "8. Active Learning Pipeline\n",
    "   â€¢ Identify high-uncertainty predictions\n",
    "   â€¢ Manually annotate difficult cases\n",
    "   â€¢ Re-train iteratively\n",
    "   â€¢ Expected gain: +8-12% on hard examples\n",
    "\n",
    "LONG-TERM RESEARCH (3-6 months):\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "9. Expand to All 60 TACO Classes\n",
    "   â€¢ Train on full dataset\n",
    "   â€¢ Hierarchical classification (material types)\n",
    "   â€¢ Multi-label support (composite objects)\n",
    "   â€¢ Real-world deployment readiness\n",
    "\n",
    "10. Instance Segmentation\n",
    "    â€¢ Implement Mask R-CNN or YOLOv8-seg\n",
    "    â€¢ Combines detection + segmentation\n",
    "    â€¢ Best of both worlds\n",
    "    â€¢ Industry-standard approach\n",
    "\n",
    "11. Temporal Models (Video)\n",
    "    â€¢ Extend to video stream processing\n",
    "    â€¢ Track waste objects over time\n",
    "    â€¢ Count objects in conveyor belt systems\n",
    "    â€¢ Real-time waste sorting application\n",
    "\n",
    "12. Domain Adaptation\n",
    "    â€¢ Train on TACO, test on other waste datasets\n",
    "    â€¢ Study generalization to new environments\n",
    "    â€¢ Transfer learning across countries/regions\n",
    "    â€¢ Robust deployment in varied conditions\n",
    "\n",
    "13. Edge Deployment\n",
    "    â€¢ Model quantization (INT8)\n",
    "    â€¢ Knowledge distillation (smaller models)\n",
    "    â€¢ Deploy on Raspberry Pi / Jetson Nano\n",
    "    â€¢ Real-time inference on embedded devices\n",
    "\n",
    "14. Interactive Annotation Tool\n",
    "    â€¢ Build web app for waste annotation\n",
    "    â€¢ Semi-automatic labeling with model assistance\n",
    "    â€¢ Crowdsource high-quality annotations\n",
    "    â€¢ Improve TACO dataset quality\n",
    "\n",
    "15. Waste Classification Hierarchy\n",
    "    â€¢ Material type (plastic, metal, paper, glass)\n",
    "    â€¢ Recyclability (recyclable, non-recyclable, hazard)\n",
    "    â€¢ Size category (small, medium, large)\n",
    "    â€¢ Multi-task learning framework\n",
    "\n",
    "RESEARCH QUESTIONS:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "â€¢ How does model performance degrade with dataset size reduction?\n",
    "â€¢ What is the optimal augmentation strategy for waste detection?\n",
    "â€¢ Can weakly-supervised learning reduce annotation cost?\n",
    "â€¢ How well do models generalize to new geographic regions?\n",
    "â€¢ What is the minimum model size for acceptable performance?\n",
    "â€¢ Can few-shot learning enable new class detection with minimal data?\n",
    "\n",
    "POTENTIAL APPLICATIONS:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "1. Automated Recycling Facilities\n",
    "   â€¢ Sort waste on conveyor belts\n",
    "   â€¢ Robotic arm guidance\n",
    "   â€¢ Quality control for contamination\n",
    "\n",
    "2. Beach/Park Cleanup Monitoring\n",
    "   â€¢ Drone-based waste detection\n",
    "   â€¢ Track cleanup effectiveness over time\n",
    "   â€¢ Identify hotspots for targeted cleanup\n",
    "\n",
    "3. Smart Waste Bins\n",
    "   â€¢ Classify waste at disposal point\n",
    "   â€¢ Guide users to correct bins\n",
    "   â€¢ Prevent contamination\n",
    "\n",
    "4. Environmental Monitoring\n",
    "   â€¢ Track waste accumulation trends\n",
    "   â€¢ Identify illegal dumping sites\n",
    "   â€¢ Generate reports for municipalities\n",
    "\n",
    "5. Educational Tools\n",
    "   â€¢ Mobile app for waste identification\n",
    "   â€¢ Teach proper recycling practices\n",
    "   â€¢ Gamify waste sorting education\n",
    "\"\"\"\n",
    "\n",
    "print(future_work)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"THANK YOU!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nProject: YOLOv8 + U-Net Waste Detection and Segmentation\")\n",
    "print(\"Course: CS4045 Deep Learning for Perception\")\n",
    "print(\"Team: Minahil Ali (22i-0849), Ayaan Khan (22i-0832)\")\n",
    "print(\"Date: November 16, 2025\")\n",
    "print(\"\\nRepository: https://github.com/minahilali117/yolov8-unet-waste-detection-and-segmentation\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4b1f2",
   "metadata": {},
   "source": [
    "---\n",
    "## End of Notebook\n",
    "\n",
    "**Project:** YOLOv8 + U-Net Waste Detection and Segmentation  \n",
    "**Course:** Deep Learning for Perception (CS4045)  \n",
    "**Authors:** Minahil Ali (22i-0849), Ayaan Khan (22i-0832)  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
