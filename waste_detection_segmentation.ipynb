{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d412467",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup & Random Seed Configuration](#1.-Environment-Setup-&-Random-Seed-Configuration)\n",
    "2. [Import Libraries](#2.-Import-Libraries)\n",
    "3. [Dataset Download & Verification](#3.-Dataset-Download-&-Verification)\n",
    "4. [Exploratory Data Analysis (EDA)](#4.-Exploratory-Data-Analysis-(EDA))\n",
    "5. [Data Preprocessing & Subset Creation](#5.-Data-Preprocessing-&-Subset-Creation)\n",
    "6. [Data Augmentation](#6.-Data-Augmentation)\n",
    "7. [YOLOv8 Object Detection](#7.-YOLOv8-Object-Detection)\n",
    "8. [U-Net Semantic Segmentation](#8.-U-Net-Semantic-Segmentation)\n",
    "9. [Model Comparison & Discussion](#9.-Model-Comparison-&-Discussion)\n",
    "10. [Conclusions & Future Work](#10.-Conclusions-&-Future-Work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d94f6",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup & Random Seed Configuration\n",
    "\n",
    "Setting all random seeds for reproducibility across:\n",
    "- Python's built-in random module\n",
    "- NumPy\n",
    "- PyTorch (CPU and CUDA)\n",
    "- Python hash seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable for Python hash seed (must be done before importing libraries)\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# Import random libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# PyTorch seeds (will be set after importing torch)\n",
    "print(\"âœ“ Python hash seed set to 0\")\n",
    "print(f\"âœ“ Random seed set to {RANDOM_SEED}\")\n",
    "print(f\"âœ“ NumPy seed set to {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch and set its random seeds\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Additional PyTorch reproducibility settings\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"âœ“ PyTorch seed set to {RANDOM_SEED}\")\n",
    "print(f\"âœ“ PyTorch CUDA seed set to {RANDOM_SEED}\")\n",
    "print(\"âœ“ CUDNN deterministic mode enabled\")\n",
    "print(\"âœ“ CUDNN benchmark disabled for reproducibility\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nâœ“ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5496c",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Import Libraries\n",
    "\n",
    "Importing all required libraries for:\n",
    "- Data manipulation and analysis\n",
    "- Image processing\n",
    "- Deep learning (PyTorch, YOLOv8)\n",
    "- Visualization\n",
    "- COCO dataset handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# YOLOv8\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "# COCO tools\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as coco_mask\n",
    "\n",
    "# Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "print(f\"\\nLibrary Versions:\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  Torchvision: {torchvision.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  Pandas: {pd.__version__}\")\n",
    "print(f\"  OpenCV: {cv2.__version__}\")\n",
    "print(f\"  Albumentations: {A.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2c86b0",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Dataset Download & Verification\n",
    "\n",
    "### 3.1 Download TACO Dataset\n",
    "\n",
    "The TACO dataset can be downloaded using:\n",
    "1. **Kaggle API** (recommended - automated)\n",
    "2. **Manual download** from Kaggle website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project directories\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "TACO_DIR = DATA_DIR / 'TACO'\n",
    "RUNS_DIR = PROJECT_ROOT / 'runs'\n",
    "WEIGHTS_DIR = PROJECT_ROOT / 'weights'\n",
    "\n",
    "# Create directories\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "RUNS_DIR.mkdir(exist_ok=True)\n",
    "WEIGHTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Project directory structure:\")\n",
    "print(f\"  Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"  Data Directory: {DATA_DIR}\")\n",
    "print(f\"  TACO Directory: {TACO_DIR}\")\n",
    "print(f\"  Runs Directory: {RUNS_DIR}\")\n",
    "print(f\"  Weights Directory: {WEIGHTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset already exists\n",
    "if TACO_DIR.exists() and len(list(TACO_DIR.glob('*'))) > 0:\n",
    "    print(\"âœ“ TACO dataset already exists!\")\n",
    "    print(f\"  Location: {TACO_DIR}\")\n",
    "else:\n",
    "    print(\"Dataset not found. Please download using one of the following methods:\\n\")\n",
    "    \n",
    "    print(\"Method 1: Kaggle API (Recommended)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"1. Install Kaggle API: pip install kaggle\")\n",
    "    print(\"2. Setup Kaggle credentials (kaggle.json)\")\n",
    "    print(\"3. Run the following commands:\\n\")\n",
    "    print(\"   kaggle datasets download -d kneroma/tacotrashdataset\")\n",
    "    print(f\"   unzip tacotrashdataset.zip -d {DATA_DIR}\")\n",
    "    print(\"\\nMethod 2: Manual Download\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"1. Visit: https://www.kaggle.com/datasets/kneroma/tacotrashdataset\")\n",
    "    print(\"2. Download the dataset\")\n",
    "    print(f\"3. Extract to: {DATA_DIR}\")\n",
    "    print(\"\\nNote: Uncomment and run the cell below to download via Kaggle API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download using Kaggle API\n",
    "# !pip install kaggle\n",
    "# !kaggle datasets download -d kneroma/tacotrashdataset\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile('tacotrashdataset.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall(DATA_DIR)\n",
    "# print(\"âœ“ Dataset downloaded and extracted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e17cc2",
   "metadata": {},
   "source": [
    "### 3.2 Verify Dataset Structure and COCO Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa52c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "if TACO_DIR.exists():\n",
    "    print(\"Dataset Structure:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # List all subdirectories and files\n",
    "    for item in sorted(TACO_DIR.glob('*')):\n",
    "        if item.is_dir():\n",
    "            file_count = len(list(item.glob('*')))\n",
    "            print(f\"ðŸ“ {item.name}/ ({file_count} items)\")\n",
    "        else:\n",
    "            file_size = item.stat().st_size / (1024 * 1024)  # Convert to MB\n",
    "            print(f\"ðŸ“„ {item.name} ({file_size:.2f} MB)\")\n",
    "    \n",
    "    # Look for annotation files\n",
    "    annotation_files = list(TACO_DIR.rglob('*.json'))\n",
    "    print(f\"\\nFound {len(annotation_files)} annotation file(s):\")\n",
    "    for ann_file in annotation_files:\n",
    "        print(f\"  - {ann_file.relative_to(TACO_DIR)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Dataset not found. Please download the dataset first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcad93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify COCO annotations\n",
    "# Note: Update the annotation file path based on actual dataset structure\n",
    "\n",
    "# Common TACO annotation file paths\n",
    "possible_ann_paths = [\n",
    "    TACO_DIR / 'annotations.json',\n",
    "    TACO_DIR / 'annotations' / 'instances_default.json',\n",
    "    TACO_DIR / 'TACO' / 'annotations.json',\n",
    "]\n",
    "\n",
    "ANNOTATION_FILE = None\n",
    "for path in possible_ann_paths:\n",
    "    if path.exists():\n",
    "        ANNOTATION_FILE = path\n",
    "        break\n",
    "\n",
    "if ANNOTATION_FILE:\n",
    "    print(f\"âœ“ Found annotation file: {ANNOTATION_FILE.name}\")\n",
    "    \n",
    "    # Load COCO annotations\n",
    "    with open(ANNOTATION_FILE, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    print(\"\\nCOCO Format Verification:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"  Images: {len(coco_data.get('images', []))}\")\n",
    "    print(f\"  Annotations: {len(coco_data.get('annotations', []))}\")\n",
    "    print(f\"  Categories: {len(coco_data.get('categories', []))}\")\n",
    "    \n",
    "    # Display first few categories\n",
    "    print(\"\\nSample Categories:\")\n",
    "    for cat in coco_data.get('categories', [])[:10]:\n",
    "        print(f\"  ID {cat['id']}: {cat['name']}\")\n",
    "    \n",
    "    if len(coco_data.get('categories', [])) > 10:\n",
    "        print(f\"  ... and {len(coco_data.get('categories', [])) - 10} more\")\n",
    "    \n",
    "    print(\"\\nâœ“ COCO format verified successfully!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Annotation file not found. Please verify dataset structure.\")\n",
    "    print(\"   Expected locations:\")\n",
    "    for path in possible_ann_paths:\n",
    "        print(f\"   - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a2c73a",
   "metadata": {},
   "source": [
    "### 3.3 Find Image Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate image directory\n",
    "possible_img_dirs = [\n",
    "    TACO_DIR / 'images',\n",
    "    TACO_DIR / 'data',\n",
    "    TACO_DIR / 'TACO' / 'images',\n",
    "    TACO_DIR,\n",
    "]\n",
    "\n",
    "IMAGE_DIR = None\n",
    "for img_dir in possible_img_dirs:\n",
    "    if img_dir.exists():\n",
    "        # Check if directory contains image files\n",
    "        img_files = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))\n",
    "        if len(img_files) > 0:\n",
    "            IMAGE_DIR = img_dir\n",
    "            print(f\"âœ“ Found image directory: {IMAGE_DIR.name}\")\n",
    "            print(f\"  Total images: {len(img_files)}\")\n",
    "            break\n",
    "\n",
    "if not IMAGE_DIR:\n",
    "    print(\"âš ï¸ Image directory not found. Please verify dataset structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ad8fc",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 4.1 Load COCO Annotations\n",
    "\n",
    "Loading the TACO dataset annotations and extracting key statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cb016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO annotations\n",
    "if ANNOTATION_FILE and ANNOTATION_FILE.exists():\n",
    "    print(f\"Loading annotations from: {ANNOTATION_FILE.name}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize COCO API\n",
    "    coco = COCO(str(ANNOTATION_FILE))\n",
    "    \n",
    "    # Get all category IDs and image IDs\n",
    "    cat_ids = coco.getCatIds()\n",
    "    img_ids = coco.getImgIds()\n",
    "    ann_ids = coco.getAnnIds()\n",
    "    \n",
    "    # Load categories and images\n",
    "    categories = coco.loadCats(cat_ids)\n",
    "    images = coco.loadImgs(img_ids)\n",
    "    annotations = coco.loadAnns(ann_ids)\n",
    "    \n",
    "    print(f\"\\nâœ“ Successfully loaded TACO dataset!\")\n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total Images: {len(images)}\")\n",
    "    print(f\"  Total Annotations: {len(annotations)}\")\n",
    "    print(f\"  Total Categories: {len(categories)}\")\n",
    "    print(f\"  Average annotations per image: {len(annotations)/len(images):.2f}\")\n",
    "    \n",
    "    # Create category name mapping\n",
    "    cat_id_to_name = {cat['id']: cat['name'] for cat in categories}\n",
    "    cat_name_to_id = {cat['name']: cat['id'] for cat in categories}\n",
    "    \n",
    "    print(f\"\\nâœ“ Category mapping created\")\n",
    "    print(f\"  Sample categories: {list(cat_id_to_name.items())[:5]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Annotation file not found. Please download the TACO dataset first.\")\n",
    "    print(\"Stopping execution - dataset is required for EDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997480c",
   "metadata": {},
   "source": [
    "### 4.2 Compute Class Frequencies and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class frequencies\n",
    "class_counts = Counter()\n",
    "image_object_counts = defaultdict(int)\n",
    "\n",
    "for ann in annotations:\n",
    "    class_counts[ann['category_id']] += 1\n",
    "    image_object_counts[ann['image_id']] += 1\n",
    "\n",
    "# Convert to sorted list\n",
    "class_freq_list = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "class_freq_df = pd.DataFrame([\n",
    "    {\n",
    "        'Category ID': cat_id,\n",
    "        'Category Name': cat_id_to_name.get(cat_id, 'Unknown'),\n",
    "        'Count': count,\n",
    "        'Percentage': (count / len(annotations)) * 100\n",
    "    }\n",
    "    for cat_id, count in class_freq_list\n",
    "])\n",
    "\n",
    "print(\"Class Frequency Distribution (Top 20):\")\n",
    "print(\"=\" * 70)\n",
    "print(class_freq_df.head(20).to_string(index=False))\n",
    "\n",
    "print(f\"\\n\\nTop 5 Most Frequent Classes (IDs 0-4 as required):\")\n",
    "print(\"=\" * 70)\n",
    "top_5_classes = class_freq_list[:5]\n",
    "for rank, (cat_id, count) in enumerate(top_5_classes, 1):\n",
    "    cat_name = cat_id_to_name.get(cat_id, 'Unknown')\n",
    "    percentage = (count / len(annotations)) * 100\n",
    "    print(f\"{rank}. ID {cat_id}: {cat_name:30s} - {count:5d} annotations ({percentage:5.2f}%)\")\n",
    "\n",
    "# Store top 5 class IDs for later use\n",
    "TOP_5_CLASS_IDS = [cat_id for cat_id, _ in top_5_classes]\n",
    "print(f\"\\nâœ“ Top 5 class IDs stored: {TOP_5_CLASS_IDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c412e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze objects per image statistics\n",
    "objects_per_img = list(image_object_counts.values())\n",
    "\n",
    "print(\"Objects per Image Statistics:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Mean objects per image: {np.mean(objects_per_img):.2f}\")\n",
    "print(f\"  Median objects per image: {np.median(objects_per_img):.2f}\")\n",
    "print(f\"  Std deviation: {np.std(objects_per_img):.2f}\")\n",
    "print(f\"  Min objects in an image: {min(objects_per_img)}\")\n",
    "print(f\"  Max objects in an image: {max(objects_per_img)}\")\n",
    "print(f\"  Total images: {len(image_object_counts)}\")\n",
    "\n",
    "# Distribution breakdown\n",
    "dist_breakdown = Counter(objects_per_img)\n",
    "print(f\"\\nDistribution breakdown:\")\n",
    "for num_objs in sorted(dist_breakdown.keys())[:10]:\n",
    "    count = dist_breakdown[num_objs]\n",
    "    print(f\"  {num_objs} object(s): {count} images ({count/len(images)*100:.1f}%)\")\n",
    "if len(dist_breakdown) > 10:\n",
    "    print(f\"  ... and {len(dist_breakdown) - 10} more categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0438843f",
   "metadata": {},
   "source": [
    "### 4.3 Visualization: Class Distribution Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart for top 20 classes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Plot 1: Top 20 classes\n",
    "top_20_df = class_freq_df.head(20)\n",
    "bars1 = ax1.barh(range(len(top_20_df)), top_20_df['Count'], color='steelblue')\n",
    "\n",
    "# Highlight top 5 classes\n",
    "for i in range(min(5, len(top_20_df))):\n",
    "    bars1[i].set_color('coral')\n",
    "    bars1[i].set_edgecolor('darkred')\n",
    "    bars1[i].set_linewidth(2)\n",
    "\n",
    "ax1.set_yticks(range(len(top_20_df)))\n",
    "ax1.set_yticklabels([f\"ID {row['Category ID']}: {row['Category Name'][:25]}\" \n",
    "                      for _, row in top_20_df.iterrows()])\n",
    "ax1.set_xlabel('Number of Annotations', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Top 20 Classes by Frequency\\n(Top 5 highlighted in coral)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, (_, row) in enumerate(top_20_df.iterrows()):\n",
    "    ax1.text(row['Count'], i, f\" {row['Count']}\", \n",
    "             va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 2: Top 5 classes only (required subset)\n",
    "top_5_df = class_freq_df.head(5)\n",
    "bars2 = ax2.bar(range(len(top_5_df)), top_5_df['Count'], \n",
    "                color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8'],\n",
    "                edgecolor='black', linewidth=2)\n",
    "\n",
    "ax2.set_xticks(range(len(top_5_df)))\n",
    "ax2.set_xticklabels([f\"ID {row['Category ID']}\\n{row['Category Name'][:20]}\" \n",
    "                      for _, row in top_5_df.iterrows()], rotation=45, ha='right')\n",
    "ax2.set_ylabel('Number of Annotations', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Top 5 Most Frequent Classes (Subset for Training)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count and percentage labels\n",
    "for i, (_, row) in enumerate(top_5_df.iterrows()):\n",
    "    ax2.text(i, row['Count'], f\"{row['Count']}\\n({row['Percentage']:.1f}%)\", \n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RUNS_DIR / 'class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Class distribution chart saved to: {RUNS_DIR / 'class_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be04759",
   "metadata": {},
   "source": [
    "### 4.4 Visualization: Objects per Image Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram of objects per image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Histogram with all data\n",
    "ax1.hist(objects_per_img, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(np.mean(objects_per_img), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {np.mean(objects_per_img):.2f}')\n",
    "ax1.axvline(np.median(objects_per_img), color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Median: {np.median(objects_per_img):.2f}')\n",
    "ax1.set_xlabel('Number of Objects per Image', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Frequency (Number of Images)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distribution of Objects per Image (All Data)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Box plot for better visualization of outliers\n",
    "ax2.boxplot(objects_per_img, vert=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightcoral', alpha=0.7),\n",
    "            medianprops=dict(color='darkred', linewidth=2),\n",
    "            whiskerprops=dict(linewidth=1.5),\n",
    "            capprops=dict(linewidth=1.5))\n",
    "ax2.set_ylabel('Number of Objects per Image', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Box Plot: Objects per Image\\n(Shows outliers and quartiles)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add statistics text\n",
    "stats_text = f\"Min: {min(objects_per_img)}\\n\"\n",
    "stats_text += f\"Q1: {np.percentile(objects_per_img, 25):.1f}\\n\"\n",
    "stats_text += f\"Median: {np.median(objects_per_img):.1f}\\n\"\n",
    "stats_text += f\"Q3: {np.percentile(objects_per_img, 75):.1f}\\n\"\n",
    "stats_text += f\"Max: {max(objects_per_img)}\"\n",
    "ax2.text(1.15, np.median(objects_per_img), stats_text, \n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "         fontsize=10, verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RUNS_DIR / 'objects_per_image_histogram.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Objects per image histogram saved to: {RUNS_DIR / 'objects_per_image_histogram.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ffc6b",
   "metadata": {},
   "source": [
    "### 4.5 Visualization: Sample Images with Bounding Boxes and Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f510864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to visualize image with annotations\n",
    "def visualize_annotations(img_id, coco_obj, img_dir, ax_bbox=None, ax_mask=None):\n",
    "    \"\"\"\n",
    "    Visualize bounding boxes and segmentation masks for a given image.\n",
    "    \n",
    "    Args:\n",
    "        img_id: COCO image ID\n",
    "        coco_obj: COCO API object\n",
    "        img_dir: Path to image directory\n",
    "        ax_bbox: Matplotlib axis for bounding box visualization\n",
    "        ax_mask: Matplotlib axis for mask visualization\n",
    "    \"\"\"\n",
    "    # Load image info\n",
    "    img_info = coco_obj.loadImgs(img_id)[0]\n",
    "    img_path = img_dir / img_info['file_name']\n",
    "    \n",
    "    # Check if image exists\n",
    "    if not img_path.exists():\n",
    "        print(f\"Warning: Image not found: {img_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Load image\n",
    "    img = cv2.imread(str(img_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get annotations for this image\n",
    "    ann_ids = coco_obj.getAnnIds(imgIds=img_id)\n",
    "    anns = coco_obj.loadAnns(ann_ids)\n",
    "    \n",
    "    # Visualization with bounding boxes\n",
    "    if ax_bbox is not None:\n",
    "        ax_bbox.imshow(img)\n",
    "        ax_bbox.axis('off')\n",
    "        \n",
    "        for ann in anns:\n",
    "            # Draw bounding box\n",
    "            bbox = ann['bbox']  # [x, y, width, height]\n",
    "            rect = patches.Rectangle(\n",
    "                (bbox[0], bbox[1]), bbox[2], bbox[3],\n",
    "                linewidth=2, edgecolor='lime', facecolor='none'\n",
    "            )\n",
    "            ax_bbox.add_patch(rect)\n",
    "            \n",
    "            # Add label\n",
    "            cat_name = cat_id_to_name.get(ann['category_id'], 'Unknown')\n",
    "            ax_bbox.text(\n",
    "                bbox[0], bbox[1] - 5,\n",
    "                f\"{cat_name[:15]}\", \n",
    "                color='white', fontsize=8, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='lime', alpha=0.7)\n",
    "            )\n",
    "        \n",
    "        ax_bbox.set_title(f\"Bounding Boxes\\n{img_info['file_name']}\\n{len(anns)} objects\", \n",
    "                         fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Visualization with segmentation masks\n",
    "    if ax_mask is not None:\n",
    "        ax_mask.imshow(img)\n",
    "        ax_mask.axis('off')\n",
    "        \n",
    "        # Create colored mask overlay\n",
    "        mask_img = np.zeros_like(img, dtype=np.float32)\n",
    "        \n",
    "        for idx, ann in enumerate(anns):\n",
    "            if 'segmentation' in ann and ann['segmentation']:\n",
    "                # Generate random color for this annotation\n",
    "                color = np.random.rand(3)\n",
    "                \n",
    "                # Handle different segmentation formats\n",
    "                if isinstance(ann['segmentation'], list):\n",
    "                    # Polygon format\n",
    "                    for seg in ann['segmentation']:\n",
    "                        poly = np.array(seg).reshape(-1, 2).astype(np.int32)\n",
    "                        cv2.fillPoly(mask_img, [poly], color)\n",
    "                elif isinstance(ann['segmentation'], dict):\n",
    "                    # RLE format\n",
    "                    rle = ann['segmentation']\n",
    "                    mask = coco_mask.decode(rle)\n",
    "                    mask_img[mask > 0] = color\n",
    "        \n",
    "        # Overlay mask on image\n",
    "        alpha = 0.5\n",
    "        overlaid = (img * (1 - alpha) + mask_img * 255 * alpha).astype(np.uint8)\n",
    "        ax_mask.imshow(overlaid)\n",
    "        \n",
    "        ax_mask.set_title(f\"Segmentation Masks\\n{img_info['file_name']}\\n{len(anns)} objects\", \n",
    "                         fontsize=10, fontweight='bold')\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"âœ“ Visualization helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03041d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and visualize 12 example images (2 rows x 6 images)\n",
    "# Try to get images with varying number of objects\n",
    "sample_img_ids = []\n",
    "for target_count in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "    # Find images with approximately this many objects\n",
    "    candidates = [img_id for img_id, count in image_object_counts.items() \n",
    "                 if abs(count - target_count) <= 2]\n",
    "    if candidates:\n",
    "        sample_img_ids.append(random.choice(candidates))\n",
    "    \n",
    "    if len(sample_img_ids) >= 12:\n",
    "        break\n",
    "\n",
    "# If we don't have enough, just take random ones\n",
    "if len(sample_img_ids) < 12:\n",
    "    remaining = 12 - len(sample_img_ids)\n",
    "    additional = random.sample(list(image_object_counts.keys()), remaining)\n",
    "    sample_img_ids.extend(additional)\n",
    "\n",
    "sample_img_ids = sample_img_ids[:12]\n",
    "\n",
    "print(f\"Selected {len(sample_img_ids)} sample images for visualization\")\n",
    "print(f\"Sample image IDs: {sample_img_ids[:6]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with bounding boxes\n",
    "fig, axes = plt.subplots(2, 6, figsize=(24, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "print(\"Visualizing bounding boxes...\")\n",
    "for idx, img_id in enumerate(sample_img_ids):\n",
    "    visualize_annotations(img_id, coco, IMAGE_DIR, ax_bbox=axes[idx])\n",
    "\n",
    "plt.suptitle('Sample Images with Bounding Boxes', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RUNS_DIR / 'sample_images_bboxes.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Bounding box visualization saved to: {RUNS_DIR / 'sample_images_bboxes.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aec710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with segmentation masks\n",
    "fig, axes = plt.subplots(2, 6, figsize=(24, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "print(\"Visualizing segmentation masks...\")\n",
    "for idx, img_id in enumerate(sample_img_ids):\n",
    "    visualize_annotations(img_id, coco, IMAGE_DIR, ax_mask=axes[idx])\n",
    "\n",
    "plt.suptitle('Sample Images with Segmentation Masks', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RUNS_DIR / 'sample_images_masks.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Segmentation mask visualization saved to: {RUNS_DIR / 'sample_images_masks.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932ea04",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Data Preprocessing & Subset Creation\n",
    "\n",
    "### 5.1 Why Use Only 5 Classes?\n",
    "\n",
    "**Justification for selecting top 5 classes:**\n",
    "\n",
    "1. **Less Label Noise**: Focusing on the most frequent classes reduces annotation inconsistencies and mislabeling that are more common in rare classes.\n",
    "\n",
    "2. **More Samples Per Class**: Top 5 classes have significantly more training examples, enabling better model learning and generalization.\n",
    "\n",
    "3. **Simpler Decision Boundary**: Fewer classes reduce inter-class confusion and make it easier for the model to learn discriminative features.\n",
    "\n",
    "4. **Balanced Training**: The top 5 classes together represent a substantial portion of the dataset, providing sufficient diversity while maintaining focus.\n",
    "\n",
    "5. **Computational Efficiency**: Training on fewer classes speeds up experimentation and allows for more thorough hyperparameter tuning.\n",
    "\n",
    "6. **Better Evaluation Metrics**: With more samples per class, evaluation metrics (Precision, Recall, mAP, IoU) are more reliable and statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f071ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.2 Filter Dataset to Top 5 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10003c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter annotations to keep only top 5 classes\n",
    "filtered_annotations = [ann for ann in annotations if ann['category_id'] in TOP_5_CLASS_IDS]\n",
    "\n",
    "# Get image IDs that have at least one annotation from top 5 classes\n",
    "filtered_img_ids = set(ann['image_id'] for ann in filtered_annotations)\n",
    "\n",
    "# Filter images\n",
    "filtered_images = [img for img in images if img['id'] in filtered_img_ids]\n",
    "\n",
    "# Filter categories to top 5\n",
    "filtered_categories = [cat for cat in categories if cat['id'] in TOP_5_CLASS_IDS]\n",
    "\n",
    "print(\"Subset Filtering Results:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Original dataset:\")\n",
    "print(f\"  Images: {len(images)}\")\n",
    "print(f\"  Annotations: {len(annotations)}\")\n",
    "print(f\"  Categories: {len(categories)}\")\n",
    "print(f\"\\nFiltered subset (Top 5 classes):\")\n",
    "print(f\"  Images: {len(filtered_images)} ({len(filtered_images)/len(images)*100:.1f}%)\")\n",
    "print(f\"  Annotations: {len(filtered_annotations)} ({len(filtered_annotations)/len(annotations)*100:.1f}%)\")\n",
    "print(f\"  Categories: {len(filtered_categories)}\")\n",
    "print(f\"\\nTop 5 classes retained:\")\n",
    "for cat_id in TOP_5_CLASS_IDS:\n",
    "    cat_name = cat_id_to_name.get(cat_id, 'Unknown')\n",
    "    count = sum(1 for ann in filtered_annotations if ann['category_id'] == cat_id)\n",
    "    print(f\"  ID {cat_id}: {cat_name:30s} - {count:5d} annotations\")\n",
    "\n",
    "# Compute statistics for filtered dataset\n",
    "filtered_img_obj_counts = Counter()\n",
    "for ann in filtered_annotations:\n",
    "    filtered_img_obj_counts[ann['image_id']] += 1\n",
    "\n",
    "print(f\"\\nFiltered dataset statistics:\")\n",
    "print(f\"  Avg objects per image: {len(filtered_annotations)/len(filtered_images):.2f}\")\n",
    "print(f\"  Min objects per image: {min(filtered_img_obj_counts.values())}\")\n",
    "print(f\"  Max objects per image: {max(filtered_img_obj_counts.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89610864",
   "metadata": {},
   "source": [
    "### 5.3 Create TACO Subset Directory and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5636eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset directory structure\n",
    "SUBSET_DIR = DATA_DIR / 'taco_subset'\n",
    "SUBSET_IMAGES_DIR = SUBSET_DIR / 'images'\n",
    "SUBSET_ANN_FILE = SUBSET_DIR / 'annotations.json'\n",
    "\n",
    "# Create directories\n",
    "SUBSET_DIR.mkdir(exist_ok=True)\n",
    "SUBSET_IMAGES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Created subset directory: {SUBSET_DIR}\")\n",
    "print(f\"Images will be stored in: {SUBSET_IMAGES_DIR}\")\n",
    "print(f\"Annotations will be saved to: {SUBSET_ANN_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ebf075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy filtered images to subset directory\n",
    "print(\"Copying images to subset directory...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "copied_count = 0\n",
    "failed_count = 0\n",
    "copied_files = []\n",
    "\n",
    "for img_info in tqdm(filtered_images, desc=\"Copying images\"):\n",
    "    src_path = IMAGE_DIR / img_info['file_name']\n",
    "    dst_path = SUBSET_IMAGES_DIR / img_info['file_name']\n",
    "    \n",
    "    if src_path.exists():\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        copied_count += 1\n",
    "        copied_files.append(img_info['file_name'])\n",
    "    else:\n",
    "        print(f\"Warning: Source image not found: {src_path}\")\n",
    "        failed_count += 1\n",
    "\n",
    "print(f\"\\nâœ“ Image copying complete!\")\n",
    "print(f\"  Successfully copied: {copied_count} images\")\n",
    "if failed_count > 0:\n",
    "    print(f\"  Failed to copy: {failed_count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filtered COCO annotation JSON\n",
    "# Re-map annotation and image IDs to be sequential starting from 1\n",
    "new_img_id_map = {old_id: new_id for new_id, old_id in enumerate(sorted(filtered_img_ids), 1)}\n",
    "new_ann_id_map = {}\n",
    "\n",
    "# Create new annotations with remapped IDs\n",
    "new_annotations = []\n",
    "for new_ann_id, ann in enumerate(filtered_annotations, 1):\n",
    "    new_ann = ann.copy()\n",
    "    new_ann['id'] = new_ann_id\n",
    "    new_ann['image_id'] = new_img_id_map[ann['image_id']]\n",
    "    new_annotations.append(new_ann)\n",
    "    new_ann_id_map[ann['id']] = new_ann_id\n",
    "\n",
    "# Create new images with remapped IDs\n",
    "new_images = []\n",
    "for img in filtered_images:\n",
    "    new_img = img.copy()\n",
    "    new_img['id'] = new_img_id_map[img['id']]\n",
    "    new_images.append(new_img)\n",
    "\n",
    "# Sort by ID for consistency\n",
    "new_images.sort(key=lambda x: x['id'])\n",
    "new_annotations.sort(key=lambda x: x['id'])\n",
    "\n",
    "# Create new categories (keep original IDs for top 5)\n",
    "new_categories = filtered_categories\n",
    "\n",
    "# Build COCO JSON structure\n",
    "subset_coco_data = {\n",
    "    'info': {\n",
    "        'description': 'TACO Subset - Top 5 Most Frequent Classes',\n",
    "        'version': '1.0',\n",
    "        'year': 2025,\n",
    "        'contributor': 'Minahil Ali (22i-0849), Ayaan Khan (22i-0832)',\n",
    "        'date_created': '2025-11-16'\n",
    "    },\n",
    "    'licenses': [],\n",
    "    'images': new_images,\n",
    "    'annotations': new_annotations,\n",
    "    'categories': new_categories\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "with open(SUBSET_ANN_FILE, 'w') as f:\n",
    "    json.dump(subset_coco_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Subset annotations saved to: {SUBSET_ANN_FILE}\")\n",
    "print(f\"\\nSubset COCO JSON contains:\")\n",
    "print(f\"  Images: {len(new_images)}\")\n",
    "print(f\"  Annotations: {len(new_annotations)}\")\n",
    "print(f\"  Categories: {len(new_categories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac76a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset manifest CSV for easy reference\n",
    "manifest_data = []\n",
    "\n",
    "for img in new_images:\n",
    "    img_anns = [ann for ann in new_annotations if ann['image_id'] == img['id']]\n",
    "    \n",
    "    manifest_data.append({\n",
    "        'image_id': img['id'],\n",
    "        'file_name': img['file_name'],\n",
    "        'width': img.get('width', 'N/A'),\n",
    "        'height': img.get('height', 'N/A'),\n",
    "        'num_objects': len(img_anns),\n",
    "        'category_ids': ','.join(str(ann['category_id']) for ann in img_anns),\n",
    "        'category_names': ','.join(cat_id_to_name.get(ann['category_id'], 'Unknown') \n",
    "                                   for ann in img_anns)\n",
    "    })\n",
    "\n",
    "manifest_df = pd.DataFrame(manifest_data)\n",
    "manifest_csv_path = SUBSET_DIR / 'subset_manifest.csv'\n",
    "manifest_df.to_csv(manifest_csv_path, index=False)\n",
    "\n",
    "print(f\"âœ“ Subset manifest CSV saved to: {manifest_csv_path}\")\n",
    "print(f\"\\nManifest preview:\")\n",
    "print(manifest_df.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"SUBSET CREATION COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Location: {SUBSET_DIR}\")\n",
    "print(f\"Files:\")\n",
    "print(f\"  - annotations.json: {SUBSET_ANN_FILE}\")\n",
    "print(f\"  - subset_manifest.csv: {manifest_csv_path}\")\n",
    "print(f\"  - images/: {SUBSET_IMAGES_DIR} ({copied_count} images)\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816118d",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Data Augmentation\n",
    "\n",
    "Coming next: Implement augmentation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb68981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for augmentation section\n",
    "print(\"Augmentation section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191cc86b",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. YOLOv8 Object Detection\n",
    "\n",
    "Coming next: Train and evaluate YOLOv8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for YOLOv8 section\n",
    "print(\"YOLOv8 section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb80624b",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. U-Net Semantic Segmentation\n",
    "\n",
    "Coming next: Build and train U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb71565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for U-Net section\n",
    "print(\"U-Net section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dbdeb4",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model Comparison & Discussion\n",
    "\n",
    "Coming next: Compare YOLO vs U-Net performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445623cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for comparison section\n",
    "print(\"Comparison section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317d9dc9",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Conclusions & Future Work\n",
    "\n",
    "Coming next: Final conclusions and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for conclusions section\n",
    "print(\"Conclusions section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4b1f2",
   "metadata": {},
   "source": [
    "---\n",
    "## End of Notebook\n",
    "\n",
    "**Project:** YOLOv8 + U-Net Waste Detection and Segmentation  \n",
    "**Course:** Deep Learning for Perception (CS4045)  \n",
    "**Authors:** Minahil Ali (22i-0849), Ayaan Khan (22i-0832)  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
