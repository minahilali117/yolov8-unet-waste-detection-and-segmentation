{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d412467",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup & Random Seed Configuration](#1.-Environment-Setup-&-Random-Seed-Configuration)\n",
    "2. [Import Libraries](#2.-Import-Libraries)\n",
    "3. [Dataset Download & Verification](#3.-Dataset-Download-&-Verification)\n",
    "4. [Exploratory Data Analysis (EDA)](#4.-Exploratory-Data-Analysis-(EDA))\n",
    "5. [Data Preprocessing & Subset Creation](#5.-Data-Preprocessing-&-Subset-Creation)\n",
    "6. [Data Augmentation](#6.-Data-Augmentation)\n",
    "7. [YOLOv8 Object Detection](#7.-YOLOv8-Object-Detection)\n",
    "8. [U-Net Semantic Segmentation](#8.-U-Net-Semantic-Segmentation)\n",
    "9. [Model Comparison & Discussion](#9.-Model-Comparison-&-Discussion)\n",
    "10. [Conclusions & Future Work](#10.-Conclusions-&-Future-Work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d94f6",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup & Random Seed Configuration\n",
    "\n",
    "Setting all random seeds for reproducibility across:\n",
    "- Python's built-in random module\n",
    "- NumPy\n",
    "- PyTorch (CPU and CUDA)\n",
    "- Python hash seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable for Python hash seed (must be done before importing libraries)\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# Import random libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# PyTorch seeds (will be set after importing torch)\n",
    "print(\"‚úì Python hash seed set to 0\")\n",
    "print(f\"‚úì Random seed set to {RANDOM_SEED}\")\n",
    "print(f\"‚úì NumPy seed set to {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch and set its random seeds\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Additional PyTorch reproducibility settings\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"‚úì PyTorch seed set to {RANDOM_SEED}\")\n",
    "print(f\"‚úì PyTorch CUDA seed set to {RANDOM_SEED}\")\n",
    "print(\"‚úì CUDNN deterministic mode enabled\")\n",
    "print(\"‚úì CUDNN benchmark disabled for reproducibility\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n‚úì Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5496c",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Import Libraries\n",
    "\n",
    "Importing all required libraries for:\n",
    "- Data manipulation and analysis\n",
    "- Image processing\n",
    "- Deep learning (PyTorch, YOLOv8)\n",
    "- Visualization\n",
    "- COCO dataset handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# YOLOv8\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "# COCO tools\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as coco_mask\n",
    "\n",
    "# Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(f\"\\nLibrary Versions:\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  Torchvision: {torchvision.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  Pandas: {pd.__version__}\")\n",
    "print(f\"  OpenCV: {cv2.__version__}\")\n",
    "print(f\"  Albumentations: {A.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2c86b0",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Dataset Download & Verification\n",
    "\n",
    "### 3.1 Download TACO Dataset\n",
    "\n",
    "The TACO dataset can be downloaded using:\n",
    "1. **Kaggle API** (recommended - automated)\n",
    "2. **Manual download** from Kaggle website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project directories\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "TACO_DIR = DATA_DIR / 'TACO'\n",
    "RUNS_DIR = PROJECT_ROOT / 'runs'\n",
    "WEIGHTS_DIR = PROJECT_ROOT / 'weights'\n",
    "\n",
    "# Create directories\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "RUNS_DIR.mkdir(exist_ok=True)\n",
    "WEIGHTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Project directory structure:\")\n",
    "print(f\"  Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"  Data Directory: {DATA_DIR}\")\n",
    "print(f\"  TACO Directory: {TACO_DIR}\")\n",
    "print(f\"  Runs Directory: {RUNS_DIR}\")\n",
    "print(f\"  Weights Directory: {WEIGHTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset already exists\n",
    "if TACO_DIR.exists() and len(list(TACO_DIR.glob('*'))) > 0:\n",
    "    print(\"‚úì TACO dataset already exists!\")\n",
    "    print(f\"  Location: {TACO_DIR}\")\n",
    "else:\n",
    "    print(\"Dataset not found. Please download using one of the following methods:\\n\")\n",
    "    \n",
    "    print(\"Method 1: Kaggle API (Recommended)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"1. Install Kaggle API: pip install kaggle\")\n",
    "    print(\"2. Setup Kaggle credentials (kaggle.json)\")\n",
    "    print(\"3. Run the following commands:\\n\")\n",
    "    print(\"   kaggle datasets download -d kneroma/tacotrashdataset\")\n",
    "    print(f\"   unzip tacotrashdataset.zip -d {DATA_DIR}\")\n",
    "    print(\"\\nMethod 2: Manual Download\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"1. Visit: https://www.kaggle.com/datasets/kneroma/tacotrashdataset\")\n",
    "    print(\"2. Download the dataset\")\n",
    "    print(f\"3. Extract to: {DATA_DIR}\")\n",
    "    print(\"\\nNote: Uncomment and run the cell below to download via Kaggle API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download using Kaggle API\n",
    "# !pip install kaggle\n",
    "# !kaggle datasets download -d kneroma/tacotrashdataset\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile('tacotrashdataset.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall(DATA_DIR)\n",
    "# print(\"‚úì Dataset downloaded and extracted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e17cc2",
   "metadata": {},
   "source": [
    "### 3.2 Verify Dataset Structure and COCO Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa52c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "if TACO_DIR.exists():\n",
    "    print(\"Dataset Structure:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # List all subdirectories and files\n",
    "    for item in sorted(TACO_DIR.glob('*')):\n",
    "        if item.is_dir():\n",
    "            file_count = len(list(item.glob('*')))\n",
    "            print(f\"üìÅ {item.name}/ ({file_count} items)\")\n",
    "        else:\n",
    "            file_size = item.stat().st_size / (1024 * 1024)  # Convert to MB\n",
    "            print(f\"üìÑ {item.name} ({file_size:.2f} MB)\")\n",
    "    \n",
    "    # Look for annotation files\n",
    "    annotation_files = list(TACO_DIR.rglob('*.json'))\n",
    "    print(f\"\\nFound {len(annotation_files)} annotation file(s):\")\n",
    "    for ann_file in annotation_files:\n",
    "        print(f\"  - {ann_file.relative_to(TACO_DIR)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dataset not found. Please download the dataset first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcad93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify COCO annotations\n",
    "# Note: Update the annotation file path based on actual dataset structure\n",
    "\n",
    "# Common TACO annotation file paths\n",
    "possible_ann_paths = [\n",
    "    TACO_DIR / 'annotations.json',\n",
    "    TACO_DIR / 'annotations' / 'instances_default.json',\n",
    "    TACO_DIR / 'TACO' / 'annotations.json',\n",
    "]\n",
    "\n",
    "ANNOTATION_FILE = None\n",
    "for path in possible_ann_paths:\n",
    "    if path.exists():\n",
    "        ANNOTATION_FILE = path\n",
    "        break\n",
    "\n",
    "if ANNOTATION_FILE:\n",
    "    print(f\"‚úì Found annotation file: {ANNOTATION_FILE.name}\")\n",
    "    \n",
    "    # Load COCO annotations\n",
    "    with open(ANNOTATION_FILE, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    print(\"\\nCOCO Format Verification:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"  Images: {len(coco_data.get('images', []))}\")\n",
    "    print(f\"  Annotations: {len(coco_data.get('annotations', []))}\")\n",
    "    print(f\"  Categories: {len(coco_data.get('categories', []))}\")\n",
    "    \n",
    "    # Display first few categories\n",
    "    print(\"\\nSample Categories:\")\n",
    "    for cat in coco_data.get('categories', [])[:10]:\n",
    "        print(f\"  ID {cat['id']}: {cat['name']}\")\n",
    "    \n",
    "    if len(coco_data.get('categories', [])) > 10:\n",
    "        print(f\"  ... and {len(coco_data.get('categories', [])) - 10} more\")\n",
    "    \n",
    "    print(\"\\n‚úì COCO format verified successfully!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Annotation file not found. Please verify dataset structure.\")\n",
    "    print(\"   Expected locations:\")\n",
    "    for path in possible_ann_paths:\n",
    "        print(f\"   - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a2c73a",
   "metadata": {},
   "source": [
    "### 3.3 Find Image Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate image directory\n",
    "possible_img_dirs = [\n",
    "    TACO_DIR / 'images',\n",
    "    TACO_DIR / 'data',\n",
    "    TACO_DIR / 'TACO' / 'images',\n",
    "    TACO_DIR,\n",
    "]\n",
    "\n",
    "IMAGE_DIR = None\n",
    "for img_dir in possible_img_dirs:\n",
    "    if img_dir.exists():\n",
    "        # Check if directory contains image files\n",
    "        img_files = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))\n",
    "        if len(img_files) > 0:\n",
    "            IMAGE_DIR = img_dir\n",
    "            print(f\"‚úì Found image directory: {IMAGE_DIR.name}\")\n",
    "            print(f\"  Total images: {len(img_files)}\")\n",
    "            break\n",
    "\n",
    "if not IMAGE_DIR:\n",
    "    print(\"‚ö†Ô∏è Image directory not found. Please verify dataset structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ad8fc",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Coming next: Comprehensive analysis of the TACO dataset including:\n",
    "- Dataset statistics\n",
    "- Class distribution and frequency\n",
    "- Object count per image\n",
    "- Class imbalance visualization\n",
    "- Sample images with annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cb016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for EDA section\n",
    "print(\"EDA section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932ea04",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Data Preprocessing & Subset Creation\n",
    "\n",
    "Coming next: Filter dataset to top 5 most frequent classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f071ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for preprocessing section\n",
    "print(\"Preprocessing section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816118d",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Data Augmentation\n",
    "\n",
    "Coming next: Implement augmentation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb68981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for augmentation section\n",
    "print(\"Augmentation section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191cc86b",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. YOLOv8 Object Detection\n",
    "\n",
    "Coming next: Train and evaluate YOLOv8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for YOLOv8 section\n",
    "print(\"YOLOv8 section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb80624b",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. U-Net Semantic Segmentation\n",
    "\n",
    "Coming next: Build and train U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb71565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for U-Net section\n",
    "print(\"U-Net section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dbdeb4",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model Comparison & Discussion\n",
    "\n",
    "Coming next: Compare YOLO vs U-Net performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445623cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for comparison section\n",
    "print(\"Comparison section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317d9dc9",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Conclusions & Future Work\n",
    "\n",
    "Coming next: Final conclusions and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for conclusions section\n",
    "print(\"Conclusions section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4b1f2",
   "metadata": {},
   "source": [
    "---\n",
    "## End of Notebook\n",
    "\n",
    "**Project:** YOLOv8 + U-Net Waste Detection and Segmentation  \n",
    "**Course:** Deep Learning for Perception (CS4045)  \n",
    "**Authors:** Minahil Ali (22i-0849), Ayaan Khan (22i-0832)  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
