{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d412467",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup & Random Seed Configuration](#1.-Environment-Setup-&-Random-Seed-Configuration)\n",
    "2. [Import Libraries](#2.-Import-Libraries)\n",
    "3. [Dataset Download & Verification](#3.-Dataset-Download-&-Verification)\n",
    "4. [Exploratory Data Analysis (EDA)](#4.-Exploratory-Data-Analysis-(EDA))\n",
    "5. [Data Preprocessing & Subset Creation](#5.-Data-Preprocessing-&-Subset-Creation)\n",
    "6. [Data Augmentation](#6.-Data-Augmentation)\n",
    "7. [YOLOv8 Object Detection](#7.-YOLOv8-Object-Detection)\n",
    "8. [U-Net Semantic Segmentation](#8.-U-Net-Semantic-Segmentation)\n",
    "9. [Model Comparison & Discussion](#9.-Model-Comparison-&-Discussion)\n",
    "10. [Conclusions & Future Work](#10.-Conclusions-&-Future-Work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d94f6",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup & Random Seed Configuration\n",
    "\n",
    "Setting all random seeds for reproducibility across:\n",
    "- Python's built-in random module\n",
    "- NumPy\n",
    "- PyTorch (CPU and CUDA)\n",
    "- Python hash seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable for Python hash seed (must be done before importing libraries)\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# Import random libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# PyTorch seeds (will be set after importing torch)\n",
    "print(\"âœ“ Python hash seed set to 0\")\n",
    "print(f\"âœ“ Random seed set to {RANDOM_SEED}\")\n",
    "print(f\"âœ“ NumPy seed set to {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch and set its random seeds\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Additional PyTorch reproducibility settings\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"âœ“ PyTorch seed set to {RANDOM_SEED}\")\n",
    "print(f\"âœ“ PyTorch CUDA seed set to {RANDOM_SEED}\")\n",
    "print(\"âœ“ CUDNN deterministic mode enabled\")\n",
    "print(\"âœ“ CUDNN benchmark disabled for reproducibility\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nâœ“ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5496c",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Import Libraries\n",
    "\n",
    "Importing all required libraries for:\n",
    "- Data manipulation and analysis\n",
    "- Image processing\n",
    "- Deep learning (PyTorch, YOLOv8)\n",
    "- Visualization\n",
    "- COCO dataset handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# YOLOv8\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "# COCO tools\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as coco_mask\n",
    "\n",
    "# Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "print(f\"\\nLibrary Versions:\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  Torchvision: {torchvision.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  Pandas: {pd.__version__}\")\n",
    "print(f\"  OpenCV: {cv2.__version__}\")\n",
    "print(f\"  Albumentations: {A.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2c86b0",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Dataset Download & Verification\n",
    "\n",
    "### 3.1 Download TACO Dataset\n",
    "\n",
    "The TACO dataset can be downloaded using:\n",
    "1. **Kaggle API** (recommended - automated)\n",
    "2. **Manual download** from Kaggle website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project directories\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "TACO_DIR = DATA_DIR / 'TACO'\n",
    "RUNS_DIR = PROJECT_ROOT / 'runs'\n",
    "WEIGHTS_DIR = PROJECT_ROOT / 'weights'\n",
    "\n",
    "# Create directories\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "RUNS_DIR.mkdir(exist_ok=True)\n",
    "WEIGHTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Project directory structure:\")\n",
    "print(f\"  Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"  Data Directory: {DATA_DIR}\")\n",
    "print(f\"  TACO Directory: {TACO_DIR}\")\n",
    "print(f\"  Runs Directory: {RUNS_DIR}\")\n",
    "print(f\"  Weights Directory: {WEIGHTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset already exists\n",
    "if TACO_DIR.exists() and len(list(TACO_DIR.glob('*'))) > 0:\n",
    "    print(\"âœ“ TACO dataset already exists!\")\n",
    "    print(f\"  Location: {TACO_DIR}\")\n",
    "else:\n",
    "    print(\"Dataset not found. Please download using one of the following methods:\\n\")\n",
    "    \n",
    "    print(\"Method 1: Kaggle API (Recommended)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"1. Install Kaggle API: pip install kaggle\")\n",
    "    print(\"2. Setup Kaggle credentials (kaggle.json)\")\n",
    "    print(\"3. Run the following commands:\\n\")\n",
    "    print(\"   kaggle datasets download -d kneroma/tacotrashdataset\")\n",
    "    print(f\"   unzip tacotrashdataset.zip -d {DATA_DIR}\")\n",
    "    print(\"\\nMethod 2: Manual Download\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"1. Visit: https://www.kaggle.com/datasets/kneroma/tacotrashdataset\")\n",
    "    print(\"2. Download the dataset\")\n",
    "    print(f\"3. Extract to: {DATA_DIR}\")\n",
    "    print(\"\\nNote: Uncomment and run the cell below to download via Kaggle API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download using Kaggle API\n",
    "# !pip install kaggle\n",
    "# !kaggle datasets download -d kneroma/tacotrashdataset\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile('tacotrashdataset.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall(DATA_DIR)\n",
    "# print(\"âœ“ Dataset downloaded and extracted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e17cc2",
   "metadata": {},
   "source": [
    "### 3.2 Verify Dataset Structure and COCO Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa52c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "if TACO_DIR.exists():\n",
    "    print(\"Dataset Structure:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # List all subdirectories and files\n",
    "    for item in sorted(TACO_DIR.glob('*')):\n",
    "        if item.is_dir():\n",
    "            file_count = len(list(item.glob('*')))\n",
    "            print(f\"ðŸ“ {item.name}/ ({file_count} items)\")\n",
    "        else:\n",
    "            file_size = item.stat().st_size / (1024 * 1024)  # Convert to MB\n",
    "            print(f\"ðŸ“„ {item.name} ({file_size:.2f} MB)\")\n",
    "    \n",
    "    # Look for annotation files\n",
    "    annotation_files = list(TACO_DIR.rglob('*.json'))\n",
    "    print(f\"\\nFound {len(annotation_files)} annotation file(s):\")\n",
    "    for ann_file in annotation_files:\n",
    "        print(f\"  - {ann_file.relative_to(TACO_DIR)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Dataset not found. Please download the dataset first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcad93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify COCO annotations\n",
    "# Note: Update the annotation file path based on actual dataset structure\n",
    "\n",
    "# Common TACO annotation file paths\n",
    "possible_ann_paths = [\n",
    "    TACO_DIR / 'annotations.json',\n",
    "    TACO_DIR / 'annotations' / 'instances_default.json',\n",
    "    TACO_DIR / 'TACO' / 'annotations.json',\n",
    "]\n",
    "\n",
    "ANNOTATION_FILE = None\n",
    "for path in possible_ann_paths:\n",
    "    if path.exists():\n",
    "        ANNOTATION_FILE = path\n",
    "        break\n",
    "\n",
    "if ANNOTATION_FILE:\n",
    "    print(f\"âœ“ Found annotation file: {ANNOTATION_FILE.name}\")\n",
    "    \n",
    "    # Load COCO annotations\n",
    "    with open(ANNOTATION_FILE, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    print(\"\\nCOCO Format Verification:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"  Images: {len(coco_data.get('images', []))}\")\n",
    "    print(f\"  Annotations: {len(coco_data.get('annotations', []))}\")\n",
    "    print(f\"  Categories: {len(coco_data.get('categories', []))}\")\n",
    "    \n",
    "    # Display first few categories\n",
    "    print(\"\\nSample Categories:\")\n",
    "    for cat in coco_data.get('categories', [])[:10]:\n",
    "        print(f\"  ID {cat['id']}: {cat['name']}\")\n",
    "    \n",
    "    if len(coco_data.get('categories', [])) > 10:\n",
    "        print(f\"  ... and {len(coco_data.get('categories', [])) - 10} more\")\n",
    "    \n",
    "    print(\"\\nâœ“ COCO format verified successfully!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Annotation file not found. Please verify dataset structure.\")\n",
    "    print(\"   Expected locations:\")\n",
    "    for path in possible_ann_paths:\n",
    "        print(f\"   - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a2c73a",
   "metadata": {},
   "source": [
    "### 3.3 Find Image Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate image directory\n",
    "possible_img_dirs = [\n",
    "    TACO_DIR / 'images',\n",
    "    TACO_DIR / 'data',\n",
    "    TACO_DIR / 'TACO' / 'images',\n",
    "    TACO_DIR,\n",
    "]\n",
    "\n",
    "IMAGE_DIR = None\n",
    "for img_dir in possible_img_dirs:\n",
    "    if img_dir.exists():\n",
    "        # Check if directory contains image files\n",
    "        img_files = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))\n",
    "        if len(img_files) > 0:\n",
    "            IMAGE_DIR = img_dir\n",
    "            print(f\"âœ“ Found image directory: {IMAGE_DIR.name}\")\n",
    "            print(f\"  Total images: {len(img_files)}\")\n",
    "            break\n",
    "\n",
    "if not IMAGE_DIR:\n",
    "    print(\"âš ï¸ Image directory not found. Please verify dataset structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ad8fc",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 4.1 Load COCO Annotations\n",
    "\n",
    "Loading the TACO dataset annotations and extracting key statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cb016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO annotations\n",
    "if ANNOTATION_FILE and ANNOTATION_FILE.exists():\n",
    "    print(f\"Loading annotations from: {ANNOTATION_FILE.name}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize COCO API\n",
    "    coco = COCO(str(ANNOTATION_FILE))\n",
    "    \n",
    "    # Get all category IDs and image IDs\n",
    "    cat_ids = coco.getCatIds()\n",
    "    img_ids = coco.getImgIds()\n",
    "    ann_ids = coco.getAnnIds()\n",
    "    \n",
    "    # Load categories and images\n",
    "    categories = coco.loadCats(cat_ids)\n",
    "    images = coco.loadImgs(img_ids)\n",
    "    annotations = coco.loadAnns(ann_ids)\n",
    "    \n",
    "    print(f\"\\nâœ“ Successfully loaded TACO dataset!\")\n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total Images: {len(images)}\")\n",
    "    print(f\"  Total Annotations: {len(annotations)}\")\n",
    "    print(f\"  Total Categories: {len(categories)}\")\n",
    "    print(f\"  Average annotations per image: {len(annotations)/len(images):.2f}\")\n",
    "    \n",
    "    # Create category name mapping\n",
    "    cat_id_to_name = {cat['id']: cat['name'] for cat in categories}\n",
    "    cat_name_to_id = {cat['name']: cat['id'] for cat in categories}\n",
    "    \n",
    "    print(f\"\\nâœ“ Category mapping created\")\n",
    "    print(f\"  Sample categories: {list(cat_id_to_name.items())[:5]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Annotation file not found. Please download the TACO dataset first.\")\n",
    "    print(\"Stopping execution - dataset is required for EDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997480c",
   "metadata": {},
   "source": [
    "### 4.2 Compute Class Frequencies and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class frequencies\n",
    "class_counts = Counter()\n",
    "image_object_counts = defaultdict(int)\n",
    "\n",
    "for ann in annotations:\n",
    "    class_counts[ann['category_id']] += 1\n",
    "    image_object_counts[ann['image_id']] += 1\n",
    "\n",
    "# Convert to sorted list\n",
    "class_freq_list = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "class_freq_df = pd.DataFrame([\n",
    "    {\n",
    "        'Category ID': cat_id,\n",
    "        'Category Name': cat_id_to_name.get(cat_id, 'Unknown'),\n",
    "        'Count': count,\n",
    "        'Percentage': (count / len(annotations)) * 100\n",
    "    }\n",
    "    for cat_id, count in class_freq_list\n",
    "])\n",
    "\n",
    "print(\"Class Frequency Distribution (Top 20):\")\n",
    "print(\"=\" * 70)\n",
    "print(class_freq_df.head(20).to_string(index=False))\n",
    "\n",
    "print(f\"\\n\\nTop 5 Most Frequent Classes (IDs 0-4 as required):\")\n",
    "print(\"=\" * 70)\n",
    "top_5_classes = class_freq_list[:5]\n",
    "for rank, (cat_id, count) in enumerate(top_5_classes, 1):\n",
    "    cat_name = cat_id_to_name.get(cat_id, 'Unknown')\n",
    "    percentage = (count / len(annotations)) * 100\n",
    "    print(f\"{rank}. ID {cat_id}: {cat_name:30s} - {count:5d} annotations ({percentage:5.2f}%)\")\n",
    "\n",
    "# Store top 5 class IDs for later use\n",
    "TOP_5_CLASS_IDS = [cat_id for cat_id, _ in top_5_classes]\n",
    "print(f\"\\nâœ“ Top 5 class IDs stored: {TOP_5_CLASS_IDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c412e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze objects per image statistics\n",
    "objects_per_img = list(image_object_counts.values())\n",
    "\n",
    "print(\"Objects per Image Statistics:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Mean objects per image: {np.mean(objects_per_img):.2f}\")\n",
    "print(f\"  Median objects per image: {np.median(objects_per_img):.2f}\")\n",
    "print(f\"  Std deviation: {np.std(objects_per_img):.2f}\")\n",
    "print(f\"  Min objects in an image: {min(objects_per_img)}\")\n",
    "print(f\"  Max objects in an image: {max(objects_per_img)}\")\n",
    "print(f\"  Total images: {len(image_object_counts)}\")\n",
    "\n",
    "# Distribution breakdown\n",
    "dist_breakdown = Counter(objects_per_img)\n",
    "print(f\"\\nDistribution breakdown:\")\n",
    "for num_objs in sorted(dist_breakdown.keys())[:10]:\n",
    "    count = dist_breakdown[num_objs]\n",
    "    print(f\"  {num_objs} object(s): {count} images ({count/len(images)*100:.1f}%)\")\n",
    "if len(dist_breakdown) > 10:\n",
    "    print(f\"  ... and {len(dist_breakdown) - 10} more categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0438843f",
   "metadata": {},
   "source": [
    "### 4.3 Visualization: Class Distribution Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart for top 20 classes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Plot 1: Top 20 classes\n",
    "top_20_df = class_freq_df.head(20)\n",
    "bars1 = ax1.barh(range(len(top_20_df)), top_20_df['Count'], color='steelblue')\n",
    "\n",
    "# Highlight top 5 classes\n",
    "for i in range(min(5, len(top_20_df))):\n",
    "    bars1[i].set_color('coral')\n",
    "    bars1[i].set_edgecolor('darkred')\n",
    "    bars1[i].set_linewidth(2)\n",
    "\n",
    "ax1.set_yticks(range(len(top_20_df)))\n",
    "ax1.set_yticklabels([f\"ID {row['Category ID']}: {row['Category Name'][:25]}\" \n",
    "                      for _, row in top_20_df.iterrows()])\n",
    "ax1.set_xlabel('Number of Annotations', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Top 20 Classes by Frequency\\n(Top 5 highlighted in coral)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, (_, row) in enumerate(top_20_df.iterrows()):\n",
    "    ax1.text(row['Count'], i, f\" {row['Count']}\", \n",
    "             va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 2: Top 5 classes only (required subset)\n",
    "top_5_df = class_freq_df.head(5)\n",
    "bars2 = ax2.bar(range(len(top_5_df)), top_5_df['Count'], \n",
    "                color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8'],\n",
    "                edgecolor='black', linewidth=2)\n",
    "\n",
    "ax2.set_xticks(range(len(top_5_df)))\n",
    "ax2.set_xticklabels([f\"ID {row['Category ID']}\\n{row['Category Name'][:20]}\" \n",
    "                      for _, row in top_5_df.iterrows()], rotation=45, ha='right')\n",
    "ax2.set_ylabel('Number of Annotations', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Top 5 Most Frequent Classes (Subset for Training)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count and percentage labels\n",
    "for i, (_, row) in enumerate(top_5_df.iterrows()):\n",
    "    ax2.text(i, row['Count'], f\"{row['Count']}\\n({row['Percentage']:.1f}%)\", \n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RUNS_DIR / 'class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Class distribution chart saved to: {RUNS_DIR / 'class_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be04759",
   "metadata": {},
   "source": [
    "### 4.4 Visualization: Objects per Image Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram of objects per image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Histogram with all data\n",
    "ax1.hist(objects_per_img, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(np.mean(objects_per_img), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {np.mean(objects_per_img):.2f}')\n",
    "ax1.axvline(np.median(objects_per_img), color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Median: {np.median(objects_per_img):.2f}')\n",
    "ax1.set_xlabel('Number of Objects per Image', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Frequency (Number of Images)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distribution of Objects per Image (All Data)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Box plot for better visualization of outliers\n",
    "ax2.boxplot(objects_per_img, vert=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightcoral', alpha=0.7),\n",
    "            medianprops=dict(color='darkred', linewidth=2),\n",
    "            whiskerprops=dict(linewidth=1.5),\n",
    "            capprops=dict(linewidth=1.5))\n",
    "ax2.set_ylabel('Number of Objects per Image', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Box Plot: Objects per Image\\n(Shows outliers and quartiles)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add statistics text\n",
    "stats_text = f\"Min: {min(objects_per_img)}\\n\"\n",
    "stats_text += f\"Q1: {np.percentile(objects_per_img, 25):.1f}\\n\"\n",
    "stats_text += f\"Median: {np.median(objects_per_img):.1f}\\n\"\n",
    "stats_text += f\"Q3: {np.percentile(objects_per_img, 75):.1f}\\n\"\n",
    "stats_text += f\"Max: {max(objects_per_img)}\"\n",
    "ax2.text(1.15, np.median(objects_per_img), stats_text, \n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "         fontsize=10, verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RUNS_DIR / 'objects_per_image_histogram.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Objects per image histogram saved to: {RUNS_DIR / 'objects_per_image_histogram.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ffc6b",
   "metadata": {},
   "source": [
    "### 4.5 Visualization: Sample Images with Bounding Boxes and Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f510864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to visualize image with annotations\n",
    "def visualize_annotations(img_id, coco_obj, img_dir, ax_bbox=None, ax_mask=None):\n",
    "    \"\"\"\n",
    "    Visualize bounding boxes and segmentation masks for a given image.\n",
    "    \n",
    "    Args:\n",
    "        img_id: COCO image ID\n",
    "        coco_obj: COCO API object\n",
    "        img_dir: Path to image directory\n",
    "        ax_bbox: Matplotlib axis for bounding box visualization\n",
    "        ax_mask: Matplotlib axis for mask visualization\n",
    "    \"\"\"\n",
    "    # Load image info\n",
    "    img_info = coco_obj.loadImgs(img_id)[0]\n",
    "    img_path = img_dir / img_info['file_name']\n",
    "    \n",
    "    # Check if image exists\n",
    "    if not img_path.exists():\n",
    "        print(f\"Warning: Image not found: {img_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Load image\n",
    "    img = cv2.imread(str(img_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get annotations for this image\n",
    "    ann_ids = coco_obj.getAnnIds(imgIds=img_id)\n",
    "    anns = coco_obj.loadAnns(ann_ids)\n",
    "    \n",
    "    # Visualization with bounding boxes\n",
    "    if ax_bbox is not None:\n",
    "        ax_bbox.imshow(img)\n",
    "        ax_bbox.axis('off')\n",
    "        \n",
    "        for ann in anns:\n",
    "            # Draw bounding box\n",
    "            bbox = ann['bbox']  # [x, y, width, height]\n",
    "            rect = patches.Rectangle(\n",
    "                (bbox[0], bbox[1]), bbox[2], bbox[3],\n",
    "                linewidth=2, edgecolor='lime', facecolor='none'\n",
    "            )\n",
    "            ax_bbox.add_patch(rect)\n",
    "            \n",
    "            # Add label\n",
    "            cat_name = cat_id_to_name.get(ann['category_id'], 'Unknown')\n",
    "            ax_bbox.text(\n",
    "                bbox[0], bbox[1] - 5,\n",
    "                f\"{cat_name[:15]}\", \n",
    "                color='white', fontsize=8, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='lime', alpha=0.7)\n",
    "            )\n",
    "        \n",
    "        ax_bbox.set_title(f\"Bounding Boxes\\n{img_info['file_name']}\\n{len(anns)} objects\", \n",
    "                         fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Visualization with segmentation masks\n",
    "    if ax_mask is not None:\n",
    "        ax_mask.imshow(img)\n",
    "        ax_mask.axis('off')\n",
    "        \n",
    "        # Create colored mask overlay\n",
    "        mask_img = np.zeros_like(img, dtype=np.float32)\n",
    "        \n",
    "        for idx, ann in enumerate(anns):\n",
    "            if 'segmentation' in ann and ann['segmentation']:\n",
    "                # Generate random color for this annotation\n",
    "                color = np.random.rand(3)\n",
    "                \n",
    "                # Handle different segmentation formats\n",
    "                if isinstance(ann['segmentation'], list):\n",
    "                    # Polygon format\n",
    "                    for seg in ann['segmentation']:\n",
    "                        poly = np.array(seg).reshape(-1, 2).astype(np.int32)\n",
    "                        cv2.fillPoly(mask_img, [poly], color)\n",
    "                elif isinstance(ann['segmentation'], dict):\n",
    "                    # RLE format\n",
    "                    rle = ann['segmentation']\n",
    "                    mask = coco_mask.decode(rle)\n",
    "                    mask_img[mask > 0] = color\n",
    "        \n",
    "        # Overlay mask on image\n",
    "        alpha = 0.5\n",
    "        overlaid = (img * (1 - alpha) + mask_img * 255 * alpha).astype(np.uint8)\n",
    "        ax_mask.imshow(overlaid)\n",
    "        \n",
    "        ax_mask.set_title(f\"Segmentation Masks\\n{img_info['file_name']}\\n{len(anns)} objects\", \n",
    "                         fontsize=10, fontweight='bold')\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"âœ“ Visualization helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03041d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and visualize 12 example images (2 rows x 6 images)\n",
    "# Try to get images with varying number of objects\n",
    "sample_img_ids = []\n",
    "for target_count in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "    # Find images with approximately this many objects\n",
    "    candidates = [img_id for img_id, count in image_object_counts.items() \n",
    "                 if abs(count - target_count) <= 2]\n",
    "    if candidates:\n",
    "        sample_img_ids.append(random.choice(candidates))\n",
    "    \n",
    "    if len(sample_img_ids) >= 12:\n",
    "        break\n",
    "\n",
    "# If we don't have enough, just take random ones\n",
    "if len(sample_img_ids) < 12:\n",
    "    remaining = 12 - len(sample_img_ids)\n",
    "    additional = random.sample(list(image_object_counts.keys()), remaining)\n",
    "    sample_img_ids.extend(additional)\n",
    "\n",
    "sample_img_ids = sample_img_ids[:12]\n",
    "\n",
    "print(f\"Selected {len(sample_img_ids)} sample images for visualization\")\n",
    "print(f\"Sample image IDs: {sample_img_ids[:6]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with bounding boxes\n",
    "fig, axes = plt.subplots(2, 6, figsize=(24, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "print(\"Visualizing bounding boxes...\")\n",
    "for idx, img_id in enumerate(sample_img_ids):\n",
    "    visualize_annotations(img_id, coco, IMAGE_DIR, ax_bbox=axes[idx])\n",
    "\n",
    "plt.suptitle('Sample Images with Bounding Boxes', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RUNS_DIR / 'sample_images_bboxes.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Bounding box visualization saved to: {RUNS_DIR / 'sample_images_bboxes.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aec710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with segmentation masks\n",
    "fig, axes = plt.subplots(2, 6, figsize=(24, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "print(\"Visualizing segmentation masks...\")\n",
    "for idx, img_id in enumerate(sample_img_ids):\n",
    "    visualize_annotations(img_id, coco, IMAGE_DIR, ax_mask=axes[idx])\n",
    "\n",
    "plt.suptitle('Sample Images with Segmentation Masks', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RUNS_DIR / 'sample_images_masks.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Segmentation mask visualization saved to: {RUNS_DIR / 'sample_images_masks.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932ea04",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Data Preprocessing & Subset Creation\n",
    "\n",
    "### 5.1 Why Use Only 5 Classes?\n",
    "\n",
    "**Justification for selecting top 5 classes:**\n",
    "\n",
    "1. **Less Label Noise**: Focusing on the most frequent classes reduces annotation inconsistencies and mislabeling that are more common in rare classes.\n",
    "\n",
    "2. **More Samples Per Class**: Top 5 classes have significantly more training examples, enabling better model learning and generalization.\n",
    "\n",
    "3. **Simpler Decision Boundary**: Fewer classes reduce inter-class confusion and make it easier for the model to learn discriminative features.\n",
    "\n",
    "4. **Balanced Training**: The top 5 classes together represent a substantial portion of the dataset, providing sufficient diversity while maintaining focus.\n",
    "\n",
    "5. **Computational Efficiency**: Training on fewer classes speeds up experimentation and allows for more thorough hyperparameter tuning.\n",
    "\n",
    "6. **Better Evaluation Metrics**: With more samples per class, evaluation metrics (Precision, Recall, mAP, IoU) are more reliable and statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f071ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.2 Filter Dataset to Top 5 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10003c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter annotations to keep only top 5 classes\n",
    "filtered_annotations = [ann for ann in annotations if ann['category_id'] in TOP_5_CLASS_IDS]\n",
    "\n",
    "# Get image IDs that have at least one annotation from top 5 classes\n",
    "filtered_img_ids = set(ann['image_id'] for ann in filtered_annotations)\n",
    "\n",
    "# Filter images\n",
    "filtered_images = [img for img in images if img['id'] in filtered_img_ids]\n",
    "\n",
    "# Filter categories to top 5\n",
    "filtered_categories = [cat for cat in categories if cat['id'] in TOP_5_CLASS_IDS]\n",
    "\n",
    "print(\"Subset Filtering Results:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Original dataset:\")\n",
    "print(f\"  Images: {len(images)}\")\n",
    "print(f\"  Annotations: {len(annotations)}\")\n",
    "print(f\"  Categories: {len(categories)}\")\n",
    "print(f\"\\nFiltered subset (Top 5 classes):\")\n",
    "print(f\"  Images: {len(filtered_images)} ({len(filtered_images)/len(images)*100:.1f}%)\")\n",
    "print(f\"  Annotations: {len(filtered_annotations)} ({len(filtered_annotations)/len(annotations)*100:.1f}%)\")\n",
    "print(f\"  Categories: {len(filtered_categories)}\")\n",
    "print(f\"\\nTop 5 classes retained:\")\n",
    "for cat_id in TOP_5_CLASS_IDS:\n",
    "    cat_name = cat_id_to_name.get(cat_id, 'Unknown')\n",
    "    count = sum(1 for ann in filtered_annotations if ann['category_id'] == cat_id)\n",
    "    print(f\"  ID {cat_id}: {cat_name:30s} - {count:5d} annotations\")\n",
    "\n",
    "# Compute statistics for filtered dataset\n",
    "filtered_img_obj_counts = Counter()\n",
    "for ann in filtered_annotations:\n",
    "    filtered_img_obj_counts[ann['image_id']] += 1\n",
    "\n",
    "print(f\"\\nFiltered dataset statistics:\")\n",
    "print(f\"  Avg objects per image: {len(filtered_annotations)/len(filtered_images):.2f}\")\n",
    "print(f\"  Min objects per image: {min(filtered_img_obj_counts.values())}\")\n",
    "print(f\"  Max objects per image: {max(filtered_img_obj_counts.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89610864",
   "metadata": {},
   "source": [
    "### 5.3 Create TACO Subset Directory and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5636eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset directory structure\n",
    "SUBSET_DIR = DATA_DIR / 'taco_subset'\n",
    "SUBSET_IMAGES_DIR = SUBSET_DIR / 'images'\n",
    "SUBSET_ANN_FILE = SUBSET_DIR / 'annotations.json'\n",
    "\n",
    "# Create directories\n",
    "SUBSET_DIR.mkdir(exist_ok=True)\n",
    "SUBSET_IMAGES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Created subset directory: {SUBSET_DIR}\")\n",
    "print(f\"Images will be stored in: {SUBSET_IMAGES_DIR}\")\n",
    "print(f\"Annotations will be saved to: {SUBSET_ANN_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ebf075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy filtered images to subset directory\n",
    "print(\"Copying images to subset directory...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "copied_count = 0\n",
    "failed_count = 0\n",
    "copied_files = []\n",
    "\n",
    "for img_info in tqdm(filtered_images, desc=\"Copying images\"):\n",
    "    src_path = IMAGE_DIR / img_info['file_name']\n",
    "    dst_path = SUBSET_IMAGES_DIR / img_info['file_name']\n",
    "    \n",
    "    if src_path.exists():\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        copied_count += 1\n",
    "        copied_files.append(img_info['file_name'])\n",
    "    else:\n",
    "        print(f\"Warning: Source image not found: {src_path}\")\n",
    "        failed_count += 1\n",
    "\n",
    "print(f\"\\nâœ“ Image copying complete!\")\n",
    "print(f\"  Successfully copied: {copied_count} images\")\n",
    "if failed_count > 0:\n",
    "    print(f\"  Failed to copy: {failed_count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filtered COCO annotation JSON\n",
    "# Re-map annotation and image IDs to be sequential starting from 1\n",
    "new_img_id_map = {old_id: new_id for new_id, old_id in enumerate(sorted(filtered_img_ids), 1)}\n",
    "new_ann_id_map = {}\n",
    "\n",
    "# Create new annotations with remapped IDs\n",
    "new_annotations = []\n",
    "for new_ann_id, ann in enumerate(filtered_annotations, 1):\n",
    "    new_ann = ann.copy()\n",
    "    new_ann['id'] = new_ann_id\n",
    "    new_ann['image_id'] = new_img_id_map[ann['image_id']]\n",
    "    new_annotations.append(new_ann)\n",
    "    new_ann_id_map[ann['id']] = new_ann_id\n",
    "\n",
    "# Create new images with remapped IDs\n",
    "new_images = []\n",
    "for img in filtered_images:\n",
    "    new_img = img.copy()\n",
    "    new_img['id'] = new_img_id_map[img['id']]\n",
    "    new_images.append(new_img)\n",
    "\n",
    "# Sort by ID for consistency\n",
    "new_images.sort(key=lambda x: x['id'])\n",
    "new_annotations.sort(key=lambda x: x['id'])\n",
    "\n",
    "# Create new categories (keep original IDs for top 5)\n",
    "new_categories = filtered_categories\n",
    "\n",
    "# Build COCO JSON structure\n",
    "subset_coco_data = {\n",
    "    'info': {\n",
    "        'description': 'TACO Subset - Top 5 Most Frequent Classes',\n",
    "        'version': '1.0',\n",
    "        'year': 2025,\n",
    "        'contributor': 'Minahil Ali (22i-0849), Ayaan Khan (22i-0832)',\n",
    "        'date_created': '2025-11-16'\n",
    "    },\n",
    "    'licenses': [],\n",
    "    'images': new_images,\n",
    "    'annotations': new_annotations,\n",
    "    'categories': new_categories\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "with open(SUBSET_ANN_FILE, 'w') as f:\n",
    "    json.dump(subset_coco_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Subset annotations saved to: {SUBSET_ANN_FILE}\")\n",
    "print(f\"\\nSubset COCO JSON contains:\")\n",
    "print(f\"  Images: {len(new_images)}\")\n",
    "print(f\"  Annotations: {len(new_annotations)}\")\n",
    "print(f\"  Categories: {len(new_categories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac76a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset manifest CSV for easy reference\n",
    "manifest_data = []\n",
    "\n",
    "for img in new_images:\n",
    "    img_anns = [ann for ann in new_annotations if ann['image_id'] == img['id']]\n",
    "    \n",
    "    manifest_data.append({\n",
    "        'image_id': img['id'],\n",
    "        'file_name': img['file_name'],\n",
    "        'width': img.get('width', 'N/A'),\n",
    "        'height': img.get('height', 'N/A'),\n",
    "        'num_objects': len(img_anns),\n",
    "        'category_ids': ','.join(str(ann['category_id']) for ann in img_anns),\n",
    "        'category_names': ','.join(cat_id_to_name.get(ann['category_id'], 'Unknown') \n",
    "                                   for ann in img_anns)\n",
    "    })\n",
    "\n",
    "manifest_df = pd.DataFrame(manifest_data)\n",
    "manifest_csv_path = SUBSET_DIR / 'subset_manifest.csv'\n",
    "manifest_df.to_csv(manifest_csv_path, index=False)\n",
    "\n",
    "print(f\"âœ“ Subset manifest CSV saved to: {manifest_csv_path}\")\n",
    "print(f\"\\nManifest preview:\")\n",
    "print(manifest_df.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"SUBSET CREATION COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Location: {SUBSET_DIR}\")\n",
    "print(f\"Files:\")\n",
    "print(f\"  - annotations.json: {SUBSET_ANN_FILE}\")\n",
    "print(f\"  - subset_manifest.csv: {manifest_csv_path}\")\n",
    "print(f\"  - images/: {SUBSET_IMAGES_DIR} ({copied_count} images)\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816118d",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Data Augmentation\n",
    "\n",
    "### 6.1 Augmentation Strategy Overview\n",
    "\n",
    "Data augmentation is crucial for improving model generalization and preventing overfitting. We'll implement three augmentation pipelines:\n",
    "\n",
    "1. **no_aug**: Baseline with no augmentation (only resize and normalize)\n",
    "2. **aug_v1**: Moderate augmentation (recommended for training)\n",
    "3. **aug_v2**: Aggressive augmentation (for comparison)\n",
    "\n",
    "**Augmentation Techniques:**\n",
    "- Random horizontal/vertical flips\n",
    "- Color jitter (hue, saturation, value)\n",
    "- Brightness and contrast adjustments\n",
    "- Random rotation and affine transformations\n",
    "- Mosaic augmentation (4-image combination for YOLO)\n",
    "- Random crop and resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb68981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentation pipelines using albumentations\n",
    "print(\"Defining augmentation pipelines...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pipeline 1: No Augmentation (Baseline)\n",
    "no_aug = A.Compose([\n",
    "    A.Resize(640, 640),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n",
    "\n",
    "print(\"âœ“ no_aug pipeline created:\")\n",
    "print(\"  - Resize to 640x640\")\n",
    "print(\"  - Normalize (ImageNet stats)\")\n",
    "print(\"  - Convert to tensor\")\n",
    "\n",
    "# Pipeline 2: Moderate Augmentation (aug_v1)\n",
    "aug_v1 = A.Compose([\n",
    "    A.Resize(640, 640),\n",
    "    \n",
    "    # Geometric transforms\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.RandomRotate90(p=0.3),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n",
    "    \n",
    "    # Color transforms\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    \n",
    "    # Quality transforms\n",
    "    A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "    \n",
    "    # Normalization\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels'], min_visibility=0.3))\n",
    "\n",
    "print(\"\\nâœ“ aug_v1 pipeline created (Moderate):\")\n",
    "print(\"  - Geometric: H-flip, V-flip, Rotate90, ShiftScaleRotate\")\n",
    "print(\"  - Color: ColorJitter, BrightnessContrast, HueSaturationValue\")\n",
    "print(\"  - Quality: GaussianBlur, GaussNoise\")\n",
    "\n",
    "# Pipeline 3: Aggressive Augmentation (aug_v2)\n",
    "aug_v2 = A.Compose([\n",
    "    A.Resize(640, 640),\n",
    "    \n",
    "    # More aggressive geometric transforms\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.3),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.15, scale_limit=0.3, rotate_limit=30, p=0.7),\n",
    "    A.Affine(scale=(0.8, 1.2), translate_percent=0.1, rotate=(-30, 30), p=0.5),\n",
    "    \n",
    "    # Aggressive color transforms\n",
    "    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.15, p=0.7),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.6),\n",
    "    A.HueSaturationValue(hue_shift_limit=30, sat_shift_limit=40, val_shift_limit=30, p=0.6),\n",
    "    A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n",
    "    \n",
    "    # Quality and occlusion\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "    A.GaussNoise(var_limit=(10.0, 80.0), p=0.3),\n",
    "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n",
    "    \n",
    "    # Normalization\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels'], min_visibility=0.2))\n",
    "\n",
    "print(\"\\nâœ“ aug_v2 pipeline created (Aggressive):\")\n",
    "print(\"  - Geometric: More aggressive rotations, affine transforms\")\n",
    "print(\"  - Color: Stronger jitter, gamma adjustments\")\n",
    "print(\"  - Quality: More noise, blur, coarse dropout\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2f95e",
   "metadata": {},
   "source": [
    "### 6.2 Augmentation Visualization - Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to apply augmentation and visualize\n",
    "def visualize_augmentation(img_path, bboxes, class_labels, augmentation, title=\"\"):\n",
    "    \"\"\"\n",
    "    Apply augmentation to an image and visualize results.\n",
    "    \n",
    "    Args:\n",
    "        img_path: Path to image\n",
    "        bboxes: List of bounding boxes in COCO format [x, y, width, height]\n",
    "        class_labels: List of class labels for each bbox\n",
    "        augmentation: Albumentations transform\n",
    "        title: Title for the plot\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(str(img_path))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Apply augmentation\n",
    "    try:\n",
    "        transformed = augmentation(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "        aug_image = transformed['image']\n",
    "        aug_bboxes = transformed['bboxes']\n",
    "        aug_labels = transformed['class_labels']\n",
    "        \n",
    "        # Convert tensor back to numpy for visualization if needed\n",
    "        if isinstance(aug_image, torch.Tensor):\n",
    "            # Denormalize\n",
    "            aug_image = aug_image.permute(1, 2, 0).numpy()\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            aug_image = std * aug_image + mean\n",
    "            aug_image = np.clip(aug_image, 0, 1)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Original image\n",
    "        ax1.imshow(image)\n",
    "        for bbox, label in zip(bboxes, class_labels):\n",
    "            x, y, w, h = bbox\n",
    "            rect = patches.Rectangle((x, y), w, h, linewidth=2, \n",
    "                                     edgecolor='lime', facecolor='none')\n",
    "            ax1.add_patch(rect)\n",
    "            ax1.text(x, y-5, f\"ID {label}\", color='white', fontsize=9, \n",
    "                    fontweight='bold', bbox=dict(boxstyle='round', facecolor='lime', alpha=0.7))\n",
    "        ax1.set_title(f\"Original Image\\n{len(bboxes)} objects\", fontsize=12, fontweight='bold')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Augmented image\n",
    "        ax2.imshow(aug_image)\n",
    "        for bbox, label in zip(aug_bboxes, aug_labels):\n",
    "            x, y, w, h = bbox\n",
    "            rect = patches.Rectangle((x, y), w, h, linewidth=2, \n",
    "                                     edgecolor='coral', facecolor='none')\n",
    "            ax2.add_patch(rect)\n",
    "            ax2.text(x, y-5, f\"ID {label}\", color='white', fontsize=9, \n",
    "                    fontweight='bold', bbox=dict(boxstyle='round', facecolor='coral', alpha=0.7))\n",
    "        ax2.set_title(f\"Augmented Image\\n{len(aug_bboxes)} objects retained\", \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt.suptitle(title, fontsize=14, fontweight='bold', y=0.98)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return fig, (image, aug_image), (bboxes, aug_bboxes)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error applying augmentation: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "print(\"âœ“ Augmentation visualization helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23255edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sample images for augmentation visualization\n",
    "num_samples = 6\n",
    "sample_indices = random.sample(range(len(filtered_images)), num_samples)\n",
    "\n",
    "print(f\"Selected {num_samples} random images for augmentation visualization\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72868b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize aug_v1 (Moderate Augmentation)\n",
    "print(\"Applying aug_v1 (Moderate Augmentation) to sample images...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "aug_v1_results = []\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices[:3]):\n",
    "    img_info = filtered_images[sample_idx]\n",
    "    img_path = IMAGE_DIR / img_info['file_name']\n",
    "    \n",
    "    # Get annotations for this image\n",
    "    img_anns = [ann for ann in filtered_annotations if ann['image_id'] == img_info['id']]\n",
    "    bboxes = [ann['bbox'] for ann in img_anns]\n",
    "    labels = [ann['category_id'] for ann in img_anns]\n",
    "    \n",
    "    # Visualize\n",
    "    fig, images, boxes = visualize_augmentation(\n",
    "        img_path, bboxes, labels, aug_v1, \n",
    "        title=f\"Moderate Augmentation (aug_v1) - Sample {idx+1}\"\n",
    "    )\n",
    "    \n",
    "    if fig:\n",
    "        aug_v1_results.append((fig, images, boxes))\n",
    "        plt.savefig(RUNS_DIR / f'aug_v1_sample_{idx+1}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ aug_v1 visualizations saved to runs/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8704e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize aug_v2 (Aggressive Augmentation)\n",
    "print(\"Applying aug_v2 (Aggressive Augmentation) to sample images...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "aug_v2_results = []\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices[3:6]):\n",
    "    img_info = filtered_images[sample_idx]\n",
    "    img_path = IMAGE_DIR / img_info['file_name']\n",
    "    \n",
    "    # Get annotations for this image\n",
    "    img_anns = [ann for ann in filtered_annotations if ann['image_id'] == img_info['id']]\n",
    "    bboxes = [ann['bbox'] for ann in img_anns]\n",
    "    labels = [ann['category_id'] for ann in img_anns]\n",
    "    \n",
    "    # Visualize\n",
    "    fig, images, boxes = visualize_augmentation(\n",
    "        img_path, bboxes, labels, aug_v2, \n",
    "        title=f\"Aggressive Augmentation (aug_v2) - Sample {idx+1}\"\n",
    "    )\n",
    "    \n",
    "    if fig:\n",
    "        aug_v2_results.append((fig, images, boxes))\n",
    "        plt.savefig(RUNS_DIR / f'aug_v2_sample_{idx+1}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ aug_v2 visualizations saved to runs/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a5bc97",
   "metadata": {},
   "source": [
    "### 6.3 Custom Dataset Classes for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e1cc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for Object Detection (YOLO format)\n",
    "class TACODetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for TACO waste detection with augmentation support.\n",
    "    Returns images and bounding boxes in YOLO format.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, images, annotations, img_dir, transform=None, class_mapping=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: List of image dictionaries from COCO\n",
    "            annotations: List of annotation dictionaries from COCO\n",
    "            img_dir: Path to images directory\n",
    "            transform: Albumentations transform pipeline\n",
    "            class_mapping: Dict mapping category IDs to class indices\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Group annotations by image ID\n",
    "        self.img_annotations = defaultdict(list)\n",
    "        for ann in annotations:\n",
    "            self.img_annotations[ann['image_id']].append(ann)\n",
    "        \n",
    "        # Create class mapping if not provided\n",
    "        if class_mapping is None:\n",
    "            unique_classes = sorted(set(ann['category_id'] for ann in annotations))\n",
    "            self.class_mapping = {cat_id: idx for idx, cat_id in enumerate(unique_classes)}\n",
    "        else:\n",
    "            self.class_mapping = class_mapping\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self.images)} images\")\n",
    "        print(f\"Class mapping: {self.class_mapping}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image info\n",
    "        img_info = self.images[idx]\n",
    "        img_path = self.img_dir / img_info['file_name']\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get annotations\n",
    "        anns = self.img_annotations.get(img_info['id'], [])\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            bbox = ann['bbox']  # [x, y, width, height] in COCO format\n",
    "            bboxes.append(bbox)\n",
    "            labels.append(self.class_mapping[ann['category_id']])\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            try:\n",
    "                transformed = self.transform(image=image, bboxes=bboxes, class_labels=labels)\n",
    "                image = transformed['image']\n",
    "                bboxes = transformed['bboxes']\n",
    "                labels = transformed['class_labels']\n",
    "            except Exception as e:\n",
    "                # Fallback: return original if transform fails\n",
    "                print(f\"Transform failed for {img_path}: {e}\")\n",
    "                pass\n",
    "        \n",
    "        # Convert bboxes to tensor [N, 4]\n",
    "        if len(bboxes) > 0:\n",
    "            bboxes = torch.tensor(bboxes, dtype=torch.float32)\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "        else:\n",
    "            bboxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'bboxes': bboxes,\n",
    "            'labels': labels,\n",
    "            'image_id': img_info['id']\n",
    "        }\n",
    "\n",
    "print(\"âœ“ TACODetectionDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea584d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for Semantic Segmentation (U-Net)\n",
    "class TACOSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for TACO waste segmentation with augmentation support.\n",
    "    Returns images and segmentation masks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, images, annotations, img_dir, transform=None, class_mapping=None, mask_size=(640, 640)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: List of image dictionaries from COCO\n",
    "            annotations: List of annotation dictionaries from COCO\n",
    "            img_dir: Path to images directory\n",
    "            transform: Albumentations transform pipeline\n",
    "            class_mapping: Dict mapping category IDs to class indices\n",
    "            mask_size: Size of output mask\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "        self.mask_size = mask_size\n",
    "        \n",
    "        # Group annotations by image ID\n",
    "        self.img_annotations = defaultdict(list)\n",
    "        for ann in annotations:\n",
    "            self.img_annotations[ann['image_id']].append(ann)\n",
    "        \n",
    "        # Create class mapping\n",
    "        if class_mapping is None:\n",
    "            unique_classes = sorted(set(ann['category_id'] for ann in annotations))\n",
    "            self.class_mapping = {cat_id: idx + 1 for idx, cat_id in enumerate(unique_classes)}\n",
    "            self.class_mapping[0] = 0  # Background class\n",
    "        else:\n",
    "            self.class_mapping = class_mapping\n",
    "        \n",
    "        self.num_classes = len(self.class_mapping)\n",
    "        \n",
    "        print(f\"Segmentation dataset initialized with {len(self.images)} images\")\n",
    "        print(f\"Number of classes: {self.num_classes} (including background)\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def create_mask(self, anns, img_shape):\n",
    "        \"\"\"Create segmentation mask from annotations.\"\"\"\n",
    "        mask = np.zeros(img_shape[:2], dtype=np.uint8)\n",
    "        \n",
    "        for ann in anns:\n",
    "            if 'segmentation' not in ann or not ann['segmentation']:\n",
    "                # Use bbox if no segmentation\n",
    "                bbox = ann['bbox']\n",
    "                x, y, w, h = map(int, bbox)\n",
    "                class_idx = self.class_mapping[ann['category_id']]\n",
    "                mask[y:y+h, x:x+w] = class_idx\n",
    "            else:\n",
    "                # Use segmentation polygon\n",
    "                class_idx = self.class_mapping[ann['category_id']]\n",
    "                seg = ann['segmentation']\n",
    "                \n",
    "                if isinstance(seg, list):\n",
    "                    # Polygon format\n",
    "                    for poly in seg:\n",
    "                        poly_array = np.array(poly).reshape(-1, 2).astype(np.int32)\n",
    "                        cv2.fillPoly(mask, [poly_array], class_idx)\n",
    "                elif isinstance(seg, dict):\n",
    "                    # RLE format\n",
    "                    rle_mask = coco_mask.decode(seg)\n",
    "                    mask[rle_mask > 0] = class_idx\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image info\n",
    "        img_info = self.images[idx]\n",
    "        img_path = self.img_dir / img_info['file_name']\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get annotations and create mask\n",
    "        anns = self.img_annotations.get(img_info['id'], [])\n",
    "        mask = self.create_mask(anns, image.shape)\n",
    "        \n",
    "        # Apply transforms (both image and mask)\n",
    "        if self.transform:\n",
    "            # For segmentation, we need different transform without bbox params\n",
    "            try:\n",
    "                transformed = self.transform(image=image, mask=mask)\n",
    "                image = transformed['image']\n",
    "                mask = transformed['mask']\n",
    "            except Exception as e:\n",
    "                print(f\"Transform failed for {img_path}: {e}\")\n",
    "        \n",
    "        # Convert mask to tensor\n",
    "        mask = torch.tensor(mask, dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'mask': mask,\n",
    "            'image_id': img_info['id']\n",
    "        }\n",
    "\n",
    "print(\"âœ“ TACOSegmentationDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed821058",
   "metadata": {},
   "source": [
    "### 6.4 Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc69069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "print(\"Splitting dataset into train and validation sets...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use 80/20 split\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "\n",
    "# Perform split with fixed random seed for reproducibility\n",
    "train_images, val_images = train_test_split(\n",
    "    filtered_images, \n",
    "    test_size=val_ratio, \n",
    "    random_state=RANDOM_SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Get corresponding annotations\n",
    "train_img_ids = set(img['id'] for img in train_images)\n",
    "val_img_ids = set(img['id'] for img in val_images)\n",
    "\n",
    "train_annotations = [ann for ann in filtered_annotations if ann['image_id'] in train_img_ids]\n",
    "val_annotations = [ann for ann in filtered_annotations if ann['image_id'] in val_img_ids]\n",
    "\n",
    "print(f\"Dataset split (seed={RANDOM_SEED}):\")\n",
    "print(f\"  Train: {len(train_images)} images, {len(train_annotations)} annotations\")\n",
    "print(f\"  Val:   {len(val_images)} images, {len(val_annotations)} annotations\")\n",
    "print(f\"  Ratio: {len(train_images)/len(filtered_images)*100:.1f}% / {len(val_images)/len(filtered_images)*100:.1f}%\")\n",
    "\n",
    "# Verify no overlap\n",
    "assert len(train_img_ids.intersection(val_img_ids)) == 0, \"Train/val split has overlap!\"\n",
    "print(\"\\nâœ“ Train/val split verified (no overlap)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d250c32",
   "metadata": {},
   "source": [
    "### 6.5 Experiment Tracking Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51936fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment tracking DataFrame\n",
    "experiments_data = {\n",
    "    'experiment_id': [],\n",
    "    'model_type': [],  # 'YOLO' or 'UNet'\n",
    "    'augmentation': [],  # 'no_aug', 'aug_v1', 'aug_v2'\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'mAP50': [],\n",
    "    'mAP50_95': [],\n",
    "    'iou': [],  # For segmentation\n",
    "    'dice': [],  # For segmentation\n",
    "    'train_time': [],  # seconds\n",
    "    'epochs': [],\n",
    "    'notes': []\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "aug_experiments_df = pd.DataFrame(experiments_data)\n",
    "\n",
    "# Save initial empty DataFrame\n",
    "aug_experiments_df.to_csv(RUNS_DIR / 'aug_experiments.csv', index=False)\n",
    "\n",
    "print(\"Experiment tracking initialized\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Tracking file: {RUNS_DIR / 'aug_experiments.csv'}\")\n",
    "print(\"\\nPlanned experiments:\")\n",
    "print(\"  1. YOLO + no_aug (baseline)\")\n",
    "print(\"  2. YOLO + aug_v1 (moderate)\")\n",
    "print(\"  3. YOLO + aug_v2 (aggressive) [optional]\")\n",
    "print(\"  4. U-Net + no_aug (baseline)\")\n",
    "print(\"  5. U-Net + aug_v1 (moderate)\")\n",
    "print(\"  6. U-Net + aug_v2 (aggressive) [optional]\")\n",
    "print(\"\\nMetrics to track:\")\n",
    "print(\"  YOLO: Precision, Recall, mAP@50, mAP@50-95\")\n",
    "print(\"  U-Net: IoU, Dice Score\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bdf739",
   "metadata": {},
   "source": [
    "### 6.6 Summary of Augmentation Strategy\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "1. **Three Pipelines Defined:**\n",
    "   - `no_aug`: Baseline (resize + normalize only)\n",
    "   - `aug_v1`: Moderate augmentation (recommended)\n",
    "   - `aug_v2`: Aggressive augmentation (for ablation)\n",
    "\n",
    "2. **Reproducibility:**\n",
    "   - Fixed random seed (42) across all operations\n",
    "   - Deterministic transforms\n",
    "   - Consistent train/val split\n",
    "\n",
    "3. **Dataset Classes:**\n",
    "   - `TACODetectionDataset`: For YOLO training with bounding boxes\n",
    "   - `TACOSegmentationDataset`: For U-Net training with masks\n",
    "   - Both support augmentation pipelines\n",
    "\n",
    "4. **Experiment Plan:**\n",
    "   - Train YOLO and U-Net with `no_aug` (baseline)\n",
    "   - Train YOLO and U-Net with `aug_v1` (compare performance)\n",
    "   - Optional: Train with `aug_v2` for ablation study\n",
    "   - Track all metrics in `aug_experiments.csv`\n",
    "\n",
    "5. **Expected Benefits:**\n",
    "   - Improved generalization\n",
    "   - Reduced overfitting\n",
    "   - Better performance on unseen data\n",
    "   - More robust models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191cc86b",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. YOLOv8 Object Detection\n",
    "\n",
    "Coming next: Train and evaluate YOLOv8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for YOLOv8 section\n",
    "print(\"YOLOv8 section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb80624b",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. U-Net Semantic Segmentation\n",
    "\n",
    "Coming next: Build and train U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb71565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for U-Net section\n",
    "print(\"U-Net section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dbdeb4",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model Comparison & Discussion\n",
    "\n",
    "Coming next: Compare YOLO vs U-Net performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445623cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for comparison section\n",
    "print(\"Comparison section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317d9dc9",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Conclusions & Future Work\n",
    "\n",
    "Coming next: Final conclusions and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for conclusions section\n",
    "print(\"Conclusions section will be implemented in the next milestone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4b1f2",
   "metadata": {},
   "source": [
    "---\n",
    "## End of Notebook\n",
    "\n",
    "**Project:** YOLOv8 + U-Net Waste Detection and Segmentation  \n",
    "**Course:** Deep Learning for Perception (CS4045)  \n",
    "**Authors:** Minahil Ali (22i-0849), Ayaan Khan (22i-0832)  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
